{"config":{"lang":["en","ja"],"separator":"[\\s\\u200b\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"RoboTwin 2.0","text":"<p>Here is the official documentation for RoboTwin 2.0, which includes installation and usage instructions for various RoboTwin functionalities, detailed information on the 50 bimanual tasks in RoboTwin 2.0, comprehensive descriptions of the RoboTwin-OD dataset, and guidelines for joining the community.</p> <p></p>"},{"location":"index.html#1-everything-about-robotwin-20","title":"1. Everything about RoboTwin 2.0","text":"<p>Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Qiwei Liang, Zixuan Li, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan-ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo, Yao Mu</p> <p>Webpage: https://robotwin-platform.github.io/</p> <p>PDF: RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</p> <p>Paper (arXiv, Coming Soon): RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</p> <p>Github Repo: http://github.com/robotwin-Platform/RoboTwin</p>"},{"location":"index.html#2-previous-works","title":"2. Previous Works","text":"<p>[CVPR 2025 Highlight] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins [CVPR 2025 Challenge@MEIS Workshop] The Technical report is coming soon ! [ECCV 2024 MAAS Workshop Best Paper] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version) [\u200b\u7b2c\u5341\u4e5d\u5c4a\u200b\u6311\u6218\u676f\u200b\u5b98\u65b9\u200b\u8d5b\u9898\u200b] \u200b\u8d5b\u9898\u200b\u94fe\u63a5\u200b</p>"},{"location":"index.html#3-citations","title":"3. Citations","text":"<p>If you find our work useful, please consider citing:</p> <p>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation <pre><code>Coming Soon !\n</code></pre></p> <p>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins, accepted to CVPR 2025 (Highlight) <pre><code>@InProceedings{Mu_2025_CVPR,\n    author    = {Mu, Yao and Chen, Tianxing and Chen, Zanxin and Peng, Shijia and Lan, Zhiqian and Gao, Zeyu and Liang, Zhixuan and Yu, Qiaojun and Zou, Yude and Xu, Mingkun and Lin, Lunkai and Xie, Zhiqiang and Ding, Mingyu and Luo, Ping},\n    title     = {RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins},\n    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\n    month     = {June},\n    year      = {2025},\n    pages     = {27649-27660}\n}\n</code></pre></p> <p>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version), accepted to ECCV Workshop 2024 (Best Paper) <pre><code>@article{mu2024robotwin,\n  title={RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)},\n  author={Mu, Yao and Chen, Tianxing and Peng, Shijia and Chen, Zanxin and Gao, Zeyu and Zou, Yude and Lin, Lunkai and Xie, Zhiqiang and Luo, Ping},\n  journal={arXiv preprint arXiv:2409.02920},\n  year={2024}\n}\n</code></pre></p>"},{"location":"index.html#4-contact","title":"4. Contact","text":"<p>Contact Tianxing Chen if you have any questions or suggestions.</p>"},{"location":"community/index.html","title":"WeChat Group","text":"<p>Contact Tianxing Chen if you have any questions or suggestions.</p> <p></p>"},{"location":"main/about.html","title":"\u5173\u4e8e","text":"<p>\u200b\u6e05\u534e\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\uff08\u200b\u7b80\u79f0\u200b\u201c\u200b\u624b\u518c\u200b\u201d\uff09\u200b\u662f\u200b\u7531\u200b\u4e00\u7fa4\u200b\u6e05\u534e\u5927\u5b66\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u6bd5\u4e1a\u751f\u200b\u7f16\u5199\u200b\u7ef4\u62a4\u200b\u7684\u200b\u5728\u7ebf\u200b\u6587\u6863\u200b\u3002</p> <p>\u200b\u8be5\u200b\u624b\u518c\u200b\u65e8\u5728\u200b\u603b\u7ed3\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u7ecf\u9a8c\u6559\u8bad\u200b\uff0c\u200b\u56de\u7b54\u200b\u7533\u8bf7\u200b\u4e2d\u200b\u9047\u5230\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e3a\u200b\u62df\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u5b66\u5f1f\u200b\u5b66\u59b9\u200b\u63d0\u4f9b\u200b\u501f\u9274\u200b\u548c\u200b\u53c2\u8003\u200b\uff0c\u200b\u51cf\u5c11\u200b\u4fe1\u606f\u200b\u5dee\u200b\u5e26\u6765\u200b\u7684\u200b\u4e0d\u200b\u516c\u5e73\u200b\uff0c\u200b\u964d\u4f4e\u200b\u51c6\u5907\u200b\u65f6\u95f4\u200b\u548c\u200b\u91d1\u94b1\u200b\u6210\u672c\u200b\uff0c\u200b\u7f13\u89e3\u200b\u7533\u8bf7\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u7126\u8651\u200b\u3002</p> <p>\u200b\u5e0c\u671b\u200b\u672c\u624b\u518c\u200b\u80fd\u591f\u200b\u5e2e\u52a9\u200b\u5230\u200b\u62df\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u6e05\u534e\u200b\u5b66\u751f\u200b\uff0c\u200b\u5728\u200b\u7533\u8bf7\u200b\u8def\u4e0a\u200b\u987a\u987a\u5229\u5229\u200b\uff0c\u200b\u83b7\u5f97\u200b\u81ea\u5df1\u200b\u5fc3\u4eea\u200b\u9879\u76ee\u200b\u7684\u200b Offer\uff01</p>"},{"location":"main/about.html#_2","title":"\u58f0\u660e","text":"<p>\u200b\u672c\u624b\u518c\u200b\u4f7f\u7528\u200b Material for Mkdocs \u200b\u6784\u5efa\u200b\uff0c\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u5f00\u6e90\u200b\uff0c\u200b\u4f7f\u7528\u200b GitHub Pages \u200b\u516c\u5f00\u200b\u53d1\u5e03\u200b\u3002</p> <p>\u200b\u624b\u518c\u200b\u5185\u5bb9\u200b\u7531\u200b\u6e05\u534e\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\u7f16\u5199\u200b\u59d4\u5458\u4f1a\u200b\u6536\u96c6\u200b\u3001\u200b\u7f16\u5199\u200b\u3001\u200b\u5ba1\u6838\u200b\u3001\u200b\u53d1\u5e03\u200b\u5e76\u200b\u7ef4\u62a4\u200b\u3002\u200b\u5185\u5bb9\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\uff0c\u200b\u4e14\u200b\u4e0d\u200b\u4ee3\u8868\u200b\u4efb\u4f55\u200b\u653f\u6cbb\u7acb\u573a\u200b\u3002\u200b\u624b\u518c\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b \u200b\u4f9d\u636e\u200bCC BY-NC 4.0\u200b\u6388\u6743\u200b\uff0c\u200b\u4e0d\u5f97\u200b\u505a\u200b\u5546\u4e1a\u7528\u9014\u200b\uff0c\u200b\u8f6c\u8f7d\u200b\u6216\u8005\u200b\u5f15\u7528\u200b\u8bf7\u200b\u6ce8\u660e\u200b\u6765\u6e90\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u4e0d\u200b\u9075\u5b88\u200b\u6b64\u200b\u58f0\u660e\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u8fdd\u6cd5\u200b\u4f7f\u7528\u200b\u672c\u6587\u200b\u5185\u5bb9\u200b\u8005\u200b\uff0c\u200b\u4f9d\u6cd5\u200b\u4fdd\u7559\u200b\u8ffd\u7a76\u200b\u6743\u7b49\u200b\u3002</p>"},{"location":"main/about.html#_3","title":"\u8fdb\u5ea6\u200b\u4e0e\u200b\u66f4\u65b0","text":"<p>\u200b\u76ee\u524d\u200b\u672c\u624b\u518c\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u5927\u90e8\u5206\u200b \u200b\u524d\u8a00\u200b, \u200b\u51c6\u5907\u200b, \u200b\u5f55\u53d6\u200b\u53ca\u200b\u4e4b\u540e\u200b \u200b\u7ae0\u8282\u200b\u4e2d\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u672a\u6765\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6dfb\u52a0\u200b\u62db\u751f\u200b\u4fe1\u606f\u200b\u3001\u200b\u9879\u76ee\u200b/\u200b\u9662\u7cfb\u200b\u4ecb\u7ecd\u200b\u7b49\u200b\u677f\u5757\u200b\uff0c\u200b\u6b22\u8fce\u200b\u6709\u610f\u200b\u53d1\u5e03\u200b\u5185\u5bb9\u200b\u7684\u200b\u8001\u5e08\u200b\u6216\u200b\u540c\u5b66\u200b \u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u3002</p> <p>\u200b\u5e0c\u671b\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u540c\u5b66\u200b\u52a0\u5165\u200b\u8fdb\u6765\u200b\uff0c\u200b\u5e2e\u52a9\u200b\u64b0\u5199\u200b\u548c\u200b\u5b8c\u5584\u200b\u7f51\u7ad9\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u63d0\u4ea4\u200b\u7533\u8bf7\u200b\u603b\u7ed3\u200b\u8bf7\u200b\u524d\u5f80\u200b\u98de\u8dc3\u200b\u6570\u636e\u5e93\u200b\u3002</p> <p>\u200b\u672c\u624b\u518c\u200b\u8fd8\u200b\u5728\u200b\u5feb\u901f\u200b\u5efa\u8bbe\u200b\u4e2d\u200b\uff0c\u200b\u6b22\u8fce\u200b\u4f60\u200b\u63d0\u51fa\u200b\u610f\u89c1\u200b\u6216\u200b\u5efa\u8bae\u200b\u3002</p>"},{"location":"main/appendix.html","title":"Appendix","text":"\u540d\u8bcd\u200b \u200b\u89e3\u91ca\u200b bg \u200b\u80cc\u666f\u200b\uff08background\uff09 tl timeline HYPSM/\u200b\u54c8\u8036\u666e\u65af\u9ebb\u200b \u200b\u54c8\u4f5b\u200b\u3001\u200b\u8036\u9c81\u200b\u3001\u200b\u666e\u6797\u65af\u987f\u200b\u3001\u200b\u65af\u5766\u798f\u200b\u3001\u200b\u9ebb\u7701\u7406\u5de5\u200b\u4e94\u6240\u200b\u5927\u5b66\u200b\u7684\u200b\u7b80\u79f0\u200b bar \u200b\u95e8\u69db\u200b wl \u200b\u7b49\u5f85\u200b\u540d\u5355\u200b/\u200b\u517b\u9c7c\u200b\u9c7c\u5858\u200b\uff08waitlist\uff09 \u200b\u4e09\u7ef4\u200b \u200b\u8bed\u8a00\u200b&amp;GRE&amp;GPA pub publication RA reseach assistant TA teaching assistant ad admission\uff0c\u200b\u5c24\u5176\u200b\u6307\u65e0\u5956\u200b\u6216\u200b\u5c0f\u5956\u200b dp datapoint"},{"location":"main/committee.html","title":"Committee","text":""},{"location":"main/committee.html#_1","title":"\u6df1\u5733\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\u7f16\u5199\u200b\u59d4\u5458\u4f1a","text":""},{"location":"main/committee.html#_2","title":"\u59d4\u5458\u4f1a\u200b\u6210\u5458","text":"<ul> <li>\u200b\u9648\u5929\u884c\u200b\uff0c2025\u200b\u5c4a\u200b\u8ba1\u8f6f\u200b\u672c\u79d1\u751f\u200b (\u200b\u521b\u59cb\u4eba\u200b)</li> <li>\u200b\u5173\u6d69\u6797\u200b\uff0c2026\u200b\u5c4a\u200b\u8ba1\u8f6f\u200b\u672c\u79d1\u751f\u200b</li> <li>\u200b\u5f6d\u5c0f\u521a\u200b\uff0c\u200b\u6df1\u5733\u5927\u5b66\u200b\u526f\u6559\u6388\u200b</li> </ul>"},{"location":"main/committee.html#_3","title":"\u8054\u7cfb\u200b\u4e0e\u200b\u652f\u6301","text":"<p>\u200b\u8d1f\u8d23\u4eba\u200b\u90ae\u7bb1\u200b: chentianxing2002@gmail.com \u200b\u8d1f\u8d23\u4eba\u200b\u5fae\u4fe1\u200b: TianxingChen_2002</p>"},{"location":"main/committee.html#_4","title":"\u58f0\u660e","text":"<p>\u200b\u672c\u624b\u518c\u200b\u4f7f\u7528\u200b Material for Mkdocs \u200b\u6784\u5efa\u200b\uff0c\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u5f00\u6e90\u200b\uff0c\u200b\u4f7f\u7528\u200b GitHub Pages \u200b\u516c\u5f00\u200b\u53d1\u5e03\u200b\u3002</p> <p>\u200b\u624b\u518c\u200b\u5185\u5bb9\u200b\u7531\u200b \u200b\u6df1\u5733\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\u7f16\u5199\u200b\u59d4\u5458\u4f1a\u200b \u200b\u6536\u96c6\u200b\u3001\u200b\u7f16\u5199\u200b\u3001\u200b\u5ba1\u6838\u200b\u3001\u200b\u53d1\u5e03\u200b\u5e76\u200b\u7ef4\u62a4\u200b\u3002\u200b\u5185\u5bb9\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\uff0c\u200b\u4e14\u200b\u4e0d\u200b\u4ee3\u8868\u200b\u4efb\u4f55\u200b\u653f\u6cbb\u7acb\u573a\u200b\u3002\u200b\u624b\u518c\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u4f9d\u636e\u200bCC BY-NC 4.0\u200b\u6388\u6743\u200b\uff0c\u200b\u4e0d\u5f97\u200b\u505a\u200b\u5546\u4e1a\u7528\u9014\u200b\uff0c\u200b\u8f6c\u8f7d\u200b\u6216\u8005\u200b\u5f15\u7528\u200b\u8bf7\u200b\u6ce8\u660e\u200b\u6765\u6e90\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u4e0d\u200b\u9075\u5b88\u200b\u6b64\u200b\u58f0\u660e\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u8fdd\u6cd5\u200b\u4f7f\u7528\u200b\u672c\u6587\u200b\u5185\u5bb9\u200b\u8005\u200b\uff0c\u200b\u4f9d\u6cd5\u200b\u4fdd\u7559\u200b\u8ffd\u7a76\u200b\u6743\u7b49\u200b\u3002</p>"},{"location":"main/contributions.html","title":"Contributions","text":""},{"location":"main/contributions.html#project-leaders","title":"Project Leaders","text":"<p>Tianxing Chen, Yao Mu, Zhixuan Liang</p>"},{"location":"main/contributions.html#1-roadmap-methodology","title":"1. Roadmap &amp; Methodology","text":"<p>Yao Mu, Tianxing Chen, Ping Luo, Yusen Qin, Xiaokang Yang, Kaixuan Wang</p>"},{"location":"main/contributions.html#2-data-generator-benchmark","title":"2. Data Generator &amp; Benchmark","text":"<p>Tianxing Chen, Zanxin Chen, Baijun Chen, Qiwei Liang, Zixuan Li, Xianliang Lin</p>"},{"location":"main/contributions.html#3-codegen-agent","title":"3. CodeGen Agent","text":"<p>Yibin Liu, Zanxin Chen, Yiheng Ge, Tianxing Chen, Mengkang Hu</p>"},{"location":"main/contributions.html#4-robotwin-od","title":"4. RoboTwin-OD","text":"<p>Baijun Chen, Qiangyu Chen, Kailun Su, Xuanbing Xie, Zanxin Chen</p>"},{"location":"main/contributions.html#5-policies-training-evaluation","title":"5. Policies Training &amp; Evaluation","text":"<p>Tianxing Chen, Zijian Cai, Tian Nian, Huan-ang Gao, Tianling Xu</p>"},{"location":"main/contributions.html#6-real-world-deployment","title":"6. Real-World Deployment","text":"<p>Tianxing Chen, Tian Nian, Weiliang Deng</p>"},{"location":"main/contributions.html#7-domain-randomization","title":"7. Domain Randomization","text":"<p>Baijun Chen, Yubin Guo, Qiwei Liang, Zhenyu Gu, Guodong Liu, Zanxin Chen, Tianxing Chen</p>"},{"location":"objects/index.html","title":"RoboTwin-OD (RoboTwin Object Dataset)","text":"<p>This is a document containing images and ID information for all objects in RoboTwin-OD, excluding those from the Objaverse dataset.</p> <p>To enhance both manipulation capability and visual understanding, we construct a large-scale object dataset with rich semantic annotations, called RoboTwin-OD, covering 147 categories and 731 diverse objects. Specifically, this includes 534 instances across 111 categories with custom-generated and optimized meshes, 153 objects from 27 categories in Objaverse, and 44 articulated object instances from 9 categories in SAPIEN PartNet-Mobility. Objects from all sources, including Objaverse, are used for cluttered scene construction, with Objaverse specifically serving to further increase the visual and semantic diversity of distractor objects. Additionally, we develop a comprehensive surface and background texture library using generative AI and human-in-the-loop verification to ensure both diversity and realism. The dataset is available at https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects.</p>"},{"location":"objects/001_bottle.html","title":"001_Bottle","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p> <p>base11</p> <p>base12</p> <p>base13</p> <p>base14</p> <p>base15</p> <p>base16</p> <p>base17</p> <p>base18</p> <p>base19</p> <p>base20</p> <p>base21</p> <p>base22</p>"},{"location":"objects/002_bowl.html","title":"002_Bowl","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/003_plate.html","title":"003_Plate","text":"<p>base0</p>"},{"location":"objects/004_fluted-block.html","title":"004_Fluted-Block","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/005_french-fries.html","title":"005_French-Fries","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/006_hamburg.html","title":"006_Hamburg","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/007_shoe-box.html","title":"007_Shoe-Box","text":"<p>base0</p>"},{"location":"objects/008_tray.html","title":"008_Tray","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/009_kettle.html","title":"009_Kettle","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/010_pen.html","title":"010_Pen","text":"Missing images for the pen object."},{"location":"objects/011_dustbin.html","title":"011_Dustbin","text":"<p>base0</p>"},{"location":"objects/012_plant-pot.html","title":"012_Plant-Pot","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/013_dumbbell-rack.html","title":"013_Dumbbell-Rack","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/014_bookcase.html","title":"014_Bookcase","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/015_laptop.html","title":"015_Laptop","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p>"},{"location":"objects/016_oven.html","title":"016_Oven","text":"Missing images for the oven object."},{"location":"objects/017_calculator.html","title":"017_Calculator","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/018_microphone.html","title":"018_Microphone","text":"<p>base0</p> <p>base1</p> <p>base4</p> <p>base5</p>"},{"location":"objects/019_coaster.html","title":"019_Coaster","text":"<p>base0</p>"},{"location":"objects/020_hammer.html","title":"020_Hammer","text":"<p>base0</p>"},{"location":"objects/021_cup.html","title":"021_Cup","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p> <p>base11</p> <p>base12</p>"},{"location":"objects/022_cup-with-liquid.html","title":"022_Cup-With-Liquid","text":"<p>base0</p>"},{"location":"objects/023_tissue-box.html","title":"023_Tissue-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/024_scanner.html","title":"024_Scanner","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/025_chips-tub.html","title":"025_Chips-Tub","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/026_pet-collar.html","title":"026_Pet-Collar","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/027_table-tennis.html","title":"027_Table-Tennis","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/028_roll-paper.html","title":"028_Roll-Paper","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/029_olive-oil.html","title":"029_Olive-Oil","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/030_drill.html","title":"030_Drill","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/031_jam-jar.html","title":"031_Jam-Jar","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/032_screwdriver.html","title":"032_Screwdriver","text":"<p>base0</p>"},{"location":"objects/033_fork.html","title":"033_Fork","text":"<p>base0</p>"},{"location":"objects/034_knife.html","title":"034_Knife","text":"<p>base0</p>"},{"location":"objects/035_apple.html","title":"035_Apple","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/036_cabinet.html","title":"036_Cabinet","text":"<p>base0</p>"},{"location":"objects/037_box.html","title":"037_Box","text":"<p>base0</p>"},{"location":"objects/038_milk-box.html","title":"038_Milk-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/039_mug.html","title":"039_Mug","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p> <p>base11</p> <p>base12</p>"},{"location":"objects/040_rack.html","title":"040_Rack","text":"<p>base0</p>"},{"location":"objects/041_shoe.html","title":"041_Shoe","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p>"},{"location":"objects/042_wooden_box.html","title":"042_Wooden_Box","text":"<p>base0</p>"},{"location":"objects/043_book.html","title":"043_Book","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/044_microwave.html","title":"044_Microwave","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/045_sand-clock.html","title":"045_Sand-Clock","text":"<p>base0</p> <p>base1</p> <p>base3</p>"},{"location":"objects/046_alarm-clock.html","title":"046_Alarm-Clock","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/047_mouse.html","title":"047_Mouse","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/048_stapler.html","title":"048_Stapler","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/049_shampoo.html","title":"049_Shampoo","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/050_bell.html","title":"050_Bell","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/051_candlestick.html","title":"051_Candlestick","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/052_dumbbell.html","title":"052_Dumbbell","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/053_teanet.html","title":"053_Teanet","text":"<p>base1</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/054_baguette.html","title":"054_Baguette","text":"<p>base2</p> <p>base3</p>"},{"location":"objects/055_small-speaker.html","title":"055_Small-Speaker","text":"<p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/056_switch.html","title":"056_Switch","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/057_toycar.html","title":"057_Toycar","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/058_markpen.html","title":"058_Markpen","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/059_pencup.html","title":"059_Pencup","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/060_kitchenpot.html","title":"060_Kitchenpot","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/061_battery.html","title":"061_Battery","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/062_plasticbox.html","title":"062_Plasticbox","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p>"},{"location":"objects/063_tabletrashbin.html","title":"063_Tabletrashbin","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p>"},{"location":"objects/064_msg.html","title":"064_Msg","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/065_soy-sauce.html","title":"065_Soy-Sauce","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/066_vinegar.html","title":"066_Vinegar","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/067_steamer.html","title":"067_Steamer","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/068_boxdrink.html","title":"068_Boxdrink","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/069_vagetable.html","title":"069_Vagetable","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/070_paymentsign.html","title":"070_Paymentsign","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/071_can.html","title":"071_Can","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base5</p> <p>base6</p>"},{"location":"objects/072_electronicscale.html","title":"072_Electronicscale","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base5</p> <p>base6</p>"},{"location":"objects/073_rubikscube.html","title":"073_Rubikscube","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/074_displaystand.html","title":"074_Displaystand","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/075_bread.html","title":"075_Bread","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/076_breadbasket.html","title":"076_Breadbasket","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/077_phone.html","title":"077_Phone","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/078_phonestand.html","title":"078_Phonestand","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/079_remotecontrol.html","title":"079_Remotecontrol","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/080_pillbottle.html","title":"080_Pillbottle","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/081_playingcards.html","title":"081_Playingcards","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/082_smallshovel.html","title":"082_Smallshovel","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/083_brush.html","title":"083_Brush","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/084_woodenmallet.html","title":"084_Woodenmallet","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/085_gong.html","title":"085_Gong","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/086_woodenblock.html","title":"086_Woodenblock","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/087_waterer.html","title":"087_Waterer","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/088_wineglass.html","title":"088_Wineglass","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/089_globe.html","title":"089_Globe","text":"<p>base2</p> <p>base3</p>"},{"location":"objects/090_trophy.html","title":"090_Trophy","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/091_kettle.html","title":"091_Kettle","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/092_notebook.html","title":"092_Notebook","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/093_brush-pen.html","title":"093_Brush-Pen","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/094_rest.html","title":"094_Rest","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/095_glue.html","title":"095_Glue","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/096_cleaner.html","title":"096_Cleaner","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/097_screen.html","title":"097_Screen","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/098_speaker.html","title":"098_Speaker","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/099_fan.html","title":"099_Fan","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/100_seal.html","title":"100_Seal","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base6</p>"},{"location":"objects/101_milk-tea.html","title":"101_Milk-Tea","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/102_roller.html","title":"102_Roller","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/103_fruit.html","title":"103_Fruit","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/104_board.html","title":"104_Board","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/105_sauce-can.html","title":"105_Sauce-Can","text":"<p>base0</p> <p>base2</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/106_skillet.html","title":"106_Skillet","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/107_soap.html","title":"107_Soap","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/108_block.html","title":"108_Block","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/109_hydrating-oil.html","title":"109_Hydrating-Oil","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base5</p>"},{"location":"objects/110_basket.html","title":"110_Basket","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/111_callbell.html","title":"111_Callbell","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/112_tea-box.html","title":"112_Tea-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/113_coffee-box.html","title":"113_Coffee-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/114_bottle.html","title":"114_Bottle","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/115_perfume.html","title":"115_Perfume","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/116_keyboard.html","title":"116_Keyboard","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/117_whiteboard-eraser.html","title":"117_Whiteboard-Eraser","text":"<p>base0</p>"},{"location":"objects/118_tooth-paste.html","title":"118_Tooth-Paste","text":"<p>base0</p>"},{"location":"objects/119_mini-chalkboard.html","title":"119_Mini-Chalkboard","text":"<p>base0</p>"},{"location":"objects/120_plant.html","title":"120_Plant","text":"<p>base0</p>"},{"location":"tasks/index.html","title":"50 RoboTwin 2.0 Tasks","text":"<p>This document introduces the 50 bimanual manipulation tasks in RoboTwin 2.0, including task videos, descriptions, average step lengths, involved objects, and success rates across different robot embodiments.</p> <p>Building on our automated task generation framework, embodiment-adaptive behavior synthesis, and the large-scale object asset library RoboTwin-OD, we construct a suite of over 50 dual-arm collaborative manipulation tasks. In addition, we support data collection and evaluation across 5 distinct robot platforms, enabling comprehensive benchmarking of manipulation policies. The complete task set is available at http://robotwin-platform.github.io/doc/tasks/.</p>"},{"location":"tasks/adjust_bottle.html","title":"Adjust Bottle","text":"Description: Pick up the bottle on the table headup with the correct arm. Average Steps: 147 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 93% 94% 34% 0% 12%"},{"location":"tasks/basic.html","title":"\u4fdd\u7814\u200b\u57fa\u7840\u77e5\u8bc6","text":""},{"location":"tasks/beat_block_hammer.html","title":"Beat Block Hammer","text":"Description: There is a hammer and a block on the table, use the arm to grab the hammer and beat the block. Average Steps: 113 (Aloha-AgileX, save_freq=15) Objects: 020_hammer, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 64% 93% 98% 15% 90%"},{"location":"tasks/blocks_ranking_rgb.html","title":"Blocks Ranking RGB","text":"Description: Place the red block, green block, and blue block in the order of red, green, and blue from left to right, placing in a row. Average Steps: 466 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 96% 97% 99% 13% 53%"},{"location":"tasks/blocks_ranking_size.html","title":"Blocks Ranking Size","text":"Description: There are three blocks on the table, the color of the blocks is random, move the blocks to the center of the table, and arrange them from largest to smallest, from left to right. Average Steps: 466 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 96% 97% 89% 7% 38%"},{"location":"tasks/click_alarmclock.html","title":"Click Alarmclock","text":"Description: Click the alarm clock's center of the top side button on the table. Average Steps: 85 (Aloha-AgileX, save_freq=15) Objects: 046_alarm-clock Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 92% 99% 100% 0% 95%"},{"location":"tasks/click_bell.html","title":"Click Bell","text":"Description: Click the bell's top center on the table. Average Steps: 85 (Aloha-AgileX, save_freq=15) Objects: 050_bell Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 100% 100% 100% 91% 100%"},{"location":"tasks/dump_bin_bigbin.html","title":"Dump Bin Bigbin","text":"Description: Grab the small bin and pour the balls into the big bin. Average Steps: 265 (Aloha-AgileX, save_freq=15) Objects: 011_dustbin, 063_tabletrashbin Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 84% 100% 84% 9% 80%"},{"location":"tasks/grab_roller.html","title":"Grab Roller","text":"Description: Use both arms to grab the roller on the table. Average Steps: 94 (Aloha-AgileX, save_freq=15) Objects: 102_roller Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 95% 69% 99% 7% 81%"},{"location":"tasks/handover_block.html","title":"Handover Block","text":"Description: Use the left arm to grasp the red block on the table, handover it to the right arm and place it on the blue pad. Average Steps: 283 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 83% 81% 0% 44% 0%"},{"location":"tasks/handover_mic.html","title":"Handover Mic","text":"Description: Use one arm to grasp the microphone on the table and handover it to the other arm. Average Steps: 223 (Aloha-AgileX, save_freq=15) Objects: 018_microphone Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 87% 98% 84% 65% 14%"},{"location":"tasks/hanging_mug.html","title":"Hanging Mug","text":"Description: Use left arm to pick the mug on the table, rotate the mug and put the mug down in the middle of the table, use the right arm to pick the mug and hang it onto the rack. Average Steps: 340 (Aloha-AgileX, save_freq=15) Objects: 039_mug, 040_rack Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 63% 73% 11% 0% 11%"},{"location":"tasks/lift_pot.html","title":"Lift Pot","text":"Description: Use arms to lift the pot. Average Steps: 112 (Aloha-AgileX, save_freq=15) Objects: 060_kitchenpot Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 27% 50% 36% 31% 40%"},{"location":"tasks/move_can_pot.html","title":"Move Can Pot","text":"Description: There is a can and a pot on the table, use one arm to pick up the can and move it to beside the pot. Average Steps: 151 (Aloha-AgileX, save_freq=15) Objects: 060_kitchenpot, 105_sauce-can Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 93% 65% 92% 96% 99%"},{"location":"tasks/move_pillbottle_pad.html","title":"Move Pillbottle Pad","text":"Description: Use one arm to pick the pillbottle and place it onto the pad. Average Steps: 147 (Aloha-AgileX, save_freq=15) Objects: 080_pillbottle, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 67% 90% 69% 47% 86%"},{"location":"tasks/move_playingcard_away.html","title":"Move Playingcard Away","text":"Description: Use the arm to pick up the playing card and move it away from the table. For example, if the playing card is on the outward side of the table, you should move it further outward side of the table. Average Steps: 120 (Aloha-AgileX, save_freq=15) Objects: 081_playingcards Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 99% 100% 100% 63% 66%"},{"location":"tasks/move_stapler_pad.html","title":"Move Stapler Pad","text":"Description: Use appropriate arm to move the stapler to a colored mat. Average Steps: 152 (Aloha-AgileX, save_freq=15) Objects: 048_stapler Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 92% 96% 89% 13% 75%"},{"location":"tasks/open_laptop.html","title":"Open Laptop","text":"Description: Use one arm to open the laptop. Average Steps: 258 (Aloha-AgileX, save_freq=15) Objects: 015_laptop Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 82% 92% 77% 23% 51%"},{"location":"tasks/open_microwave.html","title":"Open Microwave","text":"Description: Use one arm to open the microwave. Average Steps: 537 (Aloha-AgileX, save_freq=15) Objects: 044_microwave Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 96% 80% 59% 2% 23%"},{"location":"tasks/pick_diverse_bottles.html","title":"Pick Diverse Bottles","text":"Description: Pick up one bottle with one arm, and pick up another bottle with the other arm. Average Steps: 122 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 51% 2% 0% 27% 4%"},{"location":"tasks/pick_dual_bottles.html","title":"Pick Dual Bottles","text":"Description: Pick up one bottle with one arm, and pick up another bottle with the other arm. Average Steps: 127 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 92% 6% 0% 81% 7%"},{"location":"tasks/place_a2b_left.html","title":"Place A2B Left","text":"Description: Use appropriate arm to place object A on the left of object B. Average Steps: 155 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 086_woodenblock, 107_soap, 112_tea-box, 113_coffee-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 80% 88% 64% 28% 76%"},{"location":"tasks/place_a2b_right.html","title":"Place A2B Right","text":"Description: Use appropriate arm to place object A on the right of object B. Average Steps: 145 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 086_woodenblock, 107_soap, 112_tea-box, 113_coffee-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 81% 82% 64% 31% 66%"},{"location":"tasks/place_bread_basket.html","title":"Place Bread Basket","text":"Description: If there is one bread on the table, use one arm to grab the bread and put it in the basket, if there are two breads on the table, use two arms to simultaneously grab up two breads and put them in the basket. Average Steps: 231 (Aloha-AgileX, save_freq=15) Objects: 075_bread, 076_breadbasket Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 89% 88% 62% 1% 67%"},{"location":"tasks/place_bread_skillet.html","title":"Place Bread Skillet","text":"Description: If there is one bread on the table, use one arm to grab the bread and put it into the skillet. Average Steps: 162 (Aloha-AgileX, save_freq=15) Objects: 075_bread, 106_skillet Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 34% 26% 42% 0% 37%"},{"location":"tasks/place_burger_fries.html","title":"Place Burger Fries","text":"Description: Use dual arm to pick the hamburg and frenchfries and put them onto the tray. Average Steps: 242 (Aloha-AgileX, save_freq=15) Objects: 005_french-fries, 006_hamburg, 008_tray Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 97% 98% 80% 36% 74%"},{"location":"tasks/place_can_basket.html","title":"Place Can Basket","text":"Description: Use one arm to pick up the can and another arm place it in the basket. Average Steps: 255 (Aloha-AgileX, save_freq=15) Objects: 071_can, 110_basket Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 70% 28% 61% 0% 3%"},{"location":"tasks/place_cans_plasticbox.html","title":"Place Cans Plasticbox","text":"Description: Use dual arm to pick and place cans into plasticbox. Average Steps: 289 (Aloha-AgileX, save_freq=15) Objects: 062_plasticbox, 071_can Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 100% 96% 85% 0% 82%"},{"location":"tasks/place_container_plate.html","title":"Place Container Plate","text":"Description: Place the container onto the plate. Average Steps: 156 (Aloha-AgileX, save_freq=15) Objects: 002_bowl, 003_plate, 021_cup Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 89% 86% 86% 37% 81%"},{"location":"tasks/place_dual_shoes.html","title":"Place Dual Shoes","text":"Description: Use both arms to pick up the two shoes on the table and put them in the shoebox, with the shoe tip pointing to the left. Average Steps: 228 (Aloha-AgileX, save_freq=15) Objects: 007_shoe-box, 041_shoe Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 77% 31% 41% 1% 32%"},{"location":"tasks/place_empty_cup.html","title":"Place Empty Cup","text":"Description: Use an arm to place the empty cup on the coaster. Average Steps: 174 (Aloha-AgileX, save_freq=15) Objects: 019_coaster, 021_cup Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 92% 100% 100% 4% 100%"},{"location":"tasks/place_fan.html","title":"Place Fan","text":"Description: Grab the fan and place it on a colored mat, and make sure the fan is facing the robot. Average Steps: 148 (Aloha-AgileX, save_freq=15) Objects: 099_fan, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 95% 93% 83% 0% 65%"},{"location":"tasks/place_mouse_pad.html","title":"Place Mouse Pad","text":"Description: Grab the mouse and place it on a colored mat. Average Steps: 149 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 99% 89% 100% 23% 73%"},{"location":"tasks/place_object_basket.html","title":"Place Object Basket","text":"Description: Use one arm to grab the target object and put it in the basket, then use the other arm to grab the basket, and finally move the basket slightly away. Average Steps: 252 (Aloha-AgileX, save_freq=15) Objects: 057_toycar, 081_playingcards, 110_basket Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 74% 14% 61% 0% 7%"},{"location":"tasks/place_object_scale.html","title":"Place Object Scale","text":"Description: Use one arm to grab the object and put it on the scale. Average Steps: 146 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 072_electronicscale Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 78% 92% 82% 2% 76%"},{"location":"tasks/place_object_stand.html","title":"Place Object Stand","text":"Description: Use appropriate arm to place the object on the stand. Average Steps: 138 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 074_displaystand, 079_remotecontrol Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 97% 99% 81% 9% 92%"},{"location":"tasks/place_phone_stand.html","title":"Place Phone Stand","text":"Description: Pick up the phone and put it on the phone stand. Average Steps: 130 (Aloha-AgileX, save_freq=15) Objects: 077_phone, 078_phonestand Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 66% 78% 45% 53% 49%"},{"location":"tasks/place_shoe.html","title":"Place Shoe","text":"Description: Use one arm to grab the shoe from the table and place it on the mat. Average Steps: 178 (Aloha-AgileX, save_freq=15) Objects: 041_shoe, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 84% 85% 74% 7% 91%"},{"location":"tasks/press_stapler.html","title":"Press Stapler","text":"Description: Use one arm to press the stapler. Average Steps: 141 (Aloha-AgileX, save_freq=15) Objects: 048_stapler Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 98% 96% 100% 59% 72%"},{"location":"tasks/put_bottles_dustbin.html","title":"Put Bottles Dustbin","text":"Description: Use arms to grab the bottles and put them into the dustbin to the left of the table. Average Steps: 637 (Aloha-AgileX, save_freq=15) Objects: 011_dustbin, 114_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 71% 1% 0% 56% 0%"},{"location":"tasks/put_object_cabinet.html","title":"Put Object Cabinet","text":"Description: Use one arm to open the cabinet's drawer, and use another arm to put the object on the table to the drawer. Average Steps: 274 (Aloha-AgileX, save_freq=15) Objects: 036_cabinet, 047_mouse, 048_stapler, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 107_soap, 112_tea-box, 113_coffee-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 14% 24% 55% 0% 0%"},{"location":"tasks/rotate_qrcode.html","title":"Rotate QRcode","text":"Description: Use arm to catch the qrcode board on the table, pick it up and rotate to let the qrcode face towards the robot. Average Steps: 155 (Aloha-AgileX, save_freq=15) Objects: 070_paymentsign Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 75% 74% 94% 0% 67%"},{"location":"tasks/scan_object.html","title":"Scan Object","text":"Description: Use one arm to pick the scanner and use the other arm to pick the object, and use the scanner to scan the object. Average Steps: 170 (Aloha-AgileX, save_freq=15) Objects: 024_scanner, 112_tea-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 4% 45% 26% 0% 19%"},{"location":"tasks/shake_bottle.html","title":"Shake Bottle","text":"Description: Shake the bottle with proper arm. Average Steps: 246 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 89% 94% 85% 74% 97%"},{"location":"tasks/shake_bottle_horizontally.html","title":"Shake Bottle Horizontally","text":"Description: Shake the bottle horizontally with proper arm. Average Steps: 276 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 90% 94% 85% 74% 98%"},{"location":"tasks/stack_blocks_three.html","title":"Stack Blocks Three","text":"Description: There are three blocks on the table, the color of the blocks is red, green and blue. Move the blocks to the center of the table, and stack the blue block on the green block, and the green block on the red block. Average Steps: 481 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 94% 96% 80% 0% 51%"},{"location":"tasks/stack_blocks_two.html","title":"Stack Blocks Two","text":"Description: There are two blocks on the table, the color of the blocks is red, green. Move the blocks to the center of the table, and stack the geen block on the red block. Average Steps: 316 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 98% 99% 96% 2% 68%"},{"location":"tasks/stack_bowls_three.html","title":"Stack Bowls Three","text":"Description: Stack the three bowls on top of each other. Average Steps: 476 (Aloha-AgileX, save_freq=15) Objects: 002_bowl Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 43% 57% 82% 0% 81%"},{"location":"tasks/stack_bowls_two.html","title":"Stack Bowls Two","text":"Description: Stack the two bowls on top of each other. Average Steps: 313 (Aloha-AgileX, save_freq=15) Objects: 002_bowl Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 78% 82% 88% 4% 94%"},{"location":"tasks/stamp_seal.html","title":"Stamp Seal","text":"Description: Grab the stamp and stamp onto the specific color mat. Average Steps: 151 (Aloha-AgileX, save_freq=15) Objects: 100_seal, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 56% 91% 4% 37% 100%"},{"location":"tasks/template.html","title":"Template","text":"<p>\u200b\u683c\u5f0f\u200b\u4f1a\u200b\u6709\u200b\u4e13\u4eba\u200b\u8c03\u6574\u200b\uff0c\u200b\u5404\u4f4d\u200b\u53ea\u200b\u9700\u8981\u200b\u586b\u5199\u5185\u5bb9\u200b\u5373\u53ef\u200b</p>"},{"location":"tasks/template.html#-xxxx","title":"\u79f0\u8c13\u200b - \u200b\u53bb\u5904\u200bxxxx\u200b\u5c4a\u200b, \u200b\u5b66\u9662\u200b, \u200b\u4e13\u4e1a","text":"<p>\uff08\u200b\u9009\u586b\u200b\uff09\u200b\u60f3\u200b\u9001\u7ed9\u200b\u5b66\u5f1f\u200b\u5b66\u59b9\u200b\u7684\u200b\u4e00\u53e5\u200b\u8bdd\u200b: ...</p>"},{"location":"tasks/template.html#_1","title":"\u80cc\u666f","text":"<p>GPA: \u00a0\u00a0\u00a0\u00a0\u200b\u7b49\u7ea7\u5236\u200b: 4.00 / 4.5, \u200b\u767e\u5206\u5236\u200b: 88.00 / 100, \u200b\u4e13\u4e1a\u200b\u6392\u540d\u200b: 5 / 100</p> <p>\u200b\u7ade\u8d5b\u200b: \u00a0\u00a0\u00a0\u00a0- xxx\u200b\u7ade\u8d5b\u200bxxx\u200b\u5956\u200b</p> <p>\u200b\u79d1\u7814\u200b: \u00a0\u00a0\u00a0\u00a0- \u200b\u8bba\u6587\u200b\u53d1\u8868\u200b</p> <p>\u200b\u5b9e\u4e60\u200b: \u00a0\u00a0\u00a0\u00a0- xxx\u200b\u516c\u53f8\u200b, \u200b\u5b9e\u4e60\u200b\u5c97\u4f4d\u200b, \u200b\u5b9e\u4e60\u200b\u671f\u9650\u200b</p> <p>\u200b\u9879\u76ee\u200b: \u00a0\u00a0\u00a0\u00a0- xxx\u200b\u9879\u76ee\u200b, \u200b\u4e3b\u8981\u200b\u8d21\u732e\u200b\u7b49\u200b</p> <p>\u200b\u4e2a\u4eba\u200b\u8363\u8a89\u200b: \u00a0\u00a0\u00a0\u00a0- \u200b\u5956\u5b66\u91d1\u200b\u7b49\u200b</p> <p>\u200b\u4e2a\u4eba\u200b\u8d23\u4efb\u200b: \u00a0\u00a0\u00a0\u00a0- \u200b\u73ed\u5e72\u200b\u3001\u200b\u793e\u56e2\u200b\u8d1f\u8d23\u4eba\u200b\u7b49\u200b</p>"},{"location":"tasks/template.html#_2","title":"\u7533\u8bf7\u200b\u60c5\u51b5","text":"<p>ps: \u200b\u53ef\u4ee5\u200b\u5217\u51fa\u200b\u4f60\u200b\u7684\u200b\u7533\u8bf7\u200b\u60c5\u51b5\u200b (\u274c\u2705), \u200b\u53ef\u200b\u53c2\u8003\u200b2024\u200b\u5c4a\u200b\u4fdd\u7814\u200b\u7ecf\u9a8c\u200b\u8d34\u200b\uff08\u200b\u897f\u4ea4\u200b\uff0c\u200b\u4e0a\u4ea4\u200b\u7535\u9662\u200b\uff0c\u200b\u54c8\u5de5\u5927\u200b\u7535\u4fe1\u200b\uff0c\u200b\u6d59\u63a7\u200b\uff0c\u200b\u4e2d\u79d1\u5927\u200b\uff0c\u200b\u81ea\u52a8\u5316\u200b\u6240\u200b\uff09, \u200b\u4f8b\u5982\u200b\u6e05\u6df1\u200bCS\u274c, \u200b\u5317\u5927\u200b\u8f6f\u5fae\u200b\u2705</p> \u200b\u7533\u8bf7\u200b\u9879\u76ee\u200b \u200b\u7ed3\u679c\u200b \u200b\u4f8b\u200b\uff1a\u200b\u5357\u200b\u79d1\u5927\u200bCS \u2705 <p>\u200b\u6700\u7ec8\u200b\u53bb\u5411\u200b (\u200b\u9662\u6821\u200b+\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b): ...</p>"},{"location":"tasks/template.html#_3","title":"\u7ecf\u5386\u200b\u5206\u4eab","text":"<p>ps: \u200b\u53ef\u4ee5\u200b\u8be6\u7ec6\u200b\u5206\u4eab\u200b\u4f60\u200b\u7684\u200b\u7533\u8bf7\u200b\u7ecf\u5386\u200b\uff0c\u200b\u7a7f\u63d2\u200b\u611f\u609f\u200b\u3002\u200b\u5373\u4f7f\u200b\u662f\u200b\u5931\u8d25\u200b\u7684\u200b\u7ecf\u5386\u200b\u4e5f\u200b\u662f\u200b\u975e\u5e38\u200b\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002</p>"},{"location":"tasks/template.html#_4","title":"\u603b\u7ed3","text":"<p>\u200b\u5bf9\u200b\u7533\u8bf7\u200b\u5b63\u200b\u7684\u200b\u7ecf\u5386\u200b\u8fdb\u884c\u200b\u603b\u7ed3\u200b\u3002</p>"},{"location":"tasks/template.html#_5","title":"\u8054\u7cfb\u65b9\u5f0f\u200b\uff08\u200b\u9009\u586b\u200b\uff09","text":"<p>\u00a0\u00a0\u00a0\u00a0- \u200b\u4e3b\u9875\u200b: xxx \u00a0\u00a0\u00a0\u00a0- \u200b\u90ae\u7bb1\u200b: xxx \u00a0\u00a0\u00a0\u00a0- \u200b\u5fae\u4fe1\u200b: xxx</p>"},{"location":"tasks/template.html#_6","title":"\u5907\u6ce8","text":"<p>\u200b\u5173\u4e8e\u200b\u4ee5\u4e0a\u200b\u4fe1\u606f\u200b\u586b\u5199\u200b\u4e2d\u200b\u5b58\u5728\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\u7684\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6b64\u200b\u5217\u51fa\u200b\u3002</p>"},{"location":"tasks/turn_switch.html","title":"Turn Switch","text":"Description: Use the robotic arm to click the switch. Average Steps: 95 (Aloha-AgileX, save_freq=15) Objects: 056_switch Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Success Rate 74% 3% 36% 81% 10%"},{"location":"usage/index.html","title":"RoboTwin 2.0 Usage Guide","text":"<p>This documentation provides a comprehensive guide to using RoboTwin 2.0, covering environment setup, data collection and configuration, policy deployment, usage of demo policies, automatic code generation for new tasks, API tutorial, language instruction generation, and digital asset annotation.</p>"},{"location":"usage/ACT.html","title":"ACT (Action Chunking Transformer)","text":""},{"location":"usage/ACT.html#1-install","title":"1. Install","text":"<pre><code>pip install pyquaternion pyyaml rospkg pexpect mujoco==2.3.7 dm_control==1.0.14 opencv-python matplotlib einops packaging h5py ipython\n\ncd adetr &amp;&amp; pip install -e .\n</code></pre>"},{"location":"usage/ACT.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the format required for ACT training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data.</p> <pre><code>bash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n</code></pre>"},{"location":"usage/ACT.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. By default, the model is trained for 6,000 steps.</p> <pre><code>bash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${gpu_id}\n</code></pre>"},{"location":"usage/ACT.html#4-eval-policy","title":"4. Eval Policy","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>ckpt_setting</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/API.html","title":"API for Controlling Mechanical Arms","text":"<p>The API can be used to control one or two robotic arms to perform operations such as grasping, placing, moving, and returning to the origin. Each arm is identified by an <code>ArmTag</code>, which can be <code>\"left\"</code> or <code>\"right\"</code>. Actions are generated in sequences and executed together via the <code>move()</code> method.</p>"},{"location":"usage/API.html#1-class-structure","title":"1. Class Structure","text":"<ul> <li><code>self</code>: The instance.</li> <li><code>ArmTag</code>: A custom type representing a robotic arm. It supports comparison with strings: <code>ArmTag(\"left\") == \"left\"</code> returns <code>True</code>. You can obtain the opposite arm using <code>ArmTag(\"left\").opposite</code>, i.e., <code>ArmTag(\"left\").opposite == \"right\"</code> returns <code>True</code>.</li> <li><code>Actor</code>: The object being manipulated. Provides methods to retrieve key points (contact point <code>contact_point</code>, functional point <code>functional_point</code>, target point <code>target_point</code>) and its current global pose.</li> <li><code>Action</code>: A sequence of actions for controlling the arm. You only need to know that it can be executed via the <code>move()</code> function.</li> </ul>"},{"location":"usage/API.html#2-api-reference","title":"2. API Reference","text":""},{"location":"usage/API.html#21-moveself-actions_by_arm1-tuplearmtag-listaction-actions_by_arm2-tuplearmtag-listaction-none","title":"2.1 <code>move(self, actions_by_arm1: tuple[ArmTag, list[Action]], actions_by_arm2: tuple[ArmTag, list[Action]] = None)</code>","text":""},{"location":"usage/API.html#211-description","title":"2.1.1 Description","text":"<p>Executes action sequences on one or both robotic arms simultaneously.</p>"},{"location":"usage/API.html#212-parameters","title":"2.1.2 Parameters","text":"<ul> <li><code>actions_by_arm1</code>: Action sequence for the first arm, formatted as <code>(arm_tag, [action1, action2, ...])</code></li> <li><code>actions_by_arm2</code>: Optional, action sequence for the second arm</li> </ul>"},{"location":"usage/API.html#213-notes","title":"2.1.3 Notes","text":"<ul> <li>The same <code>ArmTag</code> cannot be passed twice.</li> <li>All actions must have been pre-generated.</li> </ul>"},{"location":"usage/API.html#214-example","title":"2.1.4 Example","text":"<pre><code># One arm grasps a bottle, the other moves back to avoid interference\nself.move(\n    self.grasp_actor(self.bottle, arm_tag=arm_tag),\n    self.back_to_origin(arm_tag=arm_tag.opposite)\n)\n</code></pre>"},{"location":"usage/API.html#22-grasp_actorself-actor-actor-arm_tag-armtag-pre_grasp_dis01-grasp_dis0-gripper_pos0-contact_point_idnone-tuplearmtag-listaction","title":"2.2 <code>grasp_actor(self, actor: Actor, arm_tag: ArmTag, pre_grasp_dis=0.1, grasp_dis=0, gripper_pos=0., contact_point_id=None) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#221-description","title":"2.2.1 Description","text":"<p>Generates a sequence of actions to pick up the specified <code>Actor</code>.</p>"},{"location":"usage/API.html#222-parameters","title":"2.2.2 Parameters","text":"<ul> <li><code>actor</code>: The object to grasp</li> <li><code>arm_tag</code>: Which arm to use</li> <li><code>pre_grasp_dis</code>: Pre-grasp distance (default 0.1 meters), the arm will move to this position first</li> <li><code>grasp_dis</code>: Grasping distance (default 0 meters), the arm moves from the pre-grasp position to this position and then closes the gripper</li> <li><code>gripper_pos</code>: Gripper closing position (default 0, fully closed)</li> <li><code>contact_point_id</code>: Optional list of contact point IDs; if not provided, the best grasping point is selected automatically</li> </ul>"},{"location":"usage/API.html#223-returns","title":"2.2.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the grasp actions.</p>"},{"location":"usage/API.html#224-example","title":"2.2.4 Example","text":"<pre><code># Select appropriate grasp point based on arm_tag and grasp the cup\nself.move(\n    self.grasp_actor(\n        self.cup, arm_tag=arm_tag,\n        pre_grasp_dis=0.1,\n        contact_point_id=[0, 2][int(arm_tag=='left')]\n    )\n)\n</code></pre>"},{"location":"usage/API.html#23-place_actorself-actor-actor-arm_tag-armtag-target_pose-list-npndarray-functional_point_id-int-none-pre_dis01-dis002-is_opentrue-kwargs-tuplearmtag-listaction","title":"2.3 <code>place_actor(self, actor: Actor, arm_tag: ArmTag, target_pose: list | np.ndarray, functional_point_id: int = None, pre_dis=0.1, dis=0.02, is_open=True, **kwargs) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#231-description","title":"2.3.1 Description","text":"<p>Places a currently held object at a specified target pose.</p>"},{"location":"usage/API.html#232-parameters","title":"2.3.2 Parameters","text":"<ul> <li><code>actor</code>: The currently held object</li> <li><code>arm_tag</code>: The arm holding the object</li> <li><code>target_pose</code>: Target position/orientation, length 3 or 7 (xyz + optional quaternion)</li> <li><code>functional_point_id</code>: Optional ID of the functional point; if provided, aligns this point to the target, otherwise aligns the base of the object</li> <li><code>pre_dis</code>: Pre-place distance (default 0.1 meters), arm moves to this position first</li> <li><code>dis</code>: Final placement distance (default 0.02 meters), arm moves from pre-place to this location, then opens the gripper</li> <li><code>is_open</code>: Whether to open the gripper after placing (default True)</li> <li><code>**kwargs</code>: Other optional parameters:<ul> <li><code>constrain : {'free', 'align', 'auto'}, default='auto'</code> Alignment strategy:<ul> <li><code>free</code>: Only forces the object's z-axis to align with the target point's z-axis, other axes are determined by projection.</li> <li><code>align</code>: Forces all axes of the object to align with all axes of the target point.</li> <li><code>auto</code>: Automatically selects a suitable placement pose based on grasp direction (vertical or horizontal).</li> </ul> </li> <li><code>align_axis : list of np.ndarray or np.ndarray or list, optional</code> Vectors or vector list in world coordinates to align with. For example, <code>[1, 0, 0]</code> or <code>[[1, 0, 0], [0, 1, 0]]</code>. If multiple vectors are provided, the one with the smallest dot product with the current actor axis will be chosen for alignment.</li> <li><code>actor_axis : np.ndarray or list, default=[1, 0, 0]</code> The second object axis used for alignment (the first is the z-axis which will be forced to align). Typically used for auxiliary alignment (especially when <code>constrain == 'align'</code>).</li> <li><code>actor_axis_type : {'actor', 'world'}, default='actor'</code> Specifies whether <code>actor_axis</code> is relative to the object coordinate system or world coordinate system.</li> <li><code>pre_dis_axis : {'grasp', 'fp'} or np.ndarray or list, default='grasp'</code> Specifies the pre-placement offset direction:<ul> <li><code>grasp</code>: Offset along the grasp direction (i.e., opposite to the end-effector pointing towards the object center).</li> <li><code>fp</code>: Offset along the target point's z-axis direction.</li> <li>Custom vectors can also be provided to represent the offset direction.</li> </ul> </li> </ul> </li> </ul>"},{"location":"usage/API.html#233-returns","title":"2.3.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the place actions.</p>"},{"location":"usage/API.html#234-example","title":"2.3.4 Example","text":"<pre><code># Place the cup at the specified pose, aligning functional point 0\nself.move(\n    self.place_actor(\n        self.cup, arm_tag=arm_tag,\n        target_pose=target_pose,\n        functional_point_id=0,\n        pre_dis=0.05\n    )\n)\n</code></pre>"},{"location":"usage/API.html#place-the-fan-at-a-specific-pose-aligning-it-to-the-world-coordinate-systems-y-axis-selfmove-selfplace_actor-selffan-arm_tagarm_tag-target_posetarget_pose-constrainalign-align_axis0-1-0-pre_dis004-dis0005","title":"<pre><code># Place the fan at a specific pose, aligning it to the world coordinate system's -y axis\nself.move(\n    self.place_actor(\n        self.fan, arm_tag=arm_tag, target_pose=target_pose,\n        constrain='align',\n        align_axis=[0, -1, 0],\n        pre_dis=0.04, dis=0.005\n    )\n)\n</code></pre>","text":""},{"location":"usage/API.html#24-move_by_displacementself-arm_tag-armtag-x0-y0-z0-quatnone-move_axisworld-tuplearmtag-listaction","title":"2.4 <code>move_by_displacement(self, arm_tag: ArmTag, x=0., y=0., z=0., quat=None, move_axis='world') -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#241-description","title":"2.4.1 Description","text":"<p>Moves the end-effector of the specified arm along relative directions and sets its orientation.</p>"},{"location":"usage/API.html#242-parameters","title":"2.4.2 Parameters","text":"<ul> <li><code>arm_tag</code>: The arm to control</li> <li><code>x</code>, <code>y</code>, <code>z</code>: Displacement along each axis (in meters)</li> <li><code>quat</code>: Optional quaternion specifying the target orientation; if not set, uses current orientation</li> <li><code>move_axis</code>: <code>'world'</code> means displacement is in world coordinates, <code>'arm'</code> means displacement is in local coordinates</li> </ul>"},{"location":"usage/API.html#243-returns","title":"2.4.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the move-by-displacement actions.</p>"},{"location":"usage/API.html#244-example","title":"2.4.4 Example","text":"<pre><code># Lift the arm slightly to help move objects\nself.move(\n    self.move_by_displacement(\n        arm_tag=arm_tag,\n        z=0.1, move_axis='world'\n    )\n)\n</code></pre>"},{"location":"usage/API.html#25-move_to_poseself-arm_tag-armtag-target_pose-list-tuplearmtag-listaction","title":"2.5 <code>move_to_pose(self, arm_tag: ArmTag, target_pose: list) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#251-description","title":"2.5.1 Description","text":"<p>Moves the end-effector of the specified arm to a specific absolute pose.</p>"},{"location":"usage/API.html#252-parameters","title":"2.5.2 Parameters","text":"<ul> <li><code>arm_tag</code>: The arm to control</li> <li><code>target_pose</code>: Absolute position and/or orientation, length 3 or 7 (xyz + optional quaternion)</li> </ul>"},{"location":"usage/API.html#253-returns","title":"2.5.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the move-to-pose actions.</p>"},{"location":"usage/API.html#254-example","title":"2.5.4 Example","text":"<pre><code>target_pose = self.get_arm_pose(arm_tag=arm_tag)\nif arm_tag == 'left':\n    # Set specific position and orientation for left arm\n    target_pose[:2] = [-0.1, -0.05]\n    target_pose[2] -= 0.05\n    target_pose[3:] = [-0.707, 0, -0.707, 0]\nelse:\n    # Set specific position and orientation for right arm\n    target_pose[:2] = [0.1, -0.05]\n    target_pose[2] -= 0.05\n    target_pose[3:] = [0, 0.707, 0, -0.707]\n\n# Move the skillet to the defined target pose\nself.move(\n    self.move_to_pose(arm_tag=arm_tag, target_pose=target_pose)\n)\n</code></pre>"},{"location":"usage/API.html#26-close_gripperself-arm_tag-armtag-pos0-tuplearmtag-listaction","title":"2.6 <code>close_gripper(self, arm_tag: ArmTag, pos=0.) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#261-description","title":"2.6.1 Description","text":"<p>Closes the gripper of the specified arm.</p>"},{"location":"usage/API.html#262-parameters","title":"2.6.2 Parameters","text":"<ul> <li><code>arm_tag</code>: Which arm's gripper to close</li> <li><code>pos</code>: Gripper position (0 = fully closed)</li> </ul>"},{"location":"usage/API.html#263-returns","title":"2.6.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the gripper-close action.</p>"},{"location":"usage/API.html#264-example","title":"2.6.4 Example","text":"<pre><code>self.move(\n    self.close_gripper(arm_tag=arm_tag)\n)\n</code></pre>"},{"location":"usage/API.html#27-open_gripperself-arm_tag-armtag-pos1-tuplearmtag-listaction","title":"2.7 <code>open_gripper(self, arm_tag: ArmTag, pos=1.) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#271-description","title":"2.7.1 Description","text":"<p>Opens the gripper of the specified arm.</p>"},{"location":"usage/API.html#272-parameters","title":"2.7.2 Parameters","text":"<ul> <li><code>arm_tag</code>: Which arm's gripper to open</li> <li><code>pos</code>: Gripper position (1 = fully open)</li> </ul>"},{"location":"usage/API.html#273-returns","title":"2.7.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the gripper-open action.</p>"},{"location":"usage/API.html#274-example","title":"2.7.4 Example","text":"<pre><code>self.move(\n    self.open_gripper(arm_tag=arm_tag)\n)\n</code></pre>"},{"location":"usage/API.html#28-back_to_originself-arm_tag-armtag-tuplearmtag-listaction","title":"2.8 <code>back_to_origin(self, arm_tag: ArmTag) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#281-description","title":"2.8.1 Description","text":"<p>Returns the specified arm to its predefined initial position.</p>"},{"location":"usage/API.html#282-parameters","title":"2.8.2 Parameters","text":"<ul> <li><code>arm_tag</code>: The arm to return to origin</li> </ul>"},{"location":"usage/API.html#283-returns","title":"2.8.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the return-to-origin action.</p>"},{"location":"usage/API.html#284-example","title":"2.8.4 Example","text":"<pre><code>self.move(self.back_to_origin(arm_tag=arm_tag))\n</code></pre>"},{"location":"usage/API.html#29-get_arm_poseself-arm_tag-armtag-listfloat","title":"2.9 <code>get_arm_pose(self, arm_tag: ArmTag) -&gt; list[float]</code>","text":""},{"location":"usage/API.html#291-description","title":"2.9.1 Description","text":"<p>Gets the current pose of the end-effector of the specified arm.</p>"},{"location":"usage/API.html#292-parameters","title":"2.9.2 Parameters","text":"<ul> <li><code>arm_tag</code>: Which arm to query</li> </ul>"},{"location":"usage/API.html#293-returns","title":"2.9.3 Returns","text":"<p>A list of 7 floats: <code>[x, y, z, qw, qx, qy, qz]</code>, representing position and orientation.</p>"},{"location":"usage/API.html#294-example","title":"2.9.4 Example","text":"<pre><code>pose = self.get_arm_pose(ArmTag(\"left\"))\n</code></pre>"},{"location":"usage/API.html#3-methods-of-the-actor-class","title":"3. Methods of the <code>Actor</code> Class","text":"<p><code>Actor</code> is the object being manipulated by the robotic arms. It provides methods to retrieve key points and its current global pose. The <code>Actor</code> class has the following data points:</p> <ul> <li>Target Point <code>target_point</code>: Special points available during planning (e.g., handle of a cup)</li> <li>Contact Point <code>contact_point</code>: Position where the robotic arm grasps the object (e.g., rim of a cup)</li> <li>Functional Point <code>functional_point</code>: Position where the object interacts with other objects (e.g., head of a hammer)</li> <li>Orientation Point <code>orientation_point</code>: Specifies the orientation of the object (e.g., toe of a shoe pointing left)</li> </ul> <p>These methods can be called on <code>Actor</code> objects:</p>"},{"location":"usage/API.html#31-get_contact_pointself-idx-int-listfloat","title":"3.1 <code>get_contact_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th contact point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#32-get_functional_pointself-idx-int-listfloat","title":"3.2 <code>get_functional_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th functional point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#33-get_target_pointself-idx-int-listfloat","title":"3.3 <code>get_target_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th target point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#34-get_orientation_pointself-idx-int-listfloat","title":"3.4 <code>get_orientation_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th orientation point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#35-get_poseself-sapienpose","title":"3.5 <code>get_pose(self) -&gt; sapien.Pose</code>","text":"<p>Returns the global pose of the object in SAPIEN (<code>.p</code> is position, <code>.q</code> is orientation)</p> <p>If the actor was created with method that contains \"urdf\"(e.g. <code>create_rand_sapien_urdf_actor</code>), it will be a subclass of <code>Actor</code> called <code>ArticulationActor</code>, with the following additional methods:</p>"},{"location":"usage/API.html#36-get_qlimitsself-listtuplefloat-float","title":"3.6 <code>get_qlimits(self) -&gt; list[tuple[float, float]]</code>","text":"<p>Returns a list of joint limits, where each joint limit is a tuple <code>(min, max)</code>.</p>"},{"location":"usage/API.html#37-get_qposself-listfloat","title":"3.7 <code>get_qpos(self) -&gt; list[float]</code>","text":"<p>Returns the current positions (rotational/positional) of all joints.</p>"},{"location":"usage/API.html#38-get_qvelself-listfloat","title":"3.8 <code>get_qvel(self) -&gt; list[float]</code>","text":"<p>Returns the current velocities of all joints.</p>"},{"location":"usage/DP.html","title":"DP (Diffusion Policy)","text":""},{"location":"usage/DP.html#1-install","title":"1. Install","text":"<pre><code>pip install zarr==2.12.0 wandb ipdb gpustat dm_control omegaconf hydra-core==1.2.0 dill==0.3.5.1 einops==0.4.1 diffusers==0.11.1 numba==0.56.4 moviepy imageio av matplotlib termcolor\ncd policy/DP\npip install -e .\ncd ../..\n</code></pre>"},{"location":"usage/DP.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the Zarr format required for DP training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data.</p> <pre><code>bash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n</code></pre>"},{"location":"usage/DP.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. By default, the model is trained for 600 steps. The <code>action_dim</code> parameter defines the dimensionality of the robot\u2019s action space \u2014 for example, it is 14 for the <code>aloha-agilex</code> embodiment.</p> <pre><code>bash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${action_dim} ${gpu_id}\n</code></pre>"},{"location":"usage/DP.html#4-eval-policy","title":"4. Eval Policy","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>ckpt_setting</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/DP3.html","title":"DP3 (3D Diffusion Policy)","text":"<p>Since DP3 is a 3D policy that requires point cloud input, please make sure to set <code>data_type/pointcloud</code> to <code>true</code> during data collection.</p>"},{"location":"usage/DP3.html#1-install","title":"1. Install","text":"<pre><code>cd policy/DP3/3D-Diffusion-Policy &amp;&amp; pip install -e . &amp;&amp; cd ..\npip install zarr==2.12.0 wandb ipdb gpustat dm_control omegaconf hydra-core==1.2.0 dill==0.3.5.1 einops==0.4.1 diffusers==0.11.1 numba==0.56.4 moviepy imageio av matplotlib termcolor\n</code></pre>"},{"location":"usage/DP3.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the Zarr format required for DP3 training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data.</p> <pre><code>bash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n</code></pre>"},{"location":"usage/DP3.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. By default, the model is trained for 3,000 steps.</p> <pre><code>bash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${gpu_id}\n</code></pre>"},{"location":"usage/DP3.html#4-eval-policy","title":"4. Eval Policy","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>ckpt_setting</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/DexVLA.html","title":"DexVLA","text":"<p>Coming Soon</p>"},{"location":"usage/Pi0.html","title":"OpenPI","text":""},{"location":"usage/Pi0.html#1-1-environment-setup","title":"1. 1. Environment Setup","text":"<p>Follow the official OpenPI website to configure the environment. The OpenPI + RoboTwin environment has already been pre-configured in a file, so no additional setup is needed.</p> <p><pre><code>GIT_LFS_SKIP_SMUDGE=1 uv sync\n</code></pre> install pytorch3d\uff1a <pre><code>conda deactivate\nsource .venv/bin/activate\n# At this point, you should be in the (openpi) environment\npip install portalocker tabulate yacs iopath fvcore\ncd ../../third_party/pytorch3d_simplified/\npip install .\n# if error:\npython setup.py install\npip uninstall pytorch3d\npip install .\n\ncd ../../policy/pi0/\nbash\n</code></pre> Note that the uv environment will only take effect when the current directory is set as the root directory. Or you can use uder commands: <pre><code>source .venv/bin/activate\n</code></pre></p> <p>Next, locate <code>mplib</code> within the <code>(openpi)</code> environment: <pre><code>uv run where_is_package.py\n</code></pre> Then, based on the printed output, modify the corresponding <code>mplib</code> as needed: Modification Reference</p>"},{"location":"usage/Pi0.html#2-2-generate-robotwin-data","title":"2. 2. Generate RoboTwin Data","text":"<p>See RoboTwin Tutorial (Usage Section) for more details.</p>"},{"location":"usage/Pi0.html#3-3-generate-openpi-data","title":"3. 3. Generate openpi Data","text":"<p>First, convert RoboTwin data to HDF5 data type. <pre><code>bash process_data_pi0.sh ${task_name} ${task_config} ${expert_data_num}\n</code></pre></p> <p>After generating the HDF5 data, we can directly generate the LerobotDataset format data for OpenPI. If you want to create a multi-task dataset, please place the corresponding task folders according to the example below.</p> <pre><code>training_data/  \n\u251c\u2500\u2500 my_task\n|   \u251c\u2500\u2500task_1\n|   \u251c\u2500\u2500 task_2 \n|   \u251c\u2500\u2500...\n</code></pre> <pre><code># hdf5_path: The path to the generated HDF5 data (e.g., ./training_data/my_task/)\n# repo_id: The name of the dataset (e.g., my_example_task)\nbash generate.sh ${hdf5_path} ${repo_id}\n</code></pre> <p>Generating the dataset can take some time\u2014about half an hour for 100 sets, so feel free to take a break.</p>"},{"location":"usage/Pi0.html#4-note","title":"4. note!","text":"<p>If you don't have enough disk space under the <code>~/.cache</code> path, please use the following command to set a different cache directory with sufficient space: <pre><code>export LEROBOT_HOME=/path/to/your/cache\n</code></pre></p> <p>This is because generating the <code>lerobotdataset</code> will require a large amount of space.And the datasets will be writed into <code>$LEROBOT_HOME</code>.</p>"},{"location":"usage/Pi0.html#5-4-write-the-corresponding-train_config","title":"5. 4. Write the Corresponding <code>train_config</code>","text":"<p>In <code>src/openpi/training/config.py</code>, there is a dictionary called <code>_CONFIGS</code>. You can modify two pre-configured PI0 configurations I\u2019ve written: <code>pi0_base_aloha_robotwin_lora</code> <code>pi0_fast_aloha_robotwin_lora</code> <code>pi0_base_aloha_robotwin_full</code> <code>pi0_fast_aloha_robotwin_full</code></p> <p>You only need to write <code>repo_id</code>  on your datasets. If you want to change the <code>name</code> in <code>TrainConfig</code>, please include <code>fast</code> if you choose <code>pi_fast_base</code> model. If your do not have enough gpu memory, you can set fsdp_devices, refer to config.py line <code>src/openpi/training/config.py</code> line 353.</p>"},{"location":"usage/Pi0.html#6-5-finetune-model","title":"6. 5. Finetune model","text":"<p>Simply modify the <code>repo_id</code> to fine-tune the model: <pre><code># compute norm_stat for dataset\nuv run scripts/compute_norm_stats.py --config-name ${train_config_name}\n# train_config_name: The name corresponding to the config in _CONFIGS, such as pi0_base_aloha_full\n# model_name: You can choose any name for your model\n# gpu_use: if not using multi gpu,set to gpu_id like 0;else set like 0,1,2,3\nbash finetune.sh ${train_config_name} ${model_name} ${gpu_use}\n</code></pre></p> Training mode Memory Required Example GPU Fine-Tuning (LoRA) &gt; 46 GB A6000(48G) Fine-Tuning (Full) &gt; 100 GB 2*A100 (80GB) / 2*H100 <p>If your GPU memory is insufficient, please set the <code>fsdp_devices</code> parameter according to the following GPU memory reference, or reduce the <code>batch_size</code> parameter. Or you can try setting <code>XLA_PYTHON_CLIENT_PREALLOCATE=false</code> in <code>finetune.sh</code>, it will cost lower gpu memory, but make training speed slower.</p> <p>The default <code>batch_size</code> is 32 in the table below. | GPU memory | Model type | GPU num |fsdp_devices | Example GPU | | ----- | ----- | ----- |-----| ----- | |  24G | lora | 2 | 2 | 4090(24G)  | |  40G | lora | 2 | 2 | A100(40G)  | |  48G | lora | 1 | 1 | A6000(48G) | |  40G | full | 4 | 4 | A100(40G)  | |  80G | full | 2 | 2 | A100(80G)  |</p>"},{"location":"usage/Pi0.html#7-5-eval-on-robotwin","title":"7. 5. Eval on RoboTwin","text":"<pre><code># ckpt_path like: policy/openpi/checkpoints/pi0_base_aloha_robotwin_full/my_task/30000\nbash eval.sh ${task_name} ${task_config} ${train_config_name} ${model_name} ${seed} ${gpu_id}\n</code></pre>"},{"location":"usage/RDT.html","title":"RDT","text":"<p>****# RDT</p>"},{"location":"usage/RDT.html#1-environment-setup","title":"1. Environment Setup","text":"<p>The conda environment for RDT with RoboTwin is identical to the official RDT environment. Please follow the (RDT official documentation) to install the environment and directly overwrite the RoboTwin virtual environment in INSTALLATION.md.</p> <pre><code># Make sure python version == 3.10\nconda activate RoboTwin\n\n# Install pytorch\n# Look up https://pytorch.org/get-started/previous-versions/ with your cuda version for a correct command\npip install torch==2.1.0 torchvision==0.16.0  --index-url https://download.pytorch.org/whl/cu121\n\n# Install packaging\npip install packaging==24.0\npip install ninja\n# Verify Ninja --&gt; should return exit code \"0\"\nninja --version; echo $?\n# Install flash-attn\npip install flash-attn==2.7.2.post1 --no-build-isolation\n\n# Install other prequisites\npip install -r requirements.txt\n# If you are using a PyPI mirror, you may encounter issues when downloading tfds-nightly and tensorflow. \n# Please use the official source to download these packages.\n# pip install tfds-nightly==4.9.4.dev202402070044 -i  https://pypi.org/simple\n# pip install tensorflow==2.15.0.post1 -i  https://pypi.org/simple\n</code></pre>"},{"location":"usage/RDT.html#1-2-download-model","title":"1. 2. Download Model","text":"<pre><code># In the RoboTwin/policy directory\ncd ../weights\nmkdir RDT &amp;&amp; cd RDT\n# Download the models used by RDT\nhuggingface-cli download google/t5-v1_1-xxl --local-dir t5-v1_1-xxl\nhuggingface-cli download google/siglip-so400m-patch14-384 --local-dir siglip-so400m-patch14-384\nhuggingface-cli download robotics-diffusion-transformer/rdt-1b --local-dir rdt-1b\n</code></pre>"},{"location":"usage/RDT.html#2-3-collect-robotwin-data","title":"2. 3. Collect RoboTwin Data","text":"<p>See RoboTwin Tutorial (Usage Section) for more details.</p>"},{"location":"usage/RDT.html#3-4-generate-hdf5-data","title":"3. 4. Generate HDF5 Data","text":"<p>HDF5 is the data format required for RDT training.</p> <p>First, create the <code>processed_data</code> and <code>training_data</code> folders in the <code>policy/RDT</code> directory: <pre><code>mkdir processed_data &amp;&amp; mkdir training_data\n</code></pre></p> <p>Then, run the following in the <code>RDT/</code> root directory:</p> <pre><code>bash process_data_rdt.sh ${task_name} ${task_config} ${expert_data_num} ${gpu_id}\n</code></pre> <p>If success, you will find the <code>${task_name}-${task_config}-${expert_data_num}</code> folder under <code>policy/RDT/processed_data</code>.</p>"},{"location":"usage/RDT.html#4-4-generate-configuration-file","title":"4. 4. Generate Configuration File","text":"<p>A <code>$model_name</code> manages the training of a model, including the training data and training configuration. <pre><code>cd policy/RDT\nbash generate.sh ${model_name}\n</code></pre></p> <p>This will create a folder named <code>\\${model_name}</code> under training_data and a configuration file <code>\\${model_name}.yml</code> under model_config.</p>"},{"location":"usage/RDT.html#41-41-prepare-data","title":"4.1 4.1 Prepare Data","text":"<p>Copy all the data you wish to use for training from <code>processed_data</code> into <code>training_data/${model_name}</code>. If you have multiple tasks with different data, simply copy them in the same way.</p> <p>Example folder structure: <pre><code>training_data/${model_name}\n\u251c\u2500\u2500 ${task_1}\n\u2502   \u251c\u2500\u2500 instructions\n\u2502   \u2502   \u251c\u2500\u2500 lang_embed_0.pt\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 episode_0\n|   |   |\u2500\u2500 episode_0.hdf5\n|   |   |-- instructions\n|   \u2502   \u2502   \u251c\u2500\u2500 lang_embed_0.pt\n|   \u2502   \u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ${task_2}\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ...\n</code></pre></p>"},{"location":"usage/RDT.html#42-42-modify-training-config","title":"4.2 4.2 Modify Training Config","text":"<p>In <code>model_config/${model_name}.yml</code>, you need to manually set the GPU to be used (modify <code>cuda_visible_device</code>). For a single GPU, try format like <code>0</code> to set GPU 0. For multi-GPU usage, try format like <code>0,1,4</code>. You can flexibly modify other parameters.</p>"},{"location":"usage/RDT.html#5-5-finetune-model","title":"5. 5. Finetune model","text":"<p>Once the training parameters are set, you can start training with: <pre><code>bash finetune.sh ${model_name}\n</code></pre> Note!</p> <p>If you fine-tune the model using a single GPU, DeepSpeed will not save <code>pytorch_model/mp_rank_00_model_states.pt</code>. If you wish to continue training based on the results of a single-GPU trained model, please set <code>pretrained_model_name_or_path</code> to something like <code>./checkpoints/${model_name}/checkpoint-${ckpt_id}</code>. </p> <p>This will use the pretrain pipeline to import the model, which is the same import structure as the default <code>../weights/RDT/rdt-1b</code>.</p>"},{"location":"usage/RDT.html#6-6-eval-on-robotwin","title":"6. 6. Eval on RoboTwin","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>model_name</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${model_name} ${checkpoint_id} ${seed} ${gpu_id}\n</code></pre>"},{"location":"usage/TinyVLA.html","title":"TinyVLA","text":"<p>Coming Soon</p>"},{"location":"usage/collect-data.html","title":"Collect Data","text":"<p>We provide over 100,000 pre-collected trajectories as part of the open-source release RoboTwin Dataset. However, we strongly recommend users to perform data collection themselves due to the high configurability and diversity of task and embodiment setups.</p> <p>Running the following command will first search for a random seed for the target collection quantity, and then replay the seed to collect data.</p> <pre><code>bash collect_data.sh ${task_name} ${task_config} ${gpu_id}\n# Example: bash collect_data.sh beat_block_hammer demo_randomizd 0\n</code></pre> <p>After data collection is completed, the collected data will be stored under <code>data/${task_name}/${task_config}</code>.</p> <ul> <li>Each trajectory's observation and action data are saved in HDF5 format in the <code>data</code> directory.</li> <li>The corresponding language instructions for each trajectory are stored in the <code>instructions</code> directory.</li> <li>Head camera videos of each trajectory can be found in the <code>video</code> directory.</li> <li>The <code>_traj_data</code>, <code>.cache</code>, <code>scene_info.json</code>, and <code>seed.txt</code> files are auxiliary outputs generated during the data collection process.</li> </ul> <p>All available <code>task_name</code> options can be found in the documentation. The <code>gpu_id</code> parameter specifies which GPU to use and should be set to an integer in the range <code>0</code> to <code>N-1</code>, where <code>N</code> is the number of GPUs available on your system.</p> <p>Our data synthesizer enables automated data collection by executing the task scripts in the <code>envs</code> directory, in combination with the <code>curobo</code> robot planner. Specifically, data collection is configured through a task-specific configuration file (see the tutorial in <code>./configurations.md</code>), which defines parameters such as the target embodiment, domain randomization settings, and the number of data samples to collect.</p> <p>The success rate of data generation for each embodiment across all tasks can be found at: https://robotwin-platform.github.io/doc/tasks/index.html. Due to the structural limitations of different robotic arms, not all embodiments are capable of completing every task.</p> <p>Our pipeline first explores a set of random seeds (<code>seeds.txt</code>) to identify trajectories that can yield successful data collection. It then records fine-grained action trajectories (<code>_traj_data</code>) accordingly. Collected videos are available in the <code>videos</code> directory.</p> <p>The entire process is fully automated\u2014just run a single command to get started.</p> <p>\u26a0\ufe0f The <code>missing pytorch3d</code> warning can be ignored if 3D data is not required.</p>"},{"location":"usage/configurations.html","title":"Configuration Tutorial","text":"<p>All configuration files are stored in the <code>task_config</code> folder and follow the standard YAML format.</p> <p>You can run <code>bash task_config/create_task_config.sh ${task_config_name}</code> to create new task configuration.</p>"},{"location":"usage/configurations.html#1-minimal-example","title":"1. \u2705 Minimal Example","text":"<p>Below is a minimal configuration file to start a typical data collection session:</p> <pre><code>render_freq: 0\nepisode_num: 100\nuse_seed: false\nsave_freq: 15\nembodiment:\n  - aloha-agilex\nlanguage_num: 100\n\ndomain_randomization:\n  random_background: true\n  cluttered_table: true\n  clean_background_rate: 0.02\n  random_head_camera_dis: 0\n  random_table_height: 0.03\n  random_light: true\n  crazy_random_light_rate: 0.02\n  random_embodiment: false\n\ncamera:\n  head_camera_type: D435\n  wrist_camera_type: D435\n  collect_head_camera: true\n  collect_wrist_camera: true\n\ndata_type:\n  rgb: true\n  third_view: false\n  depth: false\n  pointcloud: false\n  observer: false\n  endpose: false\n  qpos: true\n  mesh_segmentation: false\n  actor_segmentation: false\n\npcd_down_sample_num: 1024\npcd_crop: true\ndual_arm: true\nsave_path: ./data\nclear_cache_freq: 5\ncollect_data: true\neval_video_log: true\n</code></pre>"},{"location":"usage/configurations.html#2-configuration-breakdown","title":"2. \ud83d\udd27 Configuration Breakdown","text":""},{"location":"usage/configurations.html#21-task-embodiment-settings","title":"2.1 \ud83c\udfaf Task &amp; Embodiment Settings","text":"Field Type Required Description <code>embodiment</code> list \u2705 List of robot embodiment(s). For a dual-arm robot, use <code>[name]</code>, e.g., <code>[aloha-agilex]</code>; to combine two single-arm robots, use <code>[left, right, interval]</code>, e.g., <code>[piper, franka-panda, 0.6]</code>. The <code>interval</code> specifies the distance between arms (typically 0.6\u20130.8 meters). <code>dual_arm</code> bool optional Whether to use both arms. Default: <code>true</code>. <code>use_seed</code> bool \u2705 Whether to use a predefined seed list from <code>task_config/seeds/</code>. If <code>false</code>, the system will automatically explore viable seeds. <code>episode_num</code> int \u2705 Number of successful episodes to collect. <code>language_num</code> int optional If using language-conditioned task planning, sets the number of language descriptions to sample for each task."},{"location":"usage/configurations.html#22-domain-randomization","title":"2.2 \ud83e\udde0 Domain Randomization","text":"<p>Configure task variation for better generalization.</p> <pre><code>domain_randomization:\n  random_background: true\n  cluttered_table: true\n  clean_background_rate: 0.02\n  random_head_camera_dis: 0\n  random_table_height: 0.03\n  random_light: true\n  crazy_random_light_rate: 0.02\n  random_embodiment: false\n</code></pre> Field Type Description <code>random_background</code> bool Enable random textures for the table and background. <code>cluttered_table</code> bool Add distractor objects to the table to simulate a cluttered environment. <code>clean_background_rate</code> float Ratio of clean backgrounds (e.g., <code>0.02</code> = 2%). Only effective if <code>random_background</code> is <code>true</code>. <code>random_head_camera_dis</code> float Random displacement applied to the head camera position (in meters). <code>random_table_height</code> float Random variation in the table height (in meters). <code>random_light</code> bool Enable randomized lighting during simulation. <code>crazy_random_light_rate</code> float Probability of applying extreme lighting. Only effective if <code>random_light</code> is <code>true</code>. <code>random_embodiment</code> bool Enable embodiment randomization (experimental, currently not fully supported)."},{"location":"usage/configurations.html#23-camera-configuration","title":"2.3 \ud83d\udcf7 Camera Configuration","text":"<pre><code>camera:\n  head_camera_type: D435\n  wrist_camera_type: D435\n  collect_head_camera: true\n  collect_wrist_camera: true\n</code></pre> Field Type Description <code>head_camera_type</code> str Camera used for global observation. Options: see <code>task_config/_camera_config.yml</code>. <code>wrist_camera_type</code> str Camera used for close-up view. <code>collect_head_camera</code> bool Whether to collect head-view data. <code>collect_wrist_camera</code> bool Whether to collect wrist-view data."},{"location":"usage/configurations.html#24-data-collection-settings","title":"2.4 \ud83d\udce6 Data Collection Settings","text":"Field Type Required Description <code>collect_data</code> bool \u2705 Enable actual data saving. <code>save_freq</code> int \u2705 Save every N steps. <code>save_path</code> str optional Directory to save data. Default: <code>./data</code>. <code>clear_cache_freq</code> int optional Clear cached data every N episodes. Prevents memory overflow. <code>eval_video_log</code> bool optional Save evaluation videos for replay."},{"location":"usage/configurations.html#25-data-type","title":"2.5 \ud83d\udcbe Data Type","text":"<p>Specify which data to collect in each episode:</p> <pre><code>data_type:\n  rgb: true\n  third_view: false\n  depth: false\n  pointcloud: false\n  observer: false\n  endpose: false\n  qpos: true\n  mesh_segmentation: false\n  actor_segmentation: false\n</code></pre> Type Description <code>rgb</code> RGB image from multiple views. <code>third_view</code> Third-person video. <code>depth</code> Depth images from cameras. <code>pointcloud</code> Merged point cloud of the scene. <code>observer</code> Observer-view RGB frame. <code>endpose</code> End-effector 6D pose. <code>qpos</code> Robot joint angles. <code>mesh_segmentation</code> Per-object segmentation from mesh. <code>actor_segmentation</code> Per-actor segmentation from RGB."},{"location":"usage/configurations.html#26-point-cloud-settings","title":"2.6 \ud83d\udd0d Point Cloud Settings","text":"Field Type Description <code>pcd_down_sample_num</code> int FPS (Farthest Point Sampling) number; set <code>0</code> to keep all points. <code>pcd_crop</code> bool Whether to crop out table/walls based on known transforms."},{"location":"usage/configurations.html#27-rendering","title":"2.7 \ud83c\udfa5 Rendering","text":"Field Type Description <code>render_freq</code> int Render visualization every N steps. Set to <code>0</code> to disable. For servers without display, recommend <code>0</code>."},{"location":"usage/configurations.html#3-notes","title":"3. \ud83d\udccc Notes","text":"<ul> <li>All task names must correspond to files in <code>env/&lt;task_name&gt;.py</code>.</li> <li> <p>For available embodiments and cameras, refer to:</p> </li> <li> <p><code>task_config/_embodiment_config.yml</code></p> </li> <li><code>task_config/_camera_config.yml</code></li> <li>The system supports both dual-arm and single-arm setups.</li> <li>Seeds, if used, are located in <code>task_config/seeds/</code>.</li> </ul> <p>Let me know if you'd like this exported into Markdown or embedded directly into your project documentation system (e.g., MkDocs).</p>"},{"location":"usage/deploy-your-policy.html","title":"\ud83d\ude80 Deploy Your Policy","text":"<p>To deploy and evaluate your policy, you need to modify the following three files:</p> <ul> <li><code>eval.sh</code></li> <li><code>deploy_policy.yml</code></li> <li><code>deploy_policy.py</code></li> </ul>"},{"location":"usage/deploy-your-policy.html#1-1-deploy_policyyml","title":"1. \ud83d\udd27 1. <code>deploy_policy.yml</code>","text":"<p>You are free to add any parameters needed in <code>deploy_policy.yml</code> to specify your model setup (e.g., checkpoint path, model type, architecture details). The entire YAML content will be passed to <code>deploy_policy.py</code> as <code>usr_args</code>, which will be available in the <code>get_model()</code> function.</p>"},{"location":"usage/deploy-your-policy.html#2-2-evalsh","title":"2. \ud83d\udda5\ufe0f 2. <code>eval.sh</code>","text":"<p>Update the script to pass additional arguments to override default values in <code>deploy_policy.yml</code>.</p> <pre><code>#!/bin/bash\n\npolicy_name=Your_Policy\ntask_name=${1}\ntask_config=${2}\nckpt_setting=${3}\nseed=${4}\ngpu_id=${5}\n# [TODO] Add your custom command-line arguments here\n\nexport CUDA_VISIBLE_DEVICES=${gpu_id}\necho -e \"\\033[33mgpu id (to use): ${gpu_id}\\033[0m\"\n\ncd ../.. # move to project root\n\npython script/eval_policy.py --config policy/$policy_name/deploy_policy.yml \\\n    --overrides \\\n    --task_name ${task_name} \\\n    --task_config ${task_config} \\\n    --ckpt_setting ${ckpt_setting} \\\n    --seed ${seed} \\\n    --policy_name ${policy_name} \n    # [TODO] Add your custom arguments here\n</code></pre>"},{"location":"usage/deploy-your-policy.html#3-3-deploy_policypy","title":"3. \ud83e\udde0 3. <code>deploy_policy.py</code>","text":"<p>You need to implement the following methods in <code>deploy_policy.py</code>:</p>"},{"location":"usage/deploy-your-policy.html#31-1-encode_obsobs-dict-dict","title":"3.1 1. <code>encode_obs(obs: dict) -&gt; dict</code>","text":"<p>Optional. This function is used to preprocess the raw environment observation (e.g., color channel normalization, reshaping, etc.). If not needed, it can be left unchanged.</p>"},{"location":"usage/deploy-your-policy.html#32-2-get_modelusr_args-dict-any","title":"3.2 2. <code>get_model(usr_args: dict) -&gt; Any</code>","text":"<p>Required. This function receives the full configuration from <code>deploy_policy.yml</code> via <code>usr_args</code> and must return the initialized model. You can define your own loading logic here, including parsing checkpoints and network parameters.</p>"},{"location":"usage/deploy-your-policy.html#33-3-evalenv-model-observation-instruction-any","title":"3.3 3. <code>eval(env, model, observation, instruction) -&gt; Any</code>","text":"<p>Required. The main evaluation loop. Given the current environment instance, model, and observation (as a dictionary), and a natural language <code>instruction</code> (string), this function must compute the next action and execute it in the environment.</p>"},{"location":"usage/deploy-your-policy.html#34-4-update_obsobs-dict-none","title":"3.4 4. <code>update_obs(obs: dict) -&gt; None</code>","text":"<p>Optional. Used to update any internal state of the model or observation buffer. Useful if your model requires a history of frames or a memory-based context.</p>"},{"location":"usage/deploy-your-policy.html#35-5-get_actionmodel-obs-dict-any","title":"3.5 5. <code>get_action(model, obs: dict) -&gt; Any</code>","text":"<p>Optional. Given a model and current observation, return the action to be executed. This is useful if action computation is separated from the evaluation loop.</p>"},{"location":"usage/deploy-your-policy.html#36-6-reset_model-none","title":"3.6 6. <code>reset_model() -&gt; None</code>","text":"<p>Optional but recommended. This function is called before the evaluation of each episode, allowing you to reset model states such as recurrent memory, history buffers, or context encodings.</p>"},{"location":"usage/deploy-your-policy.html#4-notes","title":"4. \ud83d\udccc Notes","text":"<ul> <li>The variable <code>instruction</code> is a string containing the language command describing the task. You can choose how (or whether) to use it.</li> <li>Your policy should be compatible with the input/output format expected by the simulator.</li> </ul>"},{"location":"usage/description.html","title":"Description Gen (Object &amp; Task)","text":""},{"location":"usage/description.html#1-object-description","title":"1. Object Description","text":"<pre><code># Generate object description for all objects\npython3 utils/generate_object_description.py\n\n# Generate object description for a specific type of object with as many objects as this class contains\npython3 utils/generate_object_description.py 001_bottle\n\n# Generate object description for a specific object index of a specific type of object\npython3 utils/generate_object_description.py 001_bottle --index 0\n</code></pre>"},{"location":"usage/description.html#2-task-instruction","title":"2. Task Instruction","text":"<pre><code># Generate 60 task descriptions for a task\npython3 utils/generate_task_description.py place_shoe 60\n</code></pre> <p>It will call for <code>instruction_num % 12</code> times of API, each time returning 12 instructions shuffled into 10 seen and 2 unseen instructions.</p>"},{"location":"usage/description.html#3-episode-instruction","title":"3. Episode Instruction","text":"<pre><code># Generate 60 task descriptions for a task\npython3 utils/generate_episode_instructions.py place_shoe franka-panda-D435 1000\n</code></pre>"},{"location":"usage/description.html#31-parameters","title":"3.1 Parameters:","text":"<ul> <li><code>task_name</code>: Name of the task (JSON file name without extension)</li> <li><code>setting</code>: Setting name used to construct the data directory path</li> <li><code>max_num</code>: Maximum number of descriptions per episode</li> </ul>"},{"location":"usage/domain-randomization.html","title":"Domain Randomization","text":"<p>RoboTwin\u2019s domain randomization primarily focuses on scene clutter, random lighting, over 12,000 tabletop textures, randomized tabletop heights, and camera viewpoint perturbations. The corresponding configuration options can be found at: \ud83d\udc49 RoboTwin 2.0 Document (Usage: Configurations)</p> <p></p>"},{"location":"usage/expert-code-gen.html","title":"Expert Code Generation","text":""},{"location":"usage/expert-code-gen.html#1-code_gen-folder-structure","title":"1. Code_gen Folder Structure","text":"<p>This directory contains various modules for generating and testing robot task code:</p> <ul> <li>gpt_agent.py: API integration with LLM models</li> <li>observation_agent.py: Processes multi-modal observations for code correction</li> <li>prompt.py: Prompt templates for code generation</li> <li>run_code.py: Executes and tests generated code</li> <li>task_generation_simple.py: Basic single-pass code generation</li> <li>task_generation.py: Iterative code generation with error feedback</li> <li>task_generation_mm.py: Advanced code generation with multi-modal observation</li> <li>task_info.py: Task definitions and descriptions</li> <li>test_gen_code.py: Utility for testing generated code with detailed metrics</li> </ul> <p>The code generation system also interacts with these important directories: - ./envs/: Contains manually implemented task environments   - _base_task.py: Core environment with robot control functions and utilities     - Includes <code>save_camera_images(task_name, step_name, generate_num_id, save_dir)</code> for capturing visual observations during task execution - ./envs_gen/: Stores auto-generated task implementations - ./task_config/: Configuration files for tasks and embodiments - ./script/: Template scripts and utilities - ./assets/objects/: 3D models and metadata for simulation objects - ./camera_images/: Stores observation images captured during code generation for multi-modal feedback</p> <p>The entire pipeline enables automatic generation of robot control code from natural language task descriptions, with feedback-based refinement and multi-modal observation capabilities.</p>"},{"location":"usage/expert-code-gen.html#2-configure-llm-api-key","title":"2. Configure LLM API Key","text":"<p>Please configure the necessary API keys in the <code>code_gen/gpt_agent.py</code> file. Additionally, if the LLM you are utilizing does not support integration with the OpenAI API, you may need to make corresponding adjustments to the <code>generate()</code> function.</p>"},{"location":"usage/expert-code-gen.html#3-generate-your-task-code","title":"3. Generate Your Task Code","text":""},{"location":"usage/expert-code-gen.html#31-1-add-task-description","title":"3.1 1. Add Task Description","text":"<p>Add new task information, including the task name and natural language description, in <code>./code_gen/task_info.py</code>.</p>"},{"location":"usage/expert-code-gen.html#1-template-of-task-information","title":"1. Template of Task Information:","text":"<pre><code>TASK_NAME = {\n    \"task_name\": \"task_name\",                # Name of the task\n    \"task_description\": \"...\",               # Detailed description of the task\n    \"current_code\": '''\n                class gpt_{task_name}({task_name}):\n                    def play_once(self):\n                        pass\n                '''                          # Code template to be completed\n    \"actor_list\": {                          # List of involved objects; can be a dictionary or a simple list\n        \"self.object1\": {\n            \"name\": \"object1\",               # Object name\n            \"description\": \"...\",            # Description of the object\n            \"modelname\": \"model_name\"        # Name of the 3D model representing the object\n        },\n        \"self.object2\": {\n            \"name\": \"object2\",\n            \"description\": \"...\",\n            \"modelname\": \"model_name\"\n        },\n        # ... more objects\n    },\n    # Alternatively, the actor_list can be a simple list:\n    # \"actor_list\": [\"self.object1\", \"self.object2\", ...],\n    # To make code generation easier, the actor_list also includes some pose information\n    # like target poses or middle poses (optional and don't require modelname).\n}\n</code></pre>"},{"location":"usage/expert-code-gen.html#11-2-add-basic-task-code","title":"1.1 2. Add Basic Task Code","text":"<p>Add the basic code file <code>${task_name}.py</code> in the <code>./envs/</code> directory, following this structure:</p> <pre><code>from .base_task import Base_task\nfrom .utils import *\nimport sapien\n\nclass ${task_name}(Base_task):\n    def setup_demo(self, **kwargs):\n        # Initializes the simulation environment for the task\n        # Sets up the table, robot, planner, camera, and initial positions\n        # This function is called once at the beginning of each episode\n        pass\n\n    def load_actors(self):\n        # Loads all the necessary objects for the task into the environment\n        # Typically called from setup_demo to initialize scene objects\n        # Can also be used to set initial poses for objects\n        pass\n\n    def play_once(self):\n        # Contains the robot control code to complete the task\n        # This is the main function that will be generated by the LLM\n        # Implements the sequence of actions for the robot to achieve the task\n        pass\n\n    # Check success\n    def check_success(self):\n        # Defines criteria to determine if the task was completed successfully\n        # Returns a boolean indicating success or failure\n        # Used for evaluation and feedback during code generation\n        pass\n</code></pre> <p>In the code above, <code>{task_name}</code> should match the name of the basic code file, and the <code>check_success()</code> function is used to determine if the task is successful. No changes are needed for the rest of the code.</p> <p>Note: The <code>envs</code> folder contains manually written files with <code>setup_demo</code>, robot operation code in <code>play_once</code>, and <code>check_success</code> methods. Auto-generated code will be saved in the <code>envs_gen</code> folder.</p>"},{"location":"usage/expert-code-gen.html#12-3-generate-the-final-code","title":"1.2 3. Generate the Final Code","text":"<p>You can use three different code generation approaches depending on your needs:</p> <p>Note: The code generation process will only generate the <code>play_once()</code> method implementation, which contains the robot control logic to complete the task. Other methods like <code>setup_demo()</code>, <code>load_actors()</code>, and <code>check_success()</code> should be manually implemented.</p>"},{"location":"usage/expert-code-gen.html#121-basic-code-generation","title":"1.2.1 Basic Code Generation","text":"<p>For quick verification of new tasks or debugging existing ones without iterative correction:</p> <pre><code>python code_gen/task_generation_simple.py task_name\n</code></pre>"},{"location":"usage/expert-code-gen.html#122-code-generation-with-error-feedback","title":"1.2.2 Code Generation with Error Feedback","text":"<p>This script implements iterative code correction based on error feedback, consistent with RoboTwin 1.0:</p> <pre><code>python code_gen/task_generation.py task_name\n</code></pre>"},{"location":"usage/expert-code-gen.html#123-advanced-code-generation-with-multi-modal-observations","title":"1.2.3 Advanced Code Generation with Multi-Modal Observations","text":"<p>This script provides both error feedback iteration and multi-modal observation-based code correction, consistent with RoboTwin 2.0. It offers the best generation quality but runs slower:</p> <pre><code>python code_gen/task_generation_mm.py task_name\n</code></pre> <p>The multi-modal observation functionality is implemented in <code>code_gen/observation_agent.py</code>.</p> <p>The generated code file will be saved as <code>./envs_gen/gpt_${task_name}.py</code>. For example: <pre><code>python code_gen/task_generation_mm.py pick_dual_bottles_easy\n</code></pre> This will create <code>./envs_gen/gpt_pick_dual_bottles_easy.py</code>.</p>"},{"location":"usage/expert-code-gen.html#13-4-test-generated-code","title":"1.3 4. Test Generated Code","text":"<p>Run the following script to test the generated code:</p> <pre><code>python code_gen/run_code.py task_name\n</code></pre> <p>This will execute the task using the generated code and display the results, allowing you to validate the performance.</p>"},{"location":"usage/expert-code-gen.html#11-additional-resources","title":"1.1 Additional Resources","text":"<p>For more information on generating task descriptions and object descriptions, refer to the documentation in the description directory.</p> <p>For policy training and evaluation using the generated code, consult the policy/ACT documentation.</p>"},{"location":"usage/object-annotation.html","title":"Calibration Tool Instructions","text":""},{"location":"usage/object-annotation.html#1-rigid-body-object-annotation","title":"1. Rigid Body Object Annotation","text":""},{"location":"usage/object-annotation.html#11-1-create-calibration-window","title":"1.1 1. Create Calibration Window:","text":"<p><pre><code>python script/create_object_data.py [-s START] model_name\n\npositional arguments:\n    model_name            Model name\n\noptions:\n    -s START, --start START Start id\n</code></pre> Here, <code>model_name</code> is the name of a subdirectory under the <code>assets/objects/</code> directory. For example, to calibrate the hammer model located at <code>assets/objects/020_hammer</code>, run the command: <code>python script/create_object_data.py 020_hammer</code>. A window will then appear as shown below: </p>"},{"location":"usage/object-annotation.html#12-2-calibration-commands","title":"1.2 2. Calibration Commands:","text":"<p><pre><code>resize:\n    Usage:\n        resize &lt;x_size&gt; &lt;y_size&gt; &lt;z_size&gt;: Set scaling along x, y, z axes\n        resize &lt;size&gt;: Uniformly scale all three axes\n    Example:\n        resize 0.1\ncreate:\n    Usage:\n        create &lt;type&gt;: Create (t)arget, (c)ontact, (f)unctional, or (o)rientation point\n        create: Waits for input of point name\n    Examples:\n        create t\n        create f\nclone:\n    Usage:\n        clone &lt;type&gt; &lt;id&gt;: Clone a specified type and ID point in place\n        clone: Waits for input of point type and ID\n    Examples:\n        clone t 1: Clones contact point target_1 to create a new target point (e.g., target_2)\nrotate:\n    Usage:\n        rotate &lt;id&gt; &lt;axis&gt; &lt;interval&gt;: Rotate a specified contact point around its own axis by a given interval, generating points belonging to the same group\n    Example:\n        rotate 1 x 90: Rotates contact_1 around its x-axis every 90 degrees, creating three additional contact points, and writes the group into concat_points_group\nalign:\n    Usage:\n        align: Aligns all group points' positions to the first point in the group\nremove:\n    Usage:\n        remove &lt;type&gt; &lt;id&gt;: Removes a point with the specified name\n        remove: Waits for input of point name\n    Examples:\n        remove t 0\nsave:\n    Saves current calibration data \u2014 always remember to save!\nexit:\n    Exits the calibration window\n</code></pre> As an example using <code>020_hammer</code>, entering <code>create c</code> creates a cube centered on the object. Use your mouse to select this cube and check \"Enable\" under the Transform section in the UI window. Then choose \"Local\" to display the cube's center position and coordinate system, which represents the contact point's location and orientation: </p> <p>You can move the calibration point's position with the mouse. Click on \"Rotate\" in the Transform options to adjust the rotation along the x, y, and z axes, changing the point's coordinate system orientation: </p> <p>Next, add a functional point to the head of the hammer, adjust its orientation, and use the command <code>create f</code> to move it to the center position of the hammer head. The adjusted point is shown in the following image: </p> <p>Finally, enter <code>save</code> to save the point information, and then enter <code>exit</code> to end the calibration.</p> <p>Notes :</p> <ol> <li>After adjusting the position, you must click \"Teleport\" under the Transform menu to apply the movement.</li> <li>Always remember to save your changes before exiting the calibration window!</li> </ol>"},{"location":"usage/object-annotation.html#13-3-view-calibration-files","title":"1.3 3. View Calibration Files","text":"<p>Navigate to the asset folder you just calibrated, and you will find a newly generated <code>model_data{id}.json</code> file. You can modify the <code>\"scale\"</code> field within this file to adjust the asset's scaling in the simulation environment. </p> <p>The meanings of each field in the asset can be found in the model_data_info file.</p>"},{"location":"usage/object-annotation.html#2-urdf-articulation-objects-annotation","title":"2. URDF Articulation Objects Annotation","text":""},{"location":"usage/object-annotation.html#21-1-create-calibration-window","title":"2.1 1. Create Calibration Window:","text":"<p>Similar to rigid body object annotation, use the same command to create the articulation calibration window. The calibration program will automatically recognize the asset type.</p>"},{"location":"usage/object-annotation.html#22-2-calibration-commands","title":"2.2 2. Calibration Commands:","text":"<p><pre><code>run:\n    Usage:\n        run\n        Press &lt;Ctrl + C&gt; to stop and save information\n    Used to obtain stable points through steps, generally selected at the beginning of calibration to determine if running is necessary.\n    Since this command does not limit the step upper limit, you need to manually stop running (press Ctrl+C) based on whether the asset in the UI interface is stable.\nqpos:\n    Usage:\n        qpos\n    Get the current joint state as the initial pose when loading the asset into the task.\nmass:\n    Usage:\n        mass &lt;m1&gt; &lt;m2&gt; ...: Set the mass of the articulation joint, ensuring that the input matches the displayed link count (excluding base) in order.\n    Example:\n        mass 0.5 0.05\nresize:\n    Usage:\n        resize &lt;size&gt;: Synchronize the scaling of all three axes of the object\n    Example:\n        resize 0.1\ncreate:\n    Usage:\n        create &lt;type&gt; &lt;base_link&gt;: Create (t)arget, (c)ontact, (f)unctional, (o)rientation points\n    Example:\n        create c link1\nrebase:\n    Usage:\n        rebase &lt;type&gt; &lt;id&gt; &lt;base_link&gt;: Modify the base link of the specified point\n    Example:\n        rebase c 0 link1\nclone:\n    Usage:\n        clone &lt;type&gt; &lt;id&gt;: Create an in-place copy of the specified point (without base)\n    Example:\n        clone t 1: Create a new target point (e.g., target_2&lt;link1&gt;) by copying target_1&lt;link1&gt;\nrotate:\n    Usage:\n        rotate &lt;id&gt; &lt;axis&gt; &lt;interval&gt;: Rotate the specified contact point around its own specified axis by the specified interval, generating points belonging to the same group\n    Example:\n        rotate 1 x 90: Rotate contact_1 around its own x-axis by 90 degrees, generating three contact points, and write the grouping of the four points into concat_points_group\nalign:\n    Usage:\n        align: Align the positions of all group points to the first point in the group\nremove:\n    Usage:\n        remove &lt;type&gt; &lt;id&gt;: Remove the specified point (without base)\n    Example:\n        remove t 0\nsave:\n    Usage:\n        save: Save the current calibration data, and make sure to save!\nexit:\n    Usage:\n        exit: Exit the calibration window\n</code></pre> The calibration process is similar to rigid body object annotation, and you also need to save the data and exit after completion.</p>"},{"location":"usage/robotwin-install.html","title":"Install &amp; Download","text":""},{"location":"usage/robotwin-install.html#1-dependencies","title":"1. Dependencies","text":"<p>Python versions:</p> <ul> <li>Python 3.8, 3.10</li> </ul> <p>Operating systems:</p> <ul> <li>Linux: Ubuntu 18.04+, Centos 7+</li> </ul> <p>Hardware:</p> <ul> <li> <p>Rendering: NVIDIA or AMD GPU</p> </li> <li> <p>Ray tracing: NVIDIA RTX GPU or AMD equivalent</p> </li> <li> <p>Ray-tracing Denoising: NVIDIA GPU</p> </li> <li> <p>GPU Simulation: NVIDIA GPU</p> </li> </ul> <p>Software:</p> <ul> <li>Ray tracing: NVIDIA Driver &gt;= 470</li> <li>Denoising (OIDN): NVIDIA Driver &gt;= 520</li> </ul>"},{"location":"usage/robotwin-install.html#2-install-vulkan-if-not-installed","title":"2. Install Vulkan (if not installed)","text":"<p>Check <code>vulkaninfo</code> <pre><code>sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools\n</code></pre></p>"},{"location":"usage/robotwin-install.html#3-basic-env","title":"3. Basic Env","text":"<p>First, prepare a conda environment. <pre><code>conda create -n RoboTwin python=3.10 -y\nconda activate RoboTwin\n</code></pre></p> <p>RoboTwin 2.0 Code Repo: https://github.com/RoboTwin-Platform/RoboTwin</p> <pre><code>git clone https://github.com/RoboTwin-Platform/RoboTwin.git\n</code></pre> <p>Then, run <code>script/_install.sh</code> to install basic envs and CuRobo: <pre><code>bash script/_install.sh\n</code></pre></p> <p>If you encounter any problems, please refer to the manual installation section. If you are not using 3D data, a failed installation of pytorch3d will not affect the functionality of the project.</p>"},{"location":"usage/robotwin-install.html#4-download-assert-robotwin-od-texture-library-and-embodiments","title":"4. Download Assert (RoboTwin-OD, Texture Library and Embodiments)","text":"<p>You can download the assets by running the following command: <pre><code>bash script/_download.sh\n</code></pre></p> <p>The structure of the <code>assets</code> folder should be like this:</p> <pre><code>assets\n\u251c\u2500\u2500 cluttered_objects\n\u251c\u2500\u2500 background_texture\n\u251c\u2500\u2500 embodiments\n\u2502   \u251c\u2500\u2500 embodiment_1\n\u2502   \u2502   \u251c\u2500\u2500 config.yml\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 objects\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"usage/robotwin-install.html#5-manual-installation-only-when-step-2-failed","title":"5. Manual Installation (Only when step 2 failed)","text":"<ol> <li> <p>Install requirements <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install pytorch3d <pre><code>pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\n</code></pre></p> </li> <li> <p>Install CuRobo <pre><code>cd envs\ngit clone https://github.com/NVlabs/curobo.git\ncd curobo\npip install -e . --no-build-isolation\ncd ../..\n</code></pre></p> </li> <li> <p>Adjust code in <code>mplib</code> (Important)</p> </li> <li> <p>You can use <code>pip show mplib</code> to find where the <code>mplib</code> installed.</p> </li> <li> <p>Remove <code>or collide</code></p> </li> </ol> <pre><code># mplib.planner (mplib/planner.py) line 807\n# remove `or collide`\n\nif np.linalg.norm(delta_twist) &lt; 1e-4 or collide or not within_joint_limit:\n                return {\"status\": \"screw plan failed\"}\n=&gt;\nif np.linalg.norm(delta_twist) &lt; 1e-4 or not within_joint_limit:\n                return {\"status\": \"screw plan failed\"}\n</code></pre>"},{"location":"usage/object_marking/model_data_info.html","title":"Model data info","text":"<p>center: \u200b\u7269\u4f53\u200b\u4e2d\u5fc3\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u7269\u4f53\u200b\u8d44\u4ea7\u200b\u5750\u6807\u200b\u7684\u200b\u504f\u79fb\u200b\u5411\u91cf\u200b\uff08x y z\uff09: 1*3 matrix extents: \u200b\u8d44\u4ea7\u200b\u957f\u5bbd\u200b\u9ad8\u200b\uff08x y z \u200b\u5750\u6807\u8f74\u200b\u4e0a\u200b\u7684\u200b\u957f\u5ea6\u200b\uff09: 1*3 matrix scale: \u200b\u573a\u666f\u200b\u4e2d\u200b\u5bf9\u200b\u5b9e\u9645\u200b\u7269\u4f53\u200b\u8d44\u4ea7\u200b\u7684\u200b\u7f29\u653e\u200b\uff08x y z\u200b\u65b9\u5411\u200b\u4e0a\u200b\uff09: 1*3 matrix target_pose: \u200b\u5df2\u200b\u88ab\u200b\u5e9f\u9664\u200b\uff0c\u200b\u65e0\u200b\u542b\u4e49\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u200b\u6807\u5b9a\u200b contact_points_pose: list\u200b\u5217\u8868\u200b\uff0c\u200b\u5217\u8868\u200b\u5143\u7d20\u200b\u4e3a\u200b4*4\u200b\u65cb\u8f6c\u200b+\u200b\u5e73\u79fb\u200b\u77e9\u9635\u200b\uff0c\u200b\u8868\u793a\u200b\u6293\u53d6\u200b\u70b9\u200b\u4e0e\u200b\u7269\u4f53\u200b\u4e2d\u5fc3\u200b\u5750\u6807\u200b\u7684\u200b\u504f\u79fb\u200b: n*4*4 matrix transform_matrix: \u200b\u4e0d\u200b\u91cd\u8981\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u200b\u6807\u5b9a\u200b functional_matrix: list\u200b\u5217\u8868\u200b\uff0c\u200b\u5217\u8868\u200b\u5143\u7d20\u200b\u4e3a\u200b4*4\u200b\u65cb\u8f6c\u200b+\u200b\u5e73\u79fb\u200b\u77e9\u9635\u200b\uff0c\u200b\u8868\u793a\u200b\u529f\u80fd\u200b\u70b9\u200b\u4e0e\u200b\u7269\u4f53\u200b\u4e2d\u5fc3\u200b\u5750\u6807\u200b\u7684\u200b\u504f\u79fb\u200b: n*4*4 matrix orientation_point: \u200b\u65b9\u5411\u200b\u70b9\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u200b\u6807\u5b9a\u200b contact_points_group: \u200b\u6293\u53d6\u200b\u70b9\u200b\u5206\u7ec4\u200b\uff0c\u200b\u539f\u56e0\u200b\u4e3a\u200b\u6709\u4e9b\u200b\u6293\u53d6\u200b\u70b9\u200bxyz\u200b\u5750\u6807\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u662f\u200b\u8f74\u200b\u7684\u200b\u65b9\u5411\u4e0d\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u628a\u200b\u8fd9\u90e8\u5206\u200b\u6293\u53d6\u200b\u70b9\u5206\u200b\u5230\u200b\u540c\u4e2a\u200bgroup\u200b\u5185\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u4e00\u4e9b\u200b\u64cd\u4f5c\u200b contact_points_mask: \u200b\u81ea\u52a8\u200b\u751f\u6210\u200b\uff0c\u200b\u65e0\u9700\u200b\u6807\u5b9a\u200b\uff08\u200b\u4f3c\u4e4e\u200b\u88ab\u200b\u5e9f\u9664\u200b\u4e86\u200b\uff09 contact_points_discription: \u200b\u6293\u53d6\u200b\u70b9\u200b\u63cf\u8ff0\u200b: 1*n str functional_point_discription: \u200b\u529f\u80fd\u200b\u70b9\u200b\u63cf\u8ff0\u200b: 1*n str orientation_point_discription\uff1a \u200b\u65b9\u5411\u200b\u70b9\u200b\u63cf\u8ff0\u200b: 1*n str</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u63cf\u8ff0\u200b\u6807\u5b9a\u200b\u4e00\u822c\u200b\u901a\u8fc7\u200b\u76f4\u63a5\u200b\u66f4\u6539\u200bjson\u200b\u6587\u4ef6\u200b\u6765\u200b\u6807\u5b9a\u200b\uff0c\u200b\u4e0d\u662f\u200b\u5f3a\u200b\u683c\u5f0f\u200b\u6807\u5b9a\u200b\u6570\u636e\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u200b\u901a\u8fc7\u200b\u5176\u4ed6\u200b\u8868\u793a\u200b\u65b9\u6cd5\u200b\u5916\u90e8\u200b\u6807\u5b9a\u200b\u3002</p>"}]}