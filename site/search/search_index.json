{"config":{"lang":["en","ja"],"separator":"[\\s\\u200b\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"RoboTwin 2.0","text":"Webpage | PDF | arXiv | Repo | Talk (in Chinese) | LeaderBoard <p>Here is the official documentation for RoboTwin 2.0, which includes installation and usage instructions for various RoboTwin functionalities, detailed information on the 50 bimanual tasks in RoboTwin 2.0, comprehensive descriptions of the RoboTwin-OD dataset, and guidelines for joining the community.</p> <p></p>"},{"location":"index.html#1-everything-about-robotwin-20","title":"1. Everything about RoboTwin 2.0","text":"<p>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation Tianxing Chen<sup>* </sup>, Zanxin Chen<sup>* </sup>, Baijun Chen<sup>* </sup>, Zijian Cai<sup>* </sup>, Yibin Liu<sup>* </sup>, Qiwei Liang, Zixuan Li, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan-ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo<sup>\u2020</sup>, Yao Mu<sup>\u2020</sup></p>"},{"location":"index.html#2-previous-works","title":"2. Previous Works","text":"<p>[Under Review] RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation [CVPR 2025 Highlight] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins [CVPR 2025 Challenge@MEIS Workshop] Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop [ECCV 2024 MAAS Workshop Best Paper] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version) [\u200b\u7b2c\u5341\u4e5d\u5c4a\u200b\u6311\u6218\u676f\u200b\u5b98\u65b9\u200b\u8d5b\u9898\u200b] \u200b\u8d5b\u9898\u200b\u94fe\u63a5\u200b</p>"},{"location":"index.html#3-citations","title":"3. Citations","text":"<p>If you find our work useful, please consider citing:</p> <p>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation <pre><code>@article{chen2025robotwin,\n  title={RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation},\n  author={Chen, Tianxing and Chen, Zanxin and Chen, Baijun and Cai, Zijian and Liu, Yibin and Liang, Qiwei and Li, Zixuan and Lin, Xianliang and Ge, Yiheng and Gu, Zhenyu and others},\n  journal={arXiv preprint arXiv:2506.18088},\n  year={2025}\n}\n</code></pre></p> <p>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins, accepted to CVPR 2025 (Highlight) <pre><code>@InProceedings{Mu_2025_CVPR,\n    author    = {Mu, Yao and Chen, Tianxing and Chen, Zanxin and Peng, Shijia and Lan, Zhiqian and Gao, Zeyu and Liang, Zhixuan and Yu, Qiaojun and Zou, Yude and Xu, Mingkun and Lin, Lunkai and Xie, Zhiqiang and Ding, Mingyu and Luo, Ping},\n    title     = {RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins},\n    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\n    month     = {June},\n    year      = {2025},\n    pages     = {27649-27660}\n}\n</code></pre></p> <p>Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop <pre><code>@article{chen2025benchmarking,\n  title={Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop},\n  author={Chen, Tianxing and Wang, Kaixuan and Yang, Zhaohui and Zhang, Yuhao and Chen, Zanxin and Chen, Baijun and Dong, Wanxi and Liu, Ziyuan and Chen, Dong and Yang, Tianshuo and others},\n  journal={arXiv preprint arXiv:2506.23351},\n  year={2025}\n}\n</code></pre></p> <p>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version), accepted to ECCV Workshop 2024 (Best Paper) <pre><code>@article{mu2024robotwin,\n  title={RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)},\n  author={Mu, Yao and Chen, Tianxing and Peng, Shijia and Chen, Zanxin and Gao, Zeyu and Zou, Yude and Lin, Lunkai and Xie, Zhiqiang and Luo, Ping},\n  journal={arXiv preprint arXiv:2409.02920},\n  year={2024}\n}\n</code></pre></p>"},{"location":"index.html#4-contact","title":"4. Contact","text":"<p>Contact Tianxing Chen if you have any questions or suggestions.</p>"},{"location":"common-issue/index.html","title":"Common Issue","text":""},{"location":"common-issue/index.html#1-modify-clear_cache_freq-to-reduce-gpu-memory-pressure","title":"1. Modify <code>clear_cache_freq</code> to Reduce GPU Memory Pressure","text":"<p>If you find that your GPU memory is insufficient\u2014especially if the program consistently runs out of memory after several episodes (during data collection or evaluation)\u2014try adjusting the <code>clear_cache_freq</code> parameter in the corresponding task configuration. Note that setting a smaller <code>clear_cache_freq</code> value can reduce GPU memory usage but may also slow down runtime performance (For more details, see Configurations).</p>"},{"location":"common-issue/index.html#2-svulkan2-error-oidn-error-invalid-handle","title":"2. <code>[svulkan2] [error] OIDN Error: invalid handle</code>","text":"<p>Try [SAPIEN issue: https://github.com/haosulab/SAPIEN/issues/243], or modify the <code>clear_cache_freq</code> into <code>1</code>.</p>"},{"location":"common-issue/index.html#3-stuck-while-collecting-data-and-evaluating","title":"3. Stuck While Collecting Data and Evaluating","text":"<p>Please check your GPU model. According to user feedback and known issues reported on SAPIEN [SAPIEN issue: https://github.com/haosulab/SAPIEN/issues/219], Hopper/Ampere series GPUs (e.g., A100, H100) may occasionally experience unexpected hangs during data collection. You may try to replace the affected seed (in <code>data/${task_name}/${task_config}/seed.txt</code>) and trajectory data (<code>data/${task_name}/${task_config}/_traj_data/</code>) with the last seed and episode data.</p>"},{"location":"common-issue/index.html#4-join-the-robotwin-community","title":"4. Join the RoboTwin Community","text":"<p>Consider joining the WeChat Community to stay connected and receive updates.</p>"},{"location":"community/index.html","title":"WeChat Group","text":"<p>Contact Tianxing Chen if you have any questions or suggestions.</p> <p></p>"},{"location":"main/about.html","title":"\u5173\u4e8e","text":"<p>\u200b\u6e05\u534e\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\uff08\u200b\u7b80\u79f0\u200b\u201c\u200b\u624b\u518c\u200b\u201d\uff09\u200b\u662f\u200b\u7531\u200b\u4e00\u7fa4\u200b\u6e05\u534e\u5927\u5b66\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u6bd5\u4e1a\u751f\u200b\u7f16\u5199\u200b\u7ef4\u62a4\u200b\u7684\u200b\u5728\u7ebf\u200b\u6587\u6863\u200b\u3002</p> <p>\u200b\u8be5\u200b\u624b\u518c\u200b\u65e8\u5728\u200b\u603b\u7ed3\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u7ecf\u9a8c\u6559\u8bad\u200b\uff0c\u200b\u56de\u7b54\u200b\u7533\u8bf7\u200b\u4e2d\u200b\u9047\u5230\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e3a\u200b\u62df\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u5b66\u5f1f\u200b\u5b66\u59b9\u200b\u63d0\u4f9b\u200b\u501f\u9274\u200b\u548c\u200b\u53c2\u8003\u200b\uff0c\u200b\u51cf\u5c11\u200b\u4fe1\u606f\u200b\u5dee\u200b\u5e26\u6765\u200b\u7684\u200b\u4e0d\u200b\u516c\u5e73\u200b\uff0c\u200b\u964d\u4f4e\u200b\u51c6\u5907\u200b\u65f6\u95f4\u200b\u548c\u200b\u91d1\u94b1\u200b\u6210\u672c\u200b\uff0c\u200b\u7f13\u89e3\u200b\u7533\u8bf7\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u7126\u8651\u200b\u3002</p> <p>\u200b\u5e0c\u671b\u200b\u672c\u624b\u518c\u200b\u80fd\u591f\u200b\u5e2e\u52a9\u200b\u5230\u200b\u62df\u200b\u51fa\u56fd\u200b\uff08\u200b\u5883\u200b\uff09\u200b\u6df1\u9020\u200b\u7684\u200b\u6e05\u534e\u200b\u5b66\u751f\u200b\uff0c\u200b\u5728\u200b\u7533\u8bf7\u200b\u8def\u4e0a\u200b\u987a\u987a\u5229\u5229\u200b\uff0c\u200b\u83b7\u5f97\u200b\u81ea\u5df1\u200b\u5fc3\u4eea\u200b\u9879\u76ee\u200b\u7684\u200b Offer\uff01</p>"},{"location":"main/about.html#_2","title":"\u58f0\u660e","text":"<p>\u200b\u672c\u624b\u518c\u200b\u4f7f\u7528\u200b Material for Mkdocs \u200b\u6784\u5efa\u200b\uff0c\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u5f00\u6e90\u200b\uff0c\u200b\u4f7f\u7528\u200b GitHub Pages \u200b\u516c\u5f00\u200b\u53d1\u5e03\u200b\u3002</p> <p>\u200b\u624b\u518c\u200b\u5185\u5bb9\u200b\u7531\u200b\u6e05\u534e\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\u7f16\u5199\u200b\u59d4\u5458\u4f1a\u200b\u6536\u96c6\u200b\u3001\u200b\u7f16\u5199\u200b\u3001\u200b\u5ba1\u6838\u200b\u3001\u200b\u53d1\u5e03\u200b\u5e76\u200b\u7ef4\u62a4\u200b\u3002\u200b\u5185\u5bb9\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\uff0c\u200b\u4e14\u200b\u4e0d\u200b\u4ee3\u8868\u200b\u4efb\u4f55\u200b\u653f\u6cbb\u7acb\u573a\u200b\u3002\u200b\u624b\u518c\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b \u200b\u4f9d\u636e\u200bCC BY-NC 4.0\u200b\u6388\u6743\u200b\uff0c\u200b\u4e0d\u5f97\u200b\u505a\u200b\u5546\u4e1a\u7528\u9014\u200b\uff0c\u200b\u8f6c\u8f7d\u200b\u6216\u8005\u200b\u5f15\u7528\u200b\u8bf7\u200b\u6ce8\u660e\u200b\u6765\u6e90\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u4e0d\u200b\u9075\u5b88\u200b\u6b64\u200b\u58f0\u660e\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u8fdd\u6cd5\u200b\u4f7f\u7528\u200b\u672c\u6587\u200b\u5185\u5bb9\u200b\u8005\u200b\uff0c\u200b\u4f9d\u6cd5\u200b\u4fdd\u7559\u200b\u8ffd\u7a76\u200b\u6743\u7b49\u200b\u3002</p>"},{"location":"main/about.html#_3","title":"\u8fdb\u5ea6\u200b\u4e0e\u200b\u66f4\u65b0","text":"<p>\u200b\u76ee\u524d\u200b\u672c\u624b\u518c\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u5927\u90e8\u5206\u200b \u200b\u524d\u8a00\u200b, \u200b\u51c6\u5907\u200b, \u200b\u5f55\u53d6\u200b\u53ca\u200b\u4e4b\u540e\u200b \u200b\u7ae0\u8282\u200b\u4e2d\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u672a\u6765\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6dfb\u52a0\u200b\u62db\u751f\u200b\u4fe1\u606f\u200b\u3001\u200b\u9879\u76ee\u200b/\u200b\u9662\u7cfb\u200b\u4ecb\u7ecd\u200b\u7b49\u200b\u677f\u5757\u200b\uff0c\u200b\u6b22\u8fce\u200b\u6709\u610f\u200b\u53d1\u5e03\u200b\u5185\u5bb9\u200b\u7684\u200b\u8001\u5e08\u200b\u6216\u200b\u540c\u5b66\u200b \u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u3002</p> <p>\u200b\u5e0c\u671b\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u540c\u5b66\u200b\u52a0\u5165\u200b\u8fdb\u6765\u200b\uff0c\u200b\u5e2e\u52a9\u200b\u64b0\u5199\u200b\u548c\u200b\u5b8c\u5584\u200b\u7f51\u7ad9\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u63d0\u4ea4\u200b\u7533\u8bf7\u200b\u603b\u7ed3\u200b\u8bf7\u200b\u524d\u5f80\u200b\u98de\u8dc3\u200b\u6570\u636e\u5e93\u200b\u3002</p> <p>\u200b\u672c\u624b\u518c\u200b\u8fd8\u200b\u5728\u200b\u5feb\u901f\u200b\u5efa\u8bbe\u200b\u4e2d\u200b\uff0c\u200b\u6b22\u8fce\u200b\u4f60\u200b\u63d0\u51fa\u200b\u610f\u89c1\u200b\u6216\u200b\u5efa\u8bae\u200b\u3002</p>"},{"location":"main/appendix.html","title":"Appendix","text":"\u540d\u8bcd\u200b \u200b\u89e3\u91ca\u200b bg \u200b\u80cc\u666f\u200b\uff08background\uff09 tl timeline HYPSM/\u200b\u54c8\u8036\u666e\u65af\u9ebb\u200b \u200b\u54c8\u4f5b\u200b\u3001\u200b\u8036\u9c81\u200b\u3001\u200b\u666e\u6797\u65af\u987f\u200b\u3001\u200b\u65af\u5766\u798f\u200b\u3001\u200b\u9ebb\u7701\u7406\u5de5\u200b\u4e94\u6240\u200b\u5927\u5b66\u200b\u7684\u200b\u7b80\u79f0\u200b bar \u200b\u95e8\u69db\u200b wl \u200b\u7b49\u5f85\u200b\u540d\u5355\u200b/\u200b\u517b\u9c7c\u200b\u9c7c\u5858\u200b\uff08waitlist\uff09 \u200b\u4e09\u7ef4\u200b \u200b\u8bed\u8a00\u200b&amp;GRE&amp;GPA pub publication RA reseach assistant TA teaching assistant ad admission\uff0c\u200b\u5c24\u5176\u200b\u6307\u65e0\u5956\u200b\u6216\u200b\u5c0f\u5956\u200b dp datapoint"},{"location":"main/committee.html","title":"Committee","text":""},{"location":"main/committee.html#_1","title":"\u6df1\u5733\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\u7f16\u5199\u200b\u59d4\u5458\u4f1a","text":""},{"location":"main/committee.html#_2","title":"\u59d4\u5458\u4f1a\u200b\u6210\u5458","text":"<ul> <li>\u200b\u9648\u5929\u884c\u200b\uff0c2025\u200b\u5c4a\u200b\u8ba1\u8f6f\u200b\u672c\u79d1\u751f\u200b (\u200b\u521b\u59cb\u4eba\u200b)</li> <li>\u200b\u5173\u6d69\u6797\u200b\uff0c2026\u200b\u5c4a\u200b\u8ba1\u8f6f\u200b\u672c\u79d1\u751f\u200b</li> <li>\u200b\u5f6d\u5c0f\u521a\u200b\uff0c\u200b\u6df1\u5733\u5927\u5b66\u200b\u526f\u6559\u6388\u200b</li> </ul>"},{"location":"main/committee.html#_3","title":"\u8054\u7cfb\u200b\u4e0e\u200b\u652f\u6301","text":"<p>\u200b\u8d1f\u8d23\u4eba\u200b\u90ae\u7bb1\u200b: chentianxing2002@gmail.com \u200b\u8d1f\u8d23\u4eba\u200b\u5fae\u4fe1\u200b: TianxingChen_2002</p>"},{"location":"main/committee.html#_4","title":"\u58f0\u660e","text":"<p>\u200b\u672c\u624b\u518c\u200b\u4f7f\u7528\u200b Material for Mkdocs \u200b\u6784\u5efa\u200b\uff0c\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u5f00\u6e90\u200b\uff0c\u200b\u4f7f\u7528\u200b GitHub Pages \u200b\u516c\u5f00\u200b\u53d1\u5e03\u200b\u3002</p> <p>\u200b\u624b\u518c\u200b\u5185\u5bb9\u200b\u7531\u200b \u200b\u6df1\u5733\u5927\u5b66\u200b\u98de\u8dc3\u200b\u624b\u518c\u200b\u7f16\u5199\u200b\u59d4\u5458\u4f1a\u200b \u200b\u6536\u96c6\u200b\u3001\u200b\u7f16\u5199\u200b\u3001\u200b\u5ba1\u6838\u200b\u3001\u200b\u53d1\u5e03\u200b\u5e76\u200b\u7ef4\u62a4\u200b\u3002\u200b\u5185\u5bb9\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\uff0c\u200b\u4e14\u200b\u4e0d\u200b\u4ee3\u8868\u200b\u4efb\u4f55\u200b\u653f\u6cbb\u7acb\u573a\u200b\u3002\u200b\u624b\u518c\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u4f9d\u636e\u200bCC BY-NC 4.0\u200b\u6388\u6743\u200b\uff0c\u200b\u4e0d\u5f97\u200b\u505a\u200b\u5546\u4e1a\u7528\u9014\u200b\uff0c\u200b\u8f6c\u8f7d\u200b\u6216\u8005\u200b\u5f15\u7528\u200b\u8bf7\u200b\u6ce8\u660e\u200b\u6765\u6e90\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u4e0d\u200b\u9075\u5b88\u200b\u6b64\u200b\u58f0\u660e\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u8fdd\u6cd5\u200b\u4f7f\u7528\u200b\u672c\u6587\u200b\u5185\u5bb9\u200b\u8005\u200b\uff0c\u200b\u4f9d\u6cd5\u200b\u4fdd\u7559\u200b\u8ffd\u7a76\u200b\u6743\u7b49\u200b\u3002</p>"},{"location":"main/contributions.html","title":"Contributions","text":""},{"location":"main/contributions.html#project-leaders","title":"Project Leaders","text":"<p>Tianxing Chen, Yao Mu, Zhixuan Liang</p>"},{"location":"main/contributions.html#1-roadmap-methodology","title":"1. Roadmap &amp; Methodology","text":"<p>Yao Mu, Tianxing Chen, Ping Luo, Yusen Qin, Xiaokang Yang, Kaixuan Wang</p>"},{"location":"main/contributions.html#2-data-generator-benchmark","title":"2. Data Generator &amp; Benchmark","text":"<p>Tianxing Chen, Zanxin Chen, Baijun Chen, Qiwei Liang, Zixuan Li, Xianliang Lin</p>"},{"location":"main/contributions.html#3-codegen-agent","title":"3. CodeGen Agent","text":"<p>Yibin Liu, Zanxin Chen, Yiheng Ge, Tianxing Chen, Mengkang Hu</p>"},{"location":"main/contributions.html#4-robotwin-od","title":"4. RoboTwin-OD","text":"<p>Baijun Chen, Qiangyu Chen, Kailun Su, Xuanbing Xie, Zanxin Chen</p>"},{"location":"main/contributions.html#5-policies-training-evaluation","title":"5. Policies Training &amp; Evaluation","text":"<p>Tianxing Chen, Zijian Cai, Tian Nian, Huan-ang Gao, Tianling Xu</p>"},{"location":"main/contributions.html#6-real-world-deployment","title":"6. Real-World Deployment","text":"<p>Tianxing Chen, Tian Nian, Weiliang Deng</p>"},{"location":"main/contributions.html#7-domain-randomization","title":"7. Domain Randomization","text":"<p>Baijun Chen, Yubin Guo, Qiwei Liang, Zhenyu Gu, Guodong Liu, Zanxin Chen, Tianxing Chen</p>"},{"location":"objects/index.html","title":"RoboTwin-OD (RoboTwin Object Dataset)","text":"<p>This is a document containing images and ID information for all objects in RoboTwin-OD, excluding those from the Objaverse dataset.</p> <p>To enhance both manipulation capability and visual understanding, we construct a large-scale object dataset with rich semantic annotations, called RoboTwin-OD, covering 147 categories and 731 diverse objects. Specifically, this includes 534 instances across 111 categories with custom-generated and optimized meshes, 153 objects from 27 categories in Objaverse, and 44 articulated object instances from 9 categories in SAPIEN PartNet-Mobility. Objects from all sources, including Objaverse, are used for cluttered scene construction, with Objaverse specifically serving to further increase the visual and semantic diversity of distractor objects. Additionally, we develop a comprehensive surface and background texture library using generative AI and human-in-the-loop verification to ensure both diversity and realism. The dataset is available at https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects.</p>"},{"location":"objects/001_bottle.html","title":"001_Bottle","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p> <p>base11</p> <p>base12</p> <p>base13</p> <p>base14</p> <p>base15</p> <p>base16</p> <p>base17</p> <p>base18</p> <p>base19</p> <p>base20</p> <p>base21</p> <p>base22</p>"},{"location":"objects/002_bowl.html","title":"002_Bowl","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/003_plate.html","title":"003_Plate","text":"<p>base0</p>"},{"location":"objects/004_fluted-block.html","title":"004_Fluted-Block","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/005_french-fries.html","title":"005_French-Fries","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/006_hamburg.html","title":"006_Hamburg","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/007_shoe-box.html","title":"007_Shoe-Box","text":"<p>base0</p>"},{"location":"objects/008_tray.html","title":"008_Tray","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/009_kettle.html","title":"009_Kettle","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/010_pen.html","title":"010_Pen","text":"Missing images for the pen object."},{"location":"objects/011_dustbin.html","title":"011_Dustbin","text":"<p>base0</p>"},{"location":"objects/012_plant-pot.html","title":"012_Plant-Pot","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/013_dumbbell-rack.html","title":"013_Dumbbell-Rack","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/014_bookcase.html","title":"014_Bookcase","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/015_laptop.html","title":"015_Laptop","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p>"},{"location":"objects/016_oven.html","title":"016_Oven","text":"Missing images for the oven object."},{"location":"objects/017_calculator.html","title":"017_Calculator","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/018_microphone.html","title":"018_Microphone","text":"<p>base0</p> <p>base1</p> <p>base4</p> <p>base5</p>"},{"location":"objects/019_coaster.html","title":"019_Coaster","text":"<p>base0</p>"},{"location":"objects/020_hammer.html","title":"020_Hammer","text":"<p>base0</p>"},{"location":"objects/021_cup.html","title":"021_Cup","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p> <p>base11</p> <p>base12</p>"},{"location":"objects/022_cup-with-liquid.html","title":"022_Cup-With-Liquid","text":"<p>base0</p>"},{"location":"objects/023_tissue-box.html","title":"023_Tissue-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/024_scanner.html","title":"024_Scanner","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/025_chips-tub.html","title":"025_Chips-Tub","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/026_pet-collar.html","title":"026_Pet-Collar","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/027_table-tennis.html","title":"027_Table-Tennis","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/028_roll-paper.html","title":"028_Roll-Paper","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/029_olive-oil.html","title":"029_Olive-Oil","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/030_drill.html","title":"030_Drill","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/031_jam-jar.html","title":"031_Jam-Jar","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/032_screwdriver.html","title":"032_Screwdriver","text":"<p>base0</p>"},{"location":"objects/033_fork.html","title":"033_Fork","text":"<p>base0</p>"},{"location":"objects/034_knife.html","title":"034_Knife","text":"<p>base0</p>"},{"location":"objects/035_apple.html","title":"035_Apple","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/036_cabinet.html","title":"036_Cabinet","text":"<p>base0</p>"},{"location":"objects/037_box.html","title":"037_Box","text":"<p>base0</p>"},{"location":"objects/038_milk-box.html","title":"038_Milk-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/039_mug.html","title":"039_Mug","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p> <p>base11</p> <p>base12</p>"},{"location":"objects/040_rack.html","title":"040_Rack","text":"<p>base0</p>"},{"location":"objects/041_shoe.html","title":"041_Shoe","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p>"},{"location":"objects/042_wooden_box.html","title":"042_Wooden_Box","text":"<p>base0</p>"},{"location":"objects/043_book.html","title":"043_Book","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/044_microwave.html","title":"044_Microwave","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/045_sand-clock.html","title":"045_Sand-Clock","text":"<p>base0</p> <p>base1</p> <p>base3</p>"},{"location":"objects/046_alarm-clock.html","title":"046_Alarm-Clock","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/047_mouse.html","title":"047_Mouse","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/048_stapler.html","title":"048_Stapler","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/049_shampoo.html","title":"049_Shampoo","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/050_bell.html","title":"050_Bell","text":"<p>base0</p> <p>base1</p>"},{"location":"objects/051_candlestick.html","title":"051_Candlestick","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/052_dumbbell.html","title":"052_Dumbbell","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/053_teanet.html","title":"053_Teanet","text":"<p>base1</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/054_baguette.html","title":"054_Baguette","text":"<p>base2</p> <p>base3</p>"},{"location":"objects/055_small-speaker.html","title":"055_Small-Speaker","text":"<p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/056_switch.html","title":"056_Switch","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/057_toycar.html","title":"057_Toycar","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/058_markpen.html","title":"058_Markpen","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/059_pencup.html","title":"059_Pencup","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/060_kitchenpot.html","title":"060_Kitchenpot","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/061_battery.html","title":"061_Battery","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/062_plasticbox.html","title":"062_Plasticbox","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p>"},{"location":"objects/063_tabletrashbin.html","title":"063_Tabletrashbin","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p> <p>base8</p> <p>base9</p> <p>base10</p>"},{"location":"objects/064_msg.html","title":"064_Msg","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/065_soy-sauce.html","title":"065_Soy-Sauce","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/066_vinegar.html","title":"066_Vinegar","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/067_steamer.html","title":"067_Steamer","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/068_boxdrink.html","title":"068_Boxdrink","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/069_vagetable.html","title":"069_Vagetable","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/070_paymentsign.html","title":"070_Paymentsign","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/071_can.html","title":"071_Can","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base5</p> <p>base6</p>"},{"location":"objects/072_electronicscale.html","title":"072_Electronicscale","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base5</p> <p>base6</p>"},{"location":"objects/073_rubikscube.html","title":"073_Rubikscube","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/074_displaystand.html","title":"074_Displaystand","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/075_bread.html","title":"075_Bread","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/076_breadbasket.html","title":"076_Breadbasket","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/077_phone.html","title":"077_Phone","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/078_phonestand.html","title":"078_Phonestand","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/079_remotecontrol.html","title":"079_Remotecontrol","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/080_pillbottle.html","title":"080_Pillbottle","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/081_playingcards.html","title":"081_Playingcards","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/082_smallshovel.html","title":"082_Smallshovel","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/083_brush.html","title":"083_Brush","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/084_woodenmallet.html","title":"084_Woodenmallet","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/085_gong.html","title":"085_Gong","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/086_woodenblock.html","title":"086_Woodenblock","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/087_waterer.html","title":"087_Waterer","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p> <p>base7</p>"},{"location":"objects/088_wineglass.html","title":"088_Wineglass","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/089_globe.html","title":"089_Globe","text":"<p>base2</p> <p>base3</p>"},{"location":"objects/090_trophy.html","title":"090_Trophy","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/091_kettle.html","title":"091_Kettle","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/092_notebook.html","title":"092_Notebook","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/093_brush-pen.html","title":"093_Brush-Pen","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/094_rest.html","title":"094_Rest","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/095_glue.html","title":"095_Glue","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/096_cleaner.html","title":"096_Cleaner","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/097_screen.html","title":"097_Screen","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/098_speaker.html","title":"098_Speaker","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/099_fan.html","title":"099_Fan","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/100_seal.html","title":"100_Seal","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base6</p>"},{"location":"objects/101_milk-tea.html","title":"101_Milk-Tea","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/102_roller.html","title":"102_Roller","text":"<p>base0</p> <p>base1</p> <p>base2</p>"},{"location":"objects/103_fruit.html","title":"103_Fruit","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/104_board.html","title":"104_Board","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/105_sauce-can.html","title":"105_Sauce-Can","text":"<p>base0</p> <p>base2</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/106_skillet.html","title":"106_Skillet","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/107_soap.html","title":"107_Soap","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/108_block.html","title":"108_Block","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/109_hydrating-oil.html","title":"109_Hydrating-Oil","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base5</p>"},{"location":"objects/110_basket.html","title":"110_Basket","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/111_callbell.html","title":"111_Callbell","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/112_tea-box.html","title":"112_Tea-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p>"},{"location":"objects/113_coffee-box.html","title":"113_Coffee-Box","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p> <p>base5</p> <p>base6</p>"},{"location":"objects/114_bottle.html","title":"114_Bottle","text":"<p>base1</p> <p>base2</p> <p>base3</p> <p>base4</p>"},{"location":"objects/115_perfume.html","title":"115_Perfume","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/116_keyboard.html","title":"116_Keyboard","text":"<p>base0</p> <p>base1</p> <p>base2</p> <p>base3</p>"},{"location":"objects/117_whiteboard-eraser.html","title":"117_Whiteboard-Eraser","text":"<p>base0</p>"},{"location":"objects/118_tooth-paste.html","title":"118_Tooth-Paste","text":"<p>base0</p>"},{"location":"objects/119_mini-chalkboard.html","title":"119_Mini-Chalkboard","text":"<p>base0</p>"},{"location":"objects/120_plant.html","title":"120_Plant","text":"<p>base0</p>"},{"location":"tasks/index.html","title":"50 RoboTwin 2.0 Tasks","text":"<p>This document introduces the 50 bimanual manipulation tasks in RoboTwin 2.0, including task videos, descriptions, average step lengths, involved objects, and success rates across different robot embodiments.</p> <p>Building on our automated task generation framework, embodiment-adaptive behavior synthesis, and the large-scale object asset library RoboTwin-OD, we construct a suite of over 50 dual-arm collaborative manipulation tasks. In addition, we support data collection and evaluation across 5 distinct robot platforms, enabling comprehensive benchmarking of manipulation policies. The complete task set is available at http://robotwin-platform.github.io/doc/tasks/.</p>"},{"location":"tasks/adjust_bottle.html","title":"Adjust Bottle","text":"Description: Pick up the bottle on the table headup with the correct arm. Average Steps: 147 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 93% 94% 34% 0% 12%"},{"location":"tasks/beat_block_hammer.html","title":"Beat Block Hammer","text":"Description: There is a hammer and a block on the table, use the arm to grab the hammer and beat the block. Average Steps: 113 (Aloha-AgileX, save_freq=15) Objects: 020_hammer, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 64% 93% 98% 15% 90%"},{"location":"tasks/blocks_ranking_rgb.html","title":"Blocks Ranking RGB","text":"Description: Place the red block, green block, and blue block in the order of red, green, and blue from left to right, placing in a row. Average Steps: 466 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 96% 97% 99% 13% 53%"},{"location":"tasks/blocks_ranking_size.html","title":"Blocks Ranking Size","text":"Description: There are three blocks on the table, the color of the blocks is random, move the blocks to the center of the table, and arrange them from largest to smallest, from left to right. Average Steps: 466 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 96% 97% 89% 7% 38%"},{"location":"tasks/click_alarmclock.html","title":"Click Alarmclock","text":"Description: Click the alarm clock's center of the top side button on the table. Average Steps: 85 (Aloha-AgileX, save_freq=15) Objects: 046_alarm-clock Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 92% 99% 100% 0% 95%"},{"location":"tasks/click_bell.html","title":"Click Bell","text":"Description: Click the bell's top center on the table. Average Steps: 85 (Aloha-AgileX, save_freq=15) Objects: 050_bell Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 100% 100% 100% 91% 100%"},{"location":"tasks/dump_bin_bigbin.html","title":"Dump Bin Bigbin","text":"Description: Grab the small bin and pour the balls into the big bin. Average Steps: 265 (Aloha-AgileX, save_freq=15) Objects: 011_dustbin, 063_tabletrashbin Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 84% 100% 84% 9% 80%"},{"location":"tasks/grab_roller.html","title":"Grab Roller","text":"Description: Use both arms to grab the roller on the table. Average Steps: 94 (Aloha-AgileX, save_freq=15) Objects: 102_roller Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 95% 69% 99% 7% 81%"},{"location":"tasks/handover_block.html","title":"Handover Block","text":"Description: Use the left arm to grasp the red block on the table, handover it to the right arm and place it on the blue pad. Average Steps: 283 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 83% 81% 0% 44% 0%"},{"location":"tasks/handover_mic.html","title":"Handover Mic","text":"Description: Use one arm to grasp the microphone on the table and handover it to the other arm. Average Steps: 223 (Aloha-AgileX, save_freq=15) Objects: 018_microphone Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 87% 98% 84% 65% 14%"},{"location":"tasks/hanging_mug.html","title":"Hanging Mug","text":"Description: Use left arm to pick the mug on the table, rotate the mug and put the mug down in the middle of the table, use the right arm to pick the mug and hang it onto the rack. Average Steps: 340 (Aloha-AgileX, save_freq=15) Objects: 039_mug, 040_rack Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 63% 73% 11% 0% 11%"},{"location":"tasks/lift_pot.html","title":"Lift Pot","text":"Description: Use arms to lift the pot. Average Steps: 112 (Aloha-AgileX, save_freq=15) Objects: 060_kitchenpot Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 27% 50% 36% 31% 40%"},{"location":"tasks/move_can_pot.html","title":"Move Can Pot","text":"Description: There is a can and a pot on the table, use one arm to pick up the can and move it to beside the pot. Average Steps: 151 (Aloha-AgileX, save_freq=15) Objects: 060_kitchenpot, 105_sauce-can Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 93% 65% 92% 96% 99%"},{"location":"tasks/move_pillbottle_pad.html","title":"Move Pillbottle Pad","text":"Description: Use one arm to pick the pillbottle and place it onto the pad. Average Steps: 147 (Aloha-AgileX, save_freq=15) Objects: 080_pillbottle, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 67% 90% 69% 47% 86%"},{"location":"tasks/move_playingcard_away.html","title":"Move Playingcard Away","text":"Description: Use the arm to pick up the playing card and move it away from the table. For example, if the playing card is on the outward side of the table, you should move it further outward side of the table. Average Steps: 120 (Aloha-AgileX, save_freq=15) Objects: 081_playingcards Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 99% 100% 100% 63% 66%"},{"location":"tasks/move_stapler_pad.html","title":"Move Stapler Pad","text":"Description: Use appropriate arm to move the stapler to a colored mat. Average Steps: 152 (Aloha-AgileX, save_freq=15) Objects: 048_stapler Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 92% 96% 89% 13% 75%"},{"location":"tasks/open_laptop.html","title":"Open Laptop","text":"Description: Use one arm to open the laptop. Average Steps: 258 (Aloha-AgileX, save_freq=15) Objects: 015_laptop Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 82% 92% 77% 23% 51%"},{"location":"tasks/open_microwave.html","title":"Open Microwave","text":"Description: Use one arm to open the microwave. Average Steps: 537 (Aloha-AgileX, save_freq=15) Objects: 044_microwave Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 96% 80% 59% 2% 23%"},{"location":"tasks/pick_diverse_bottles.html","title":"Pick Diverse Bottles","text":"Description: Pick up one bottle with one arm, and pick up another bottle with the other arm. Average Steps: 122 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 51% 2% 0% 27% 4%"},{"location":"tasks/pick_dual_bottles.html","title":"Pick Dual Bottles","text":"Description: Pick up one bottle with one arm, and pick up another bottle with the other arm. Average Steps: 127 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 92% 6% 0% 81% 7%"},{"location":"tasks/place_a2b_left.html","title":"Place A2B Left","text":"Description: Use appropriate arm to place object A on the left of object B. Average Steps: 155 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 086_woodenblock, 107_soap, 112_tea-box, 113_coffee-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 80% 88% 64% 28% 76%"},{"location":"tasks/place_a2b_right.html","title":"Place A2B Right","text":"Description: Use appropriate arm to place object A on the right of object B. Average Steps: 145 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 086_woodenblock, 107_soap, 112_tea-box, 113_coffee-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 81% 82% 64% 31% 66%"},{"location":"tasks/place_bread_basket.html","title":"Place Bread Basket","text":"Description: If there is one bread on the table, use one arm to grab the bread and put it in the basket, if there are two breads on the table, use two arms to simultaneously grab up two breads and put them in the basket. Average Steps: 231 (Aloha-AgileX, save_freq=15) Objects: 075_bread, 076_breadbasket Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 89% 88% 62% 1% 67%"},{"location":"tasks/place_bread_skillet.html","title":"Place Bread Skillet","text":"Description: If there is one bread on the table, use one arm to grab the bread and put it into the skillet. Average Steps: 162 (Aloha-AgileX, save_freq=15) Objects: 075_bread, 106_skillet Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 34% 26% 42% 0% 37%"},{"location":"tasks/place_burger_fries.html","title":"Place Burger Fries","text":"Description: Use dual arm to pick the hamburg and frenchfries and put them onto the tray. Average Steps: 242 (Aloha-AgileX, save_freq=15) Objects: 005_french-fries, 006_hamburg, 008_tray Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 97% 98% 80% 36% 74%"},{"location":"tasks/place_can_basket.html","title":"Place Can Basket","text":"Description: Use one arm to pick up the can and another arm place it in the basket. Average Steps: 255 (Aloha-AgileX, save_freq=15) Objects: 071_can, 110_basket Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 70% 28% 61% 0% 3%"},{"location":"tasks/place_cans_plasticbox.html","title":"Place Cans Plasticbox","text":"Description: Use dual arm to pick and place cans into plasticbox. Average Steps: 289 (Aloha-AgileX, save_freq=15) Objects: 062_plasticbox, 071_can Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 100% 96% 85% 0% 82%"},{"location":"tasks/place_container_plate.html","title":"Place Container Plate","text":"Description: Place the container onto the plate. Average Steps: 156 (Aloha-AgileX, save_freq=15) Objects: 002_bowl, 003_plate, 021_cup Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 89% 86% 86% 37% 81%"},{"location":"tasks/place_dual_shoes.html","title":"Place Dual Shoes","text":"Description: Use both arms to pick up the two shoes on the table and put them in the shoebox, with the shoe tip pointing to the left. Average Steps: 228 (Aloha-AgileX, save_freq=15) Objects: 007_shoe-box, 041_shoe Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 77% 31% 41% 1% 32%"},{"location":"tasks/place_empty_cup.html","title":"Place Empty Cup","text":"Description: Use an arm to place the empty cup on the coaster. Average Steps: 174 (Aloha-AgileX, save_freq=15) Objects: 019_coaster, 021_cup Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 92% 100% 100% 4% 100%"},{"location":"tasks/place_fan.html","title":"Place Fan","text":"Description: Grab the fan and place it on a colored mat, and make sure the fan is facing the robot. Average Steps: 148 (Aloha-AgileX, save_freq=15) Objects: 099_fan, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 95% 93% 83% 0% 65%"},{"location":"tasks/place_mouse_pad.html","title":"Place Mouse Pad","text":"Description: Grab the mouse and place it on a colored mat. Average Steps: 149 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 99% 89% 100% 23% 73%"},{"location":"tasks/place_object_basket.html","title":"Place Object Basket","text":"Description: Use one arm to grab the target object and put it in the basket, then use the other arm to grab the basket, and finally move the basket slightly away. Average Steps: 252 (Aloha-AgileX, save_freq=15) Objects: 057_toycar, 081_playingcards, 110_basket Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 74% 14% 61% 0% 7%"},{"location":"tasks/place_object_scale.html","title":"Place Object Scale","text":"Description: Use one arm to grab the object and put it on the scale. Average Steps: 146 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 072_electronicscale Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 78% 92% 82% 2% 76%"},{"location":"tasks/place_object_stand.html","title":"Place Object Stand","text":"Description: Use appropriate arm to place the object on the stand. Average Steps: 138 (Aloha-AgileX, save_freq=15) Objects: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 074_displaystand, 079_remotecontrol Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 97% 99% 81% 9% 92%"},{"location":"tasks/place_phone_stand.html","title":"Place Phone Stand","text":"Description: Pick up the phone and put it on the phone stand. Average Steps: 130 (Aloha-AgileX, save_freq=15) Objects: 077_phone, 078_phonestand Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 66% 78% 45% 53% 49%"},{"location":"tasks/place_shoe.html","title":"Place Shoe","text":"Description: Use one arm to grab the shoe from the table and place it on the mat. Average Steps: 178 (Aloha-AgileX, save_freq=15) Objects: 041_shoe, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 84% 85% 74% 7% 91%"},{"location":"tasks/press_stapler.html","title":"Press Stapler","text":"Description: Use one arm to press the stapler. Average Steps: 141 (Aloha-AgileX, save_freq=15) Objects: 048_stapler Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 98% 96% 100% 59% 72%"},{"location":"tasks/put_bottles_dustbin.html","title":"Put Bottles Dustbin","text":"Description: Use arms to grab the bottles and put them into the dustbin to the left of the table. Average Steps: 637 (Aloha-AgileX, save_freq=15) Objects: 011_dustbin, 114_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 71% 1% 0% 56% 0%"},{"location":"tasks/put_object_cabinet.html","title":"Put Object Cabinet","text":"Description: Use one arm to open the cabinet's drawer, and use another arm to put the object on the table to the drawer. Average Steps: 274 (Aloha-AgileX, save_freq=15) Objects: 036_cabinet, 047_mouse, 048_stapler, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 107_soap, 112_tea-box, 113_coffee-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 14% 24% 55% 0% 0%"},{"location":"tasks/rotate_qrcode.html","title":"Rotate QRcode","text":"Description: Use arm to catch the qrcode board on the table, pick it up and rotate to let the qrcode face towards the robot. Average Steps: 155 (Aloha-AgileX, save_freq=15) Objects: 070_paymentsign Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 75% 74% 94% 0% 67%"},{"location":"tasks/scan_object.html","title":"Scan Object","text":"Description: Use one arm to pick the scanner and use the other arm to pick the object, and use the scanner to scan the object. Average Steps: 170 (Aloha-AgileX, save_freq=15) Objects: 024_scanner, 112_tea-box Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 4% 45% 26% 0% 19%"},{"location":"tasks/shake_bottle.html","title":"Shake Bottle","text":"Description: Shake the bottle with proper arm. Average Steps: 246 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 89% 94% 85% 74% 97%"},{"location":"tasks/shake_bottle_horizontally.html","title":"Shake Bottle Horizontally","text":"Description: Shake the bottle horizontally with proper arm. Average Steps: 276 (Aloha-AgileX, save_freq=15) Objects: 001_bottle Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 90% 94% 85% 74% 98%"},{"location":"tasks/stack_blocks_three.html","title":"Stack Blocks Three","text":"Description: There are three blocks on the table, the color of the blocks is red, green and blue. Move the blocks to the center of the table, and stack the blue block on the green block, and the green block on the red block. Average Steps: 481 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 94% 96% 80% 0% 51%"},{"location":"tasks/stack_blocks_two.html","title":"Stack Blocks Two","text":"Description: There are two blocks on the table, the color of the blocks is red, green. Move the blocks to the center of the table, and stack the geen block on the red block. Average Steps: 316 (Aloha-AgileX, save_freq=15) Objects: block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 98% 99% 96% 2% 68%"},{"location":"tasks/stack_bowls_three.html","title":"Stack Bowls Three","text":"Description: Stack the three bowls on top of each other. Average Steps: 476 (Aloha-AgileX, save_freq=15) Objects: 002_bowl Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 43% 57% 82% 0% 81%"},{"location":"tasks/stack_bowls_two.html","title":"Stack Bowls Two","text":"Description: Stack the two bowls on top of each other. Average Steps: 313 (Aloha-AgileX, save_freq=15) Objects: 002_bowl Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 78% 82% 88% 4% 94%"},{"location":"tasks/stamp_seal.html","title":"Stamp Seal","text":"Description: Grab the stamp and stamp onto the specific color mat. Average Steps: 151 (Aloha-AgileX, save_freq=15) Objects: 100_seal, block Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 56% 91% 4% 37% 100%"},{"location":"tasks/template.html","title":"Template","text":"<p>\u200b\u683c\u5f0f\u200b\u4f1a\u200b\u6709\u200b\u4e13\u4eba\u200b\u8c03\u6574\u200b\uff0c\u200b\u5404\u4f4d\u200b\u53ea\u200b\u9700\u8981\u200b\u586b\u5199\u5185\u5bb9\u200b\u5373\u53ef\u200b</p>"},{"location":"tasks/template.html#-xxxx","title":"\u79f0\u8c13\u200b - \u200b\u53bb\u5904\u200bxxxx\u200b\u5c4a\u200b, \u200b\u5b66\u9662\u200b, \u200b\u4e13\u4e1a","text":"<p>\uff08\u200b\u9009\u586b\u200b\uff09\u200b\u60f3\u200b\u9001\u7ed9\u200b\u5b66\u5f1f\u200b\u5b66\u59b9\u200b\u7684\u200b\u4e00\u53e5\u200b\u8bdd\u200b: ...</p>"},{"location":"tasks/template.html#_1","title":"\u80cc\u666f","text":"<p>GPA: \u00a0\u00a0\u00a0\u00a0\u200b\u7b49\u7ea7\u5236\u200b: 4.00 / 4.5, \u200b\u767e\u5206\u5236\u200b: 88.00 / 100, \u200b\u4e13\u4e1a\u200b\u6392\u540d\u200b: 5 / 100</p> <p>\u200b\u7ade\u8d5b\u200b: \u00a0\u00a0\u00a0\u00a0- xxx\u200b\u7ade\u8d5b\u200bxxx\u200b\u5956\u200b</p> <p>\u200b\u79d1\u7814\u200b: \u00a0\u00a0\u00a0\u00a0- \u200b\u8bba\u6587\u200b\u53d1\u8868\u200b</p> <p>\u200b\u5b9e\u4e60\u200b: \u00a0\u00a0\u00a0\u00a0- xxx\u200b\u516c\u53f8\u200b, \u200b\u5b9e\u4e60\u200b\u5c97\u4f4d\u200b, \u200b\u5b9e\u4e60\u200b\u671f\u9650\u200b</p> <p>\u200b\u9879\u76ee\u200b: \u00a0\u00a0\u00a0\u00a0- xxx\u200b\u9879\u76ee\u200b, \u200b\u4e3b\u8981\u200b\u8d21\u732e\u200b\u7b49\u200b</p> <p>\u200b\u4e2a\u4eba\u200b\u8363\u8a89\u200b: \u00a0\u00a0\u00a0\u00a0- \u200b\u5956\u5b66\u91d1\u200b\u7b49\u200b</p> <p>\u200b\u4e2a\u4eba\u200b\u8d23\u4efb\u200b: \u00a0\u00a0\u00a0\u00a0- \u200b\u73ed\u5e72\u200b\u3001\u200b\u793e\u56e2\u200b\u8d1f\u8d23\u4eba\u200b\u7b49\u200b</p>"},{"location":"tasks/template.html#_2","title":"\u7533\u8bf7\u200b\u60c5\u51b5","text":"<p>ps: \u200b\u53ef\u4ee5\u200b\u5217\u51fa\u200b\u4f60\u200b\u7684\u200b\u7533\u8bf7\u200b\u60c5\u51b5\u200b (\u274c\u2705), \u200b\u53ef\u200b\u53c2\u8003\u200b2024\u200b\u5c4a\u200b\u4fdd\u7814\u200b\u7ecf\u9a8c\u200b\u8d34\u200b\uff08\u200b\u897f\u4ea4\u200b\uff0c\u200b\u4e0a\u4ea4\u200b\u7535\u9662\u200b\uff0c\u200b\u54c8\u5de5\u5927\u200b\u7535\u4fe1\u200b\uff0c\u200b\u6d59\u63a7\u200b\uff0c\u200b\u4e2d\u79d1\u5927\u200b\uff0c\u200b\u81ea\u52a8\u5316\u200b\u6240\u200b\uff09, \u200b\u4f8b\u5982\u200b\u6e05\u6df1\u200bCS\u274c, \u200b\u5317\u5927\u200b\u8f6f\u5fae\u200b\u2705</p> \u200b\u7533\u8bf7\u200b\u9879\u76ee\u200b \u200b\u7ed3\u679c\u200b \u200b\u4f8b\u200b\uff1a\u200b\u5357\u200b\u79d1\u5927\u200bCS \u2705 <p>\u200b\u6700\u7ec8\u200b\u53bb\u5411\u200b (\u200b\u9662\u6821\u200b+\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b): ...</p>"},{"location":"tasks/template.html#_3","title":"\u7ecf\u5386\u200b\u5206\u4eab","text":"<p>ps: \u200b\u53ef\u4ee5\u200b\u8be6\u7ec6\u200b\u5206\u4eab\u200b\u4f60\u200b\u7684\u200b\u7533\u8bf7\u200b\u7ecf\u5386\u200b\uff0c\u200b\u7a7f\u63d2\u200b\u611f\u609f\u200b\u3002\u200b\u5373\u4f7f\u200b\u662f\u200b\u5931\u8d25\u200b\u7684\u200b\u7ecf\u5386\u200b\u4e5f\u200b\u662f\u200b\u975e\u5e38\u200b\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002</p>"},{"location":"tasks/template.html#_4","title":"\u603b\u7ed3","text":"<p>\u200b\u5bf9\u200b\u7533\u8bf7\u200b\u5b63\u200b\u7684\u200b\u7ecf\u5386\u200b\u8fdb\u884c\u200b\u603b\u7ed3\u200b\u3002</p>"},{"location":"tasks/template.html#_5","title":"\u8054\u7cfb\u65b9\u5f0f\u200b\uff08\u200b\u9009\u586b\u200b\uff09","text":"<p>\u00a0\u00a0\u00a0\u00a0- \u200b\u4e3b\u9875\u200b: xxx \u00a0\u00a0\u00a0\u00a0- \u200b\u90ae\u7bb1\u200b: xxx \u00a0\u00a0\u00a0\u00a0- \u200b\u5fae\u4fe1\u200b: xxx</p>"},{"location":"tasks/template.html#_6","title":"\u5907\u6ce8","text":"<p>\u200b\u5173\u4e8e\u200b\u4ee5\u4e0a\u200b\u4fe1\u606f\u200b\u586b\u5199\u200b\u4e2d\u200b\u5b58\u5728\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\u7684\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6b64\u200b\u5217\u51fa\u200b\u3002</p>"},{"location":"tasks/turn_switch.html","title":"Turn Switch","text":"Description: Use the robotic arm to click the switch. Average Steps: 95 (Aloha-AgileX, save_freq=15) Objects: 056_switch Embodiments Aloha-AgileX ARX-X5 Franka-Panda Piper UR5-Wsg Data Generation Success Rate 74% 3% 36% 81% 10%"},{"location":"usage/index.html","title":"RoboTwin 2.0 Usage Guide","text":"<p>This documentation provides a comprehensive guide to using RoboTwin 2.0, covering environment setup, data collection and configuration, policy deployment, usage of demo policies, automatic code generation for new tasks, API tutorial, language instruction generation, and digital asset annotation.</p>"},{"location":"usage/ACT.html","title":"ACT (Action Chunking Transformer)","text":""},{"location":"usage/ACT.html#1-install","title":"1. Install","text":"<pre><code>cd policy/ACT\n\npip install pyquaternion pyyaml rospkg pexpect mujoco==2.3.7 dm_control==1.0.14 opencv-python matplotlib einops packaging h5py ipython\n\ncd detr &amp;&amp; pip install -e . &amp;&amp; cd ..\n</code></pre>"},{"location":"usage/ACT.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the format required for ACT training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data.</p> <pre><code>bash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data.sh beat_block_hammer demo_randomized 50\n</code></pre>"},{"location":"usage/ACT.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. By default, the model is trained for 6,000 steps.</p> <pre><code>bash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${gpu_id}\n# bash train.sh beat_block_hammer demo_randomized 50 0 0\n</code></pre>"},{"location":"usage/ACT.html#4-eval-policy","title":"4. Eval Policy","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>ckpt_setting</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized demo_randomized 50 0 0\n# This command trains the policy using the `demo_randomized` setting ($ckpt_setting)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean demo_randomized 50 0 0\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/API.html","title":"API for Controlling Mechanical Arms","text":"<p>The API can be used to control one or two robotic arms to perform operations such as grasping, placing, moving, and returning to the origin. Each arm is identified by an <code>ArmTag</code>, which can be <code>\"left\"</code> or <code>\"right\"</code>. Actions are generated in sequences and executed together via the <code>move()</code> method.</p>"},{"location":"usage/API.html#1-class-structure","title":"1. Class Structure","text":"<ul> <li><code>self</code>: The task class inherit from <code>Base_Task</code>.</li> <li><code>ArmTag</code>: A custom type representing a robotic arm. It supports comparison with strings: <code>ArmTag(\"left\") == \"left\"</code> returns <code>True</code>. You can obtain the opposite arm using <code>ArmTag(\"left\").opposite</code>, i.e., <code>ArmTag(\"left\").opposite == \"right\"</code> returns <code>True</code>.</li> <li><code>Actor</code>/<code>ArticulationActor</code>: The object being manipulated. Provides methods to retrieve key points (contact point <code>contact_point</code>, functional point <code>functional_point</code>, target point <code>target_point</code>) and its current global pose.</li> <li><code>Action</code>: A sequence of actions for controlling the arm. You only need to know that it can be executed via the <code>move()</code> function.</li> </ul>"},{"location":"usage/API.html#2-controlling-apis","title":"2. Controlling APIs","text":""},{"location":"usage/API.html#21-moveself-actions_by_arm1-tuplearmtag-listaction-actions_by_arm2-tuplearmtag-listaction-none","title":"2.1 <code>move(self, actions_by_arm1: tuple[ArmTag, list[Action]], actions_by_arm2: tuple[ArmTag, list[Action]] = None)</code>","text":""},{"location":"usage/API.html#211-description","title":"2.1.1 Description","text":"<p>Executes action sequences on one or both robotic arms simultaneously.</p>"},{"location":"usage/API.html#212-parameters","title":"2.1.2 Parameters","text":"<ul> <li><code>actions_by_arm1</code>: Action sequence for the first arm, formatted as <code>(arm_tag, [action1, action2, ...])</code></li> <li><code>actions_by_arm2</code>: Optional, action sequence for the second arm</li> </ul>"},{"location":"usage/API.html#213-notes","title":"2.1.3 Notes","text":"<ul> <li>The same <code>ArmTag</code> cannot be passed twice.</li> <li>All actions must have been pre-generated.</li> </ul>"},{"location":"usage/API.html#214-example","title":"2.1.4 Example","text":"<p>One arm grasps a bottle, the other moves back to avoid interference. <pre><code>self.move(\n    self.grasp_actor(self.bottle, arm_tag=arm_tag),\n    self.back_to_origin(arm_tag=arm_tag.opposite)\n)\n</code></pre></p>"},{"location":"usage/API.html#22-grasp_actorself-actor-actor-arm_tag-armtag-pre_grasp_dis01-grasp_dis0-gripper_pos0-contact_point_idnone-tuplearmtag-listaction","title":"2.2 <code>grasp_actor(self, actor: Actor, arm_tag: ArmTag, pre_grasp_dis=0.1, grasp_dis=0, gripper_pos=0., contact_point_id=None) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#221-description","title":"2.2.1 Description","text":"<p>Generates a sequence of actions to pick up the specified <code>Actor</code>.</p>"},{"location":"usage/API.html#222-parameters","title":"2.2.2 Parameters","text":"<ul> <li><code>actor</code>: The object to grasp</li> <li><code>arm_tag</code>: Which arm to use</li> <li><code>pre_grasp_dis</code>: Pre-grasp distance (default 0.1 meters), the arm will move to this position first</li> <li><code>grasp_dis</code>: Grasping distance (default 0 meters), the arm moves from the pre-grasp position to this position and then closes the gripper</li> <li><code>gripper_pos</code>: Gripper closing position (default 0, fully closed)</li> <li><code>contact_point_id</code>: Optional list of contact point IDs; if not provided, the best grasping point is selected automatically</li> </ul>"},{"location":"usage/API.html#223-returns","title":"2.2.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the grasp actions.</p>"},{"location":"usage/API.html#224-example","title":"2.2.4 Example","text":"<p>Select appropriate grasp point based on arm_tag and grasp the cup. <pre><code>self.move(\n    self.grasp_actor(\n        self.cup, arm_tag=arm_tag,\n        pre_grasp_dis=0.1,\n        contact_point_id=[0, 2][int(arm_tag=='left')]\n    )\n)\n</code></pre></p>"},{"location":"usage/API.html#23-place_actorself-actor-actor-arm_tag-armtag-target_pose-list-npndarray-functional_point_id-int-none-pre_dis01-dis002-is_opentrue-kwargs-tuplearmtag-listaction","title":"2.3 <code>place_actor(self, actor: Actor, arm_tag: ArmTag, target_pose: list | np.ndarray, functional_point_id: int = None, pre_dis=0.1, dis=0.02, is_open=True, **kwargs) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#231-description","title":"2.3.1 Description","text":"<p>Places a currently held object at a specified target pose.</p>"},{"location":"usage/API.html#232-parameters","title":"2.3.2 Parameters","text":"<ul> <li><code>actor</code>: The currently held object</li> <li><code>arm_tag</code>: The arm holding the object</li> <li><code>target_pose</code>: Target position/orientation, length 3 or 7 (xyz + optional quaternion)</li> <li><code>functional_point_id</code>: Optional ID of the functional point; if provided, aligns this point to the target, otherwise aligns the base of the object</li> <li><code>pre_dis</code>: Pre-place distance (default 0.1 meters), arm moves to this position first</li> <li><code>dis</code>: Final placement distance (default 0.02 meters), arm moves from pre-place to this location, then opens the gripper</li> <li><code>is_open</code>: Whether to open the gripper after placing (default True)</li> <li><code>**kwargs</code>: Other optional parameters:<ul> <li><code>constrain : {'free', 'align', 'auto'}, default='auto'</code> Alignment strategy:<ul> <li><code>free</code>: Only forces the object's z-axis to align with the target point's z-axis, other axes are determined by projection.</li> <li><code>align</code>: Forces all axes of the object to align with all axes of the target point.</li> <li><code>auto</code>: Automatically selects a suitable placement pose based on grasp direction (vertical or horizontal).</li> </ul> </li> <li><code>align_axis : list of np.ndarray or np.ndarray or list, optional</code> Vectors or vector list in world coordinates to align with. For example, <code>[1, 0, 0]</code> or <code>[[1, 0, 0], [0, 1, 0]]</code>. If multiple vectors are provided, the one with the smallest dot product with the current actor axis will be chosen for alignment.</li> <li><code>actor_axis : np.ndarray or list, default=[1, 0, 0]</code> The second object axis used for alignment (the first is the z-axis which will be forced to align). Typically used for auxiliary alignment (especially when <code>constrain == 'align'</code>).</li> <li><code>actor_axis_type : {'actor', 'world'}, default='actor'</code> Specifies whether <code>actor_axis</code> is relative to the object coordinate system or world coordinate system.</li> <li><code>pre_dis_axis : {'grasp', 'fp'} or np.ndarray or list, default='grasp'</code> Specifies the pre-placement offset direction:<ul> <li><code>grasp</code>: Offset along the grasp direction (i.e., opposite to the end-effector pointing towards the object center).</li> <li><code>fp</code>: Offset along the target point's z-axis direction.</li> <li>Custom vectors can also be provided to represent the offset direction.</li> </ul> </li> </ul> </li> </ul>"},{"location":"usage/API.html#233-returns","title":"2.3.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the place actions.</p>"},{"location":"usage/API.html#234-example","title":"2.3.4 Example","text":"<p>When stacking one object on top of another (for example, placing blockA on top of blockB). <pre><code>target_pose = self.last_actor.get_functional_point(point_id, \"pose\")\n# Use this target_pose in place_actor to place the object exactly on top of last_actor at the specified functional point.\nself.move(\n    self.place_actor(\n        actor=self.current_actor, # The object to be placed\n        target_pose=target_pose, # The pose acquired from last_actor\n        arm_tag=arm_tag,\n        functional_point_id=0, # Align functional point 0, or specify as needed\n        pre_dis=0.1,\n        dis=0.02,\n        pre_dis_axis=\"fp\", # Use functional point direction for pre-displacement, if the functional point is used\n    )\n)\n</code></pre></p> <p>Place the actor at actor_pose (already a Pose object). <pre><code>self.move(\n    self.place_actor(\n        self.box,\n        target_pose=self.actor_pose, # already a Pose, no need for get_pose()\n        arm_tag=grasp_arm_tag,\n        functional_point_id=0, # functional_point_id can be retrived from the actor list if the actor has functional points\n        pre_dis=0,\n        dis=0,  # set dis to 0 if is_open is False, and the gripper will not open after placing. Set the `dis` to a small value like 0.02 if you want the gripper to open after placing.\n        is_open=False, # if is_open is False, pre_dis and dis will be 0, and the gripper will not open after placing.\n        constrain=\"free\", # if task requires the object to be placed in a specific pose that mentioned in the task description (like \"the head of the actor should be toward xxx), you can set constrain to \"align\", in all of other cases, you should set constrain to \"free\".\n        pre_dis_axis='fp', # Use functional point direction for pre-displacement, if the functional_point_id is used\n    )\n)\n</code></pre></p>"},{"location":"usage/API.html#24-move_by_displacementself-arm_tag-armtag-x0-y0-z0-quatnone-move_axisworld-tuplearmtag-listaction","title":"2.4 <code>move_by_displacement(self, arm_tag: ArmTag, x=0., y=0., z=0., quat=None, move_axis='world') -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#241-description","title":"2.4.1 Description","text":"<p>Moves the end-effector of the specified arm along relative directions and sets its orientation.</p>"},{"location":"usage/API.html#242-parameters","title":"2.4.2 Parameters","text":"<ul> <li><code>arm_tag</code>: The arm to control</li> <li><code>x</code>, <code>y</code>, <code>z</code>: Displacement along each axis (in meters)</li> <li><code>quat</code>: Optional quaternion specifying the target orientation; if not set, uses current orientation</li> <li><code>move_axis</code>: <code>'world'</code> means displacement is in world coordinates, <code>'arm'</code> means displacement is in local coordinates</li> </ul>"},{"location":"usage/API.html#243-returns","title":"2.4.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the move-by-displacement actions.</p>"},{"location":"usage/API.html#244-example","title":"2.4.4 Example","text":"<p>Lift the object up by moving relative to current position, you should lift the arm up evrery time after grasping an object to avoid collision. <pre><code>self.move(\n    self.move_by_displacement(\n        arm_tag=arm_tag,\n        z=0.07,  # Move 7cm upward\n        move_axis='world'\n    )\n)\n</code></pre></p>"},{"location":"usage/API.html#25-move_to_poseself-arm_tag-armtag-target_pose-list-tuplearmtag-listaction","title":"2.5 <code>move_to_pose(self, arm_tag: ArmTag, target_pose: list) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#251-description","title":"2.5.1 Description","text":"<p>Moves the end-effector of the specified arm to a specific absolute pose.</p>"},{"location":"usage/API.html#252-parameters","title":"2.5.2 Parameters","text":"<ul> <li><code>arm_tag</code>: The arm to control</li> <li><code>target_pose</code>: Absolute position and/or orientation, length 3 or 7 (xyz + optional quaternion)</li> </ul>"},{"location":"usage/API.html#253-returns","title":"2.5.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the move-to-pose actions.</p>"},{"location":"usage/API.html#254-example","title":"2.5.4 Example","text":"<p>Move the arm to a specific pose, for example, to place an object in a certain position decided by which arm is placing the object. <pre><code>target_pose = self.get_arm_pose(arm_tag=arm_tag)\nif arm_tag == 'left':\n    # Set specific position and orientation for left arm\n    target_pose[:2] = [-0.1, -0.05]\n    target_pose[2] -= 0.05\n    target_pose[3:] = [-0.707, 0, -0.707, 0]\nelse:\n    # Set specific position and orientation for right arm\n    target_pose[:2] = [0.1, -0.05]\n    target_pose[2] -= 0.05\n    target_pose[3:] = [0, 0.707, 0, -0.707]\n\n# Move the skillet to the defined target pose\nself.move(\n    self.move_to_pose(arm_tag=arm_tag, target_pose=target_pose)\n)\n</code></pre></p>"},{"location":"usage/API.html#26-close_gripperself-arm_tag-armtag-pos0-tuplearmtag-listaction","title":"2.6 <code>close_gripper(self, arm_tag: ArmTag, pos=0.) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#261-description","title":"2.6.1 Description","text":"<p>Closes the gripper of the specified arm.</p>"},{"location":"usage/API.html#262-parameters","title":"2.6.2 Parameters","text":"<ul> <li><code>arm_tag</code>: Which arm's gripper to close</li> <li><code>pos</code>: Gripper position (0 = fully closed)</li> </ul>"},{"location":"usage/API.html#263-returns","title":"2.6.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the gripper-close action.</p>"},{"location":"usage/API.html#264-example","title":"2.6.4 Example","text":"<pre><code>self.move(\n    self.close_gripper(arm_tag=arm_tag)\n)\n</code></pre>"},{"location":"usage/API.html#27-open_gripperself-arm_tag-armtag-pos1-tuplearmtag-listaction","title":"2.7 <code>open_gripper(self, arm_tag: ArmTag, pos=1.) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#271-description","title":"2.7.1 Description","text":"<p>Opens the gripper of the specified arm.</p>"},{"location":"usage/API.html#272-parameters","title":"2.7.2 Parameters","text":"<ul> <li><code>arm_tag</code>: Which arm's gripper to open</li> <li><code>pos</code>: Gripper position (1 = fully open)</li> </ul>"},{"location":"usage/API.html#273-returns","title":"2.7.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the gripper-open action.</p>"},{"location":"usage/API.html#274-example","title":"2.7.4 Example","text":"<pre><code>self.move(\n    self.open_gripper(arm_tag=arm_tag)\n)\n</code></pre>"},{"location":"usage/API.html#28-back_to_originself-arm_tag-armtag-tuplearmtag-listaction","title":"2.8 <code>back_to_origin(self, arm_tag: ArmTag) -&gt; tuple[ArmTag, list[Action]]</code>","text":""},{"location":"usage/API.html#281-description","title":"2.8.1 Description","text":"<p>Returns the specified arm to its predefined initial position.</p>"},{"location":"usage/API.html#282-parameters","title":"2.8.2 Parameters","text":"<ul> <li><code>arm_tag</code>: The arm to return to origin</li> </ul>"},{"location":"usage/API.html#283-returns","title":"2.8.3 Returns","text":"<p><code>(arm_tag, action_list)</code> containing the return-to-origin action.</p>"},{"location":"usage/API.html#284-example","title":"2.8.4 Example","text":"<p>Place left object while moving right arm back to origin. <pre><code>move_arm_tag = ArmTag(\"left\")  # Specify which arm is placing the object\nback_arm_tag = ArmTag(\"right\")  # Specify which arm is moving back to origin\nself.move(\n    self.place_actor(\n        actor=self.left_actor,\n        arm_tag=move_arm_tag,\n        target_pose=target_pose,\n        pre_dis_axis=\"fp\",\n    ),\n    self.back_to_origin(arm_tag=back_arm_tag)\n)\n</code></pre></p>"},{"location":"usage/API.html#29-get_arm_poseself-arm_tag-armtag-listfloat","title":"2.9 <code>get_arm_pose(self, arm_tag: ArmTag) -&gt; list[float]</code>","text":""},{"location":"usage/API.html#291-description","title":"2.9.1 Description","text":"<p>Gets the current pose of the end-effector of the specified arm.</p>"},{"location":"usage/API.html#292-parameters","title":"2.9.2 Parameters","text":"<ul> <li><code>arm_tag</code>: Which arm to query</li> </ul>"},{"location":"usage/API.html#293-returns","title":"2.9.3 Returns","text":"<p>A list of 7 floats: <code>[x, y, z, qw, qx, qy, qz]</code>, representing position and orientation.</p>"},{"location":"usage/API.html#294-example","title":"2.9.4 Example","text":"<pre><code>pose = self.get_arm_pose(ArmTag(\"left\"))\n</code></pre>"},{"location":"usage/API.html#3-actor-class-apis","title":"3. <code>Actor</code> Class APIs","text":"<p><code>Actor</code> is the object being manipulated by the robotic arms. It provides methods to retrieve key points and its current global pose. The <code>Actor</code> class has the following data points:</p> <ul> <li>Target Point <code>target_point</code>: Special points available during planning (e.g., handle of a cup)</li> <li>Contact Point <code>contact_point</code>: Position where the robotic arm grasps the object (e.g., rim of a cup)</li> <li>Functional Point <code>functional_point</code>: Position where the object interacts with other objects (e.g., head of a hammer)</li> <li>Orientation Point <code>orientation_point</code>: Specifies the orientation of the object (e.g., toe of a shoe pointing left)</li> </ul> <p>These methods can be called on <code>Actor</code> objects:</p>"},{"location":"usage/API.html#31-get_contact_pointself-idx-int-listfloat","title":"3.1 <code>get_contact_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th contact point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#32-get_functional_pointself-idx-int-listfloat","title":"3.2 <code>get_functional_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th functional point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#33-get_target_pointself-idx-int-listfloat","title":"3.3 <code>get_target_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th target point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#34-get_orientation_pointself-idx-int-listfloat","title":"3.4 <code>get_orientation_point(self, idx: int) -&gt; list[float]</code>","text":"<p>Returns the pose of the <code>idx</code>-th orientation point as <code>[x, y, z, qw, qx, qy, qz]</code></p>"},{"location":"usage/API.html#35-get_poseself-sapienpose","title":"3.5 <code>get_pose(self) -&gt; sapien.Pose</code>","text":"<p>Returns the global pose of the object in SAPIEN (<code>.p</code> is position, <code>.q</code> is orientation)</p>"},{"location":"usage/API.html#4-articulationactor-class-apis","title":"4. <code>ArticulationActor</code> Class APIs","text":"<p>If the actor was created with method that contains \"urdf\"(e.g. <code>create_rand_sapien_urdf_actor</code>), it will be a subclass of <code>Actor</code> called <code>ArticulationActor</code>, with the following additional methods:</p>"},{"location":"usage/API.html#41-get_qlimitsself-listtuplefloat-float","title":"4.1 <code>get_qlimits(self) -&gt; list[tuple[float, float]]</code>","text":"<p>Returns a list of joint limits, where each joint limit is a tuple <code>(min, max)</code>.</p>"},{"location":"usage/API.html#42-get_qposself-listfloat","title":"4.2 <code>get_qpos(self) -&gt; list[float]</code>","text":"<p>Returns the current positions (rotational/positional) of all joints.</p>"},{"location":"usage/API.html#43-get_qvelself-listfloat","title":"4.3 <code>get_qvel(self) -&gt; list[float]</code>","text":"<p>Returns the current velocities of all joints.</p>"},{"location":"usage/DP.html","title":"DP (Diffusion Policy)","text":""},{"location":"usage/DP.html#1-install","title":"1. Install","text":"<pre><code>cd policy/DP\npip install zarr==2.12.0 wandb ipdb gpustat dm_control omegaconf hydra-core==1.2.0 dill==0.3.5.1 einops==0.4.1 diffusers==0.11.1 numba==0.56.4 moviepy imageio av matplotlib termcolor sympy\npip install -e .\n</code></pre>"},{"location":"usage/DP.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the Zarr format required for DP training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data.</p> <pre><code>bash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data.sh beat_block_hammer demo_randomized 50\n</code></pre>"},{"location":"usage/DP.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. By default, the model is trained for 600 steps. The <code>action_dim</code> parameter defines the dimensionality of the robot\u2019s action space \u2014 for example, it is 14 for the <code>aloha-agilex</code> embodiment.</p> <pre><code>bash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${action_dim} ${gpu_id}\n# bash train.sh beat_block_hammer demo_randomized 50 0 14 0\n# For `aloha-agilex` embodiment, the action_dim is 14\n</code></pre>"},{"location":"usage/DP.html#4-eval-policy","title":"4. Eval Policy","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>ckpt_setting</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized demo_randomized 50 0 0\n# This command trains the policy using the `demo_randomized` setting ($ckpt_setting)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean demo_randomized 50 0 0\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/DP3.html","title":"DP3 (3D Diffusion Policy)","text":"<p>Since DP3 is a 3D policy that requires point cloud input, please make sure to set <code>data_type/pointcloud</code> to <code>true</code> during data collection.</p>"},{"location":"usage/DP3.html#1-install","title":"1. Install","text":"<pre><code>cd policy/DP3/3D-Diffusion-Policy &amp;&amp; pip install -e . &amp;&amp; cd ..\npip install zarr==2.12.0 wandb ipdb gpustat dm_control omegaconf hydra-core==1.2.0 dill==0.3.5.1 einops==0.4.1 diffusers==0.11.1 numba==0.56.4 moviepy imageio av matplotlib termcolor\n</code></pre>"},{"location":"usage/DP3.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>If you meet <code>ZeroDivisionError: division by zero</code>:  Since DP3 is a 3D policy that requires point cloud input, please make sure to set <code>data_type/pointcloud</code> to <code>true</code> during data collection.</p> <p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the Zarr format required for DP3 training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data.</p> <pre><code>bash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data.sh beat_block_hammer demo_randomized 50\n</code></pre>"},{"location":"usage/DP3.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. By default, the model is trained for 3,000 steps.</p> <pre><code>bash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${gpu_id}\n# bash train.sh beat_block_hammer demo_randomized 50 0 0\n</code></pre>"},{"location":"usage/DP3.html#4-eval-policy","title":"4. Eval Policy","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>ckpt_setting</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized demo_randomized 50 0 0\n# This command trains the policy using the `demo_randomized` setting ($ckpt_setting)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean demo_randomized 50 0 0\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/DexVLA.html","title":"DexVLA (Vision-Language Model with Plug-In Diffusion Expert for Visuomotor Policy Learning)","text":"<p>Contributed by Midea Group</p>"},{"location":"usage/DexVLA.html#1-install","title":"1. Install","text":"<p>To guarantee clean isolation between training and evaluation environments for both DexVLA and TinyVLA, we provide two distinct, self-contained setups.The training and testing environment can be used for both DexVLA and TinyVLA.</p> <p>Training Environment\uff1a <pre><code>cd policy/DexVLA\nconda env create -f Train_Tiny_DexVLA_train.yml\nconda activate dexvla-robo\ncd policy_heads\npip install -e .\n</code></pre> Evaluation Environment:</p> <p>If you already have RoboTwin 2.0 installed, activate this conda environment and add the evaluation dependencies: <pre><code>conda activate your_RoboTwin_env\npip install -r Eval_Tiny_DexVLA_requirements.txt \n</code></pre></p>"},{"location":"usage/DexVLA.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the format required for DexVLA training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data. <pre><code>python process_data.py ${task_name} ${task_config} ${expert_data_num}\n# python process_data.py beat_block_hammer demo_randomized 50\n</code></pre> If success, you will find the data in the <code>policy/Dexvla/data/sim_${task_name}/${setting}_${expert_data_num}</code> folder.</p>"},{"location":"usage/DexVLA.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process.</p>"},{"location":"usage/DexVLA.html#31-download-official-qwen2_vl-weights","title":"3.1 Download official Qwen2_VL weights","text":"<p>We construct the VLM backbone by integrating Qwen2-VL-2B.You can download the official weights from this link:</p> Model Link Qwen2-VL (~2B) huggingface <p>\u2757\u2757 After downloading the standard weights, you have to modify the official <code>config.json</code> file in the folder. Please update the 'architectures' field from \"Qwen2VLForConditionalGenerationForVLA\" to \"DexVLA\", and change the 'model_type' field from \"qwen2_vla\" to \"dex_vla\".</p>"},{"location":"usage/DexVLA.html#32-download-our-pretrained-scaledp-h-weights","title":"3.2 Download our pretrained ScaleDP-H weights","text":"<p>We released our pretrained weights of ScaleDP-H which is trained after Stage1. Now you can download the weights and directly finetuning your data on Stage 2.</p> Model Link ScaleDP-H (~1B) huggingface ScaleDP-L (~400M) huggingface ### 3.3 Train The training script are \"scripts/aloha/vla_stage2_train.sh\". And you need to change following parameters: 1. OUTPUT : refers to the save directory for training, which must include the keyword \"qwen2\" (and optionally \"lora\"). If LoRA training is used, the name must include \"lora\" (e.g., \"qwen2_lora\"). 2. TASKNAME : refers to the tasks used for training, which should be corresponded to \"your_task_name\" in aloha_scripts/constant.py 3. mnop : path to the pretrained VLM weights 4. load_pretrain_dit : True 5. DIT_PRETRAIN :Path to pretrained policy head (ScaleDP). <p>Other hyperparameters like \"batch_size\", \"save_steps\" could be customized according to your computation resources.</p> <p>Start training by following commands: <pre><code>bash ./scripts/aloha/vla_stage2_train.sh\n</code></pre></p>"},{"location":"usage/DexVLA.html#4-eval-policy","title":"4. Eval Policy","text":"<p>You need to modify the corresponding path in the <code>deploy_policy.yml</code> file:  1. model_path : Path to the trained model, in the OUTPUT path. 2. state_path : Path to <code>dataset_stats.pkl</code>, in the OUTPUT path.</p> <p>Then execute: <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized 0 50 0 0\n</code></pre></p>"},{"location":"usage/DexVLA.html#5-citation","title":"5. Citation","text":"<p>If you find our works useful for your research and applications, please cite using these BibTeX:</p>"},{"location":"usage/DexVLA.html#51-dexvla","title":"5.1 DexVLA","text":"<pre><code>@article{wen2025dexvla,\n  title={DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control},\n  author={Wen, Junjie and Zhu, Yichen and Li, Jinming and Tang, Zhibin and Shen, Chaomin and Feng, Feifei},\n  journal={arXiv preprint arXiv:2502.05855},\n  year={2025}\n}\n</code></pre>"},{"location":"usage/DexVLA.html#52-diffusionvla","title":"5.2 DiffusionVLA","text":"<pre><code>@article{wen2024diffusion,\n  title={Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression},\n  author={Wen, Junjie and Zhu, Minjie and Zhu, Yichen and Tang, Zhibin and Li, Jinming and Zhou, Zhongyi and Li, Chengmeng and Liu, Xiaoyu and Peng, Yaxin and Shen, Chaomin and others},\n  journal={arXiv preprint arXiv:2412.03293},\n  year={2024}\n}\n</code></pre>"},{"location":"usage/DexVLA.html#53-scaledp","title":"5.3 ScaleDP","text":"<p>```bibtex @article{zhu2024scaling,   title={Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation},   author={Zhu, Minjie and Zhu, Yichen and Li, Jinming and Wen, Junjie and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and others},   journal={arXiv preprint arXiv:2409.14411},   year={2024} }</p>"},{"location":"usage/LLaVA-VLA.html","title":"LLaVA-VLA","text":"<p>Contributed by IRPN Lab, HKUST(GZ) Email: songwenxuan0115@gmail.com, sunxiaoquan@hust.edu.cn</p>"},{"location":"usage/LLaVA-VLA.html#1-environment-setup","title":"1. Environment Setup","text":"<p>See LLaVA-VLA installation for more details.</p>"},{"location":"usage/LLaVA-VLA.html#2-download-model","title":"2. Download Model","text":"<p>Please download the corresponding model from the model zoo.</p>"},{"location":"usage/LLaVA-VLA.html#3-collect-robotwin-data","title":"3. Collect RoboTwin Data","text":"<p>See RoboTwin Tutorial (Usage Section) for more details.</p>"},{"location":"usage/LLaVA-VLA.html#4-generate-image-and-data","title":"4. Generate Image and Data","text":"<p>First, create the pictures folder in the policy/LLaVA-VLA directory: <pre><code>mkdir pictures &amp;&amp; training_data\ncd scripts &amp;&amp; cd helper\n</code></pre> Then, extract the original image from RoboTwin data. <pre><code>bash image_extraction.sh ${task_name} ${task_config}\n# bash image_extraction.sh grab_roller demo_randomized\n# bash image_extraction.sh all demo_randomized\n# In task_name, you can directly select a task(such as: grab_roller) or choose \"all\" (just modify it in task_list).\n</code></pre> Next, generate the format data required for LLaVA-VLA training. <pre><code>bash process_data.sh ${task_name} ${task_config} ${future_chunk}\n# bash process_data.sh grab_roller demo_randomized 5\n# bash process_data.sh all demo_randomized 5\n# In task_name, you can directly select a task(such as: grab_roller) or choose \"all\" (just modify it in task_list). \n# future_chunk: The number of output steps in the future (default is 5).\n</code></pre> Example folder structure: <pre><code>training_data\n\u251c\u2500\u2500 ${task_1}\n\u2502   \u251c\u2500\u2500 ${task_config_1}\n|   |   |\u2500\u2500 episode0.json\n|   |   |\u2500\u2500 episode1.json\n\u2502   \u251c\u2500\u2500 ${task_config_2}\n|   |   |\u2500\u2500 episode0.json\n|   |   |\u2500\u2500 episode1.json\n\u251c\u2500\u2500 ${task_2}\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ...\n</code></pre> <pre><code>pictures\n\u251c\u2500\u2500 ${task_1}\n\u2502   \u251c\u2500\u2500 ${task_config_1}\n|   |   |\u2500\u2500 episode0\n|   |   |   |\u2500\u2500 01.jpg\n|   |   |   |\u2500\u2500 02.jpg\n\u2502   \u251c\u2500\u2500 ${task_config_2}\n|   |   |\u2500\u2500 episode0\n|   |   |   |\u2500\u2500 01.jpg\n|   |   |   |\u2500\u2500 ...\n\u251c\u2500\u2500 ${task_2}\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ...\n</code></pre></p>"},{"location":"usage/LLaVA-VLA.html#5-merge-json-and-generate-yaml-file","title":"5. merge json and Generate yaml file","text":"<p>In this step, we need to merge all the JSON files generated by the previous <code>process_data</code> step into a single JSON file. <pre><code>python llava/process_data/merge_json.py\n# please replace `yourpath` with your actual path!\n</code></pre> <pre><code>python llava/process_data/yaml_general.py\n</code></pre></p>"},{"location":"usage/LLaVA-VLA.html#6-pre-training","title":"6. Pre-Training","text":"<p>Before starting the training, please replace <code>yourpath</code> with your actual path! <pre><code>bash calvin_finetune_obs.sh\n</code></pre></p>"},{"location":"usage/LLaVA-VLA.html#7-fine-tuning","title":"7. Fine-tuning","text":"<p>Please note to change <code>MODEL_NAME_OR_PATH</code> to the checkpoint generated in the previous step. For the dataset you fine-tuned, please regenerate the <code>ACTION_STAT</code> file and modify <code>JSON_PATH</code>.Then <pre><code>bash calvin_finetune_obs.sh\n</code></pre></p>"},{"location":"usage/LLaVA-VLA.html#8-eval-on-robotwin","title":"8. Eval on RoboTwin","text":"<p>You need to modify the corresponding path in the deploy_policy.yml file:  1. <code>model_path</code> : Path to the checkpoint.  2. <code>action_stat</code> : Path to dataset_statistic.yaml. <pre><code>bash eval.sh ${gpu_id}\n# bash eval.sh 0\n</code></pre> The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/LLaVA-VLA.html#9-citation","title":"9. Citation","text":"<p>If you find our works useful for your research and applications, please cite using these BibTeX: <pre><code>@article{pdvla,\n  title={Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding},\n  author={Song, Wenxuan and Chen, Jiayi and Ding, Pengxiang and Zhao, Han and Zhao, Wei and Zhong, Zhide and Ge, Zongyuan and Ma, Jun and Li, Haoang},\n  journal={arXiv preprint arXiv:2503.02310},\n  year={2025}\n}\n</code></pre></p>"},{"location":"usage/Pi0.html","title":"OpenPI","text":""},{"location":"usage/Pi0.html#1-environment-setup","title":"1. Environment Setup","text":"<p>We use uv to manage Python dependencies,you can add uv your conda environment.</p> <p><pre><code>conda activate RoboTwin\n# Install uv\npip install uv\n</code></pre> Once uv is installed, run the following commands to set up the environment:</p> <pre><code>cd policy/pi0\n# Install prequisites in uv environment\nGIT_LFS_SKIP_SMUDGE=1 uv sync\n</code></pre> <p>If you want to eval pi0 policy in RoboTwin\uff0cyou are required to install curobo in your uv environment\uff1a</p> <pre><code>conda deactivate\nsource .venv/bin/activate\n# At this point, you should be in the (openpi) environment\ncd ../../envs\ngit clone https://github.com/NVlabs/curobo.git\ncd curobo\npip install -e . --no-build-isolation\ncd ../../policy/pi0/\nbash\n</code></pre>"},{"location":"usage/Pi0.html#2-generate-robotwin-data","title":"2. Generate RoboTwin Data","text":"<p>See RoboTwin Tutorial (Usage Section) for more details.</p>"},{"location":"usage/Pi0.html#3-generate-openpi-data","title":"3. Generate openpi Data","text":"<p>First, create the <code>processed_data</code> and <code>training_data</code> folders in the <code>policy/pi0</code> directory:</p> <pre><code>mkdir processed_data &amp;&amp; mkdir training_data\n</code></pre> <p>Then, convert RoboTwin data to HDF5 data type.</p> <pre><code>bash process_data_pi0.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data_pi0.sh beat_block_hammer demo_randomized 50\n</code></pre> <p>If success, you will find the <code>${task_name}-${task_config}-${expert_data_num}</code> folder under <code>policy/pi0/processed_data</code>.</p> <p>Example folder structure:</p> <pre><code>processed_data/ \n\u251c\u2500\u2500${task_name}-${task_config}-${expert_data_num}\n|       |   \u251c\u2500\u2500episode_0\n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_0.hdf5  \n|       |   \u251c\u2500\u2500 episode_1 \n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_1.hdf5  \n|       |   \u251c\u2500\u2500 ...\n</code></pre> <p>Copy all the data you wish to use for training from <code>processed_data</code> into <code>training_data/${model_name}</code>. If you have multiple tasks with different data, simply copy them in the same way.please Place the corresponding task folders according to the example below.</p> <pre><code>#multi-task dataset example\ntraining_data/  \n\u251c\u2500\u2500 ${model_name}\n|       \u251c\u2500\u2500${task_0}\n|       |   \u251c\u2500\u2500episode_0\n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_0.hdf5  \n|       |   \u251c\u2500\u2500 episode_1 \n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_1.hdf5  \n|       |   \u251c\u2500\u2500 ...\n|       \u251c\u2500\u2500 ${task_1}\n|       |   \u251c\u2500\u2500episode_0\n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_0.hdf5  \n|       |   \u251c\u2500\u2500 episode_1 \n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_1.hdf5  \n|       |   \u251c\u2500\u2500 ...\n\n#sigle task example\ntraining_data/  \n\u251c\u2500\u2500 demo_randomized\n|       \u251c\u2500\u2500beat_block_hammer-demo_randomized-50\n|       |   \u251c\u2500\u2500episode_0\n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_0.hdf5  \n|       |   \u251c\u2500\u2500 episode_1 \n|       |   |   \u251c\u2500\u2500 instructions.json  \n|       |   |   \u251c\u2500\u2500 episode_1.hdf5  \n|       |   \u251c\u2500\u2500 ...\n</code></pre> <p>Before generating the LerobotDataset format data for pi0,please make sure you have enough disk space under the <code>~/.cache</code>.This is because generating the <code>lerobotdataset</code> will require a large amount of space.And the datasets will be writed into <code>$XDG_CACHE_HOME</code>,which default path is  <code>~/.cache</code>.If you don't have enough disk space under the <code>~/.cache</code> path, please use the following command to set a different cache directory with sufficient space:</p> <pre><code>export XDG_CACHE_HOME=/path/to/your/cache\n</code></pre> <p>Now, we can directly generate the LerobotDataset format data for pi0</p> <pre><code># hdf5_path: The path to the generated HDF5 data (e.g., ./training_data/${model_name}/)\n# repo_id: The name of the dataset (e.g., my_repo)\nbash generate.sh ${hdf5_path} ${repo_id}\n#bash generate.sh ./training_data/demo_randomized/ demo_randomized_repo\n</code></pre> <p>LerobotDataset format data will be writed into <code>${XDG_CACHE_HOME}/huggingface/lerobot/${repo_id}</code></p>"},{"location":"usage/Pi0.html#4-write-the-corresponding-train_config","title":"4. Write the Corresponding <code>train_config</code>","text":"<p>In <code>src/openpi/training/config.py</code>, there is a dictionary called <code>_CONFIGS</code>. You can modify 4 pre-configured PI0 configurations I\u2019ve written: <code>pi0_base_aloha_robotwin_lora</code> <code>pi0_fast_aloha_robotwin_lora</code> <code>pi0_base_aloha_robotwin_full</code> <code>pi0_fast_aloha_robotwin_full</code></p> <p>You only need to write <code>repo_id</code>  on your datasets.(e.g., <code>repo_id=demo_randomized_repo</code>) If you want to change the <code>name</code> in <code>TrainConfig</code>, please include <code>fast</code> if you choose <code>pi_fast_base</code> model. If your do not have enough gpu memory, you can set <code>fsdp_devices</code>, refer to <code>config.py</code> line <code>src/openpi/training/config.py</code> line 352.</p>"},{"location":"usage/Pi0.html#5-5-finetune-model","title":"5. 5. Finetune model","text":"<pre><code># compute norm_stat for dataset\nuv run scripts/compute_norm_stats.py --config-name ${train_config_name}\n# uv run scripts/compute_norm_stats.py --config-name pi0_base_aloha_robotwin_full\n\n# train_config_name: The name corresponding to the config in _CONFIGS, such as pi0_base_aloha_robotwin_full\n# model_name: You can choose any name for your model\n# gpu_use: if not using multi gpu,set to gpu_id like 0;else set like 0,1,2,3\nbash finetune.sh ${train_config_name} ${model_name} ${gpu_use}\n#bash finetune.sh pi0_base_aloha_robotwin_full demo_randomized 0,1,2,3\n</code></pre> Training mode Memory Required Example GPU Fine-Tuning (LoRA) &gt; 46 GB A6000(48G) Fine-Tuning (Full) &gt; 100 GB 2*A100 (80GB) / 2*H100 <p>If your GPU memory is insufficient, please set the <code>fsdp_devices</code> parameter according to the following GPU memory reference, or reduce the <code>batch_size</code> parameter. Or you can try setting <code>XLA_PYTHON_CLIENT_PREALLOCATE=false</code> in <code>finetune.sh</code>, it will cost lower gpu memory, but make training speed slower.</p> <p>The default <code>batch_size</code> is 32 in the table below.</p> GPU memory Model type GPU num fsdp_devices Example GPU 24G lora 2 2 4090(24G) 40G lora 2 2 A100(40G) 48G lora 1 1 A6000(48G) 40G full 4 4 A100(40G) 80G full 2 2 A100(80G)"},{"location":"usage/Pi0.html#6-eval-on-robotwin","title":"6. Eval on RoboTwin","text":"<p>Checkpoints will be saved in policy/pi0/checkpoints/\\({train_config_name}/\\)}/${checkpoint_id</p> <p>You can modify the <code>deploy_policy.yml</code> file to change the <code>checkpoint_id</code> you want to evaluate.</p> <pre><code># ckpt_path like: policy/pi0/checkpoints/pi0_base_aloha_robotwin_full/demo_randomized/30000\nbash eval.sh ${task_name} ${task_config} ${train_config_name} ${model_name} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized pi0_base_aloha_robotwin_full demo_randomized 0 0\n# This command trains the policy using the `demo_randomized` setting ($model_name)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean pi0_base_aloha_robotwin_full demo_randomized 0 0\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/RDT.html","title":"RDT","text":""},{"location":"usage/RDT.html#1-environment-setup","title":"1. Environment Setup","text":"<p>The conda environment for RDT with RoboTwin is identical to the official RDT environment. Please follow the (RDT official documentation) to install the environment and directly overwrite the RoboTwin virtual environment in INSTALLATION.md.</p> <pre><code># Make sure python version == 3.10\nconda activate RoboTwin\n\n# Install pytorch\n# Look up https://pytorch.org/get-started/previous-versions/ with your cuda version for a correct command\npip install torch==2.1.0 torchvision==0.16.0  --index-url https://download.pytorch.org/whl/cu121\n\n# Install packaging\npip install packaging==24.0\npip install ninja\n# Verify Ninja --&gt; should return exit code \"0\"\nninja --version; echo $?\n# Install flash-attn\npip install flash-attn==2.7.2.post1 --no-build-isolation\n\n# Install other prequisites\npip install -r requirements.txt\n# If you are using a PyPI mirror, you may encounter issues when downloading tfds-nightly and tensorflow. \n# Please use the official source to download these packages.\n# pip install tfds-nightly==4.9.4.dev202402070044 -i  https://pypi.org/simple\n# pip install tensorflow==2.15.0.post1 -i  https://pypi.org/simple\n</code></pre>"},{"location":"usage/RDT.html#2-download-model","title":"2. Download Model","text":"<pre><code># In the ROOT directory\ncd policy \nmkdir weights\ncd weights\nmkdir RDT &amp;&amp; cd RDT\n# Download the models used by RDT\nhuggingface-cli download google/t5-v1_1-xxl --local-dir t5-v1_1-xxl\nhuggingface-cli download google/siglip-so400m-patch14-384 --local-dir siglip-so400m-patch14-384\nhuggingface-cli download robotics-diffusion-transformer/rdt-1b --local-dir rdt-1b\n</code></pre>"},{"location":"usage/RDT.html#3-collect-robotwin-data","title":"3. Collect RoboTwin Data","text":"<p>See RoboTwin Tutorial (Usage Section) for more details.</p>"},{"location":"usage/RDT.html#4-generate-hdf5-data","title":"4. Generate HDF5 Data","text":"<p>HDF5 is the data format required for RDT training.</p> <p>First, create the <code>processed_data</code> and <code>training_data</code> folders in the <code>policy/RDT</code> directory: <pre><code>mkdir processed_data &amp;&amp; mkdir training_data\n</code></pre></p> <p>Then, run the following in the <code>RDT/</code> root directory:</p> <pre><code>bash process_data_rdt.sh ${task_name} ${task_config} ${expert_data_num} ${gpu_id}\n</code></pre> <p>If success, you will find the <code>${task_name}-${task_config}-${expert_data_num}</code> folder under <code>policy/RDT/processed_data</code>.</p>"},{"location":"usage/RDT.html#5-generate-configuration-file","title":"5. Generate Configuration File","text":"<p>A <code>$model_name</code> manages the training of a model, including the training data and training configuration. <pre><code>cd policy/RDT\nbash generate.sh ${model_name}\n# bash generate.sh RDT_demo_randomized\n</code></pre></p> <p>This will create a folder named <code>\\${model_name}</code> under training_data and a configuration file <code>\\${model_name}.yml</code> under model_config.</p>"},{"location":"usage/RDT.html#51-prepare-data","title":"5.1 Prepare Data","text":"<p>Copy all the data you wish to use for training from <code>processed_data</code> into <code>training_data/${model_name}</code>. If you have multiple tasks with different data, simply copy them in the same way.</p> <p>Example folder structure: <pre><code>training_data/${model_name}\n\u251c\u2500\u2500 ${task_1}\n\u2502   \u251c\u2500\u2500 episode_0\n|   |   |\u2500\u2500 episode_0.hdf5\n|   |   |-- instructions\n|   \u2502   \u2502   \u251c\u2500\u2500 lang_embed_0.pt\n|   \u2502   \u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ${task_2}\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ...\n</code></pre></p>"},{"location":"usage/RDT.html#52-modify-training-config","title":"5.2 Modify Training Config","text":"<p>In <code>model_config/${model_name}.yml</code>, you need to manually set the GPU to be used (modify <code>cuda_visible_device</code>). For a single GPU, try format like <code>0</code> to set GPU 0. For multi-GPU usage, try format like <code>0,1,4</code>. You can flexibly modify other parameters.</p>"},{"location":"usage/RDT.html#6-finetune-model","title":"6. Finetune model","text":"<p>Once the training parameters are set, you can start training with: <pre><code>bash finetune.sh ${model_name}\n# bash finetune.sh RDT_demo_randomized\n</code></pre> Note!</p> <p>If you fine-tune the model using a single GPU, DeepSpeed will not save <code>pytorch_model/mp_rank_00_model_states.pt</code>. If you wish to continue training based on the results of a single-GPU trained model, please set <code>pretrained_model_name_or_path</code> to something like <code>./checkpoints/${model_name}/checkpoint-${ckpt_id}</code>. </p> <p>This will use the pretrain pipeline to import the model, which is the same import structure as the default <code>../weights/RDT/rdt-1b</code>.</p>"},{"location":"usage/RDT.html#7-eval-on-robotwin","title":"7. Eval on RoboTwin","text":"<p>The <code>task_config</code> field refers to the evaluation environment configuration, while the <code>model_name</code> field refers to the training data configuration used during policy learning.</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${model_name} ${checkpoint_id} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized RDT_demo_randomized 10000 0 0\n# This command trains the policy using the `RDT_demo_randomized` setting ($model_name)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean RDT_demo_randomized 10000 0 0\n</code></pre> <p>The evaluation results, including videos, will be saved in the <code>eval_result</code> directory under the project root.</p>"},{"location":"usage/TinyVLA.html","title":"Tiny-VLA (Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation)","text":"<p>Contributed by Midea Group</p>"},{"location":"usage/TinyVLA.html#1-install","title":"1. Install","text":"<p>To guarantee clean isolation between training and evaluation environments for both DexVLA and TinyVLA, we provide two distinct, self-contained setups.The training and testing environment can be used for both DexVLA and TinyVLA.</p> <p>Training Environment\uff1a <pre><code>cd policy/TinyVLA\nconda env create -f Train_Tiny_DexVLA_train.yml\nconda activate dexvla-robo\ncd policy_heads\npip install -e .\n</code></pre> Evaluation Environment:</p> <p>If you already have RoboTwin 2.0 installed, activate its conda environment and add the evaluation dependencies: <pre><code>conda activate your_RoboTwin_env\npip install -r Eval_Tiny_DexVLA_requirements.txt \n</code></pre></p>"},{"location":"usage/TinyVLA.html#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>This step performs data preprocessing, converting the original RoboTwin 2.0 data into the format required for TinyVLA training. The <code>expert_data_num</code> parameter specifies the number of trajectory pairs to be used as training data. <pre><code>python process_data.py ${task_name} ${task_config} ${expert_data_num}\n# python process_data.py beat_block_hammer demo_randomized 50\n</code></pre> If success, you will find the <code>sim_${task_name}/${setting}_${expert_data_num}</code> folder under <code>policy/Tinyvla/data</code>.</p>"},{"location":"usage/TinyVLA.html#3-train-policy","title":"3. Train Policy","text":"<p>This step launches the training process. First, download the VLM model InternVL3-1B (huggingface) to the path <code>.../policy/TinyVLA/model_param/InternVL3-1B</code>. Then modify the <code>config.json</code> file in the folder as follows: <pre><code>{\n    \"_name_or_path\": \".../robotiwin/policy/TinyVLA/vla/models/internvl\", # Modify this.\n    \"architectures\": [\n        \"TinyVLA\" # Change this.\n    ],\n    # \"auto_map\":{...} # Delete this.\n    ...\n    \"llm_config\": {}, # Don't Change.\n    \"min_dynamic_patch\": 1,\n    \"model_type\": \"tinyvla\", # Change this.\n    ...\n}\n</code></pre> Then add an task config item in <code>.../policy/TinyVLA/aloha_scripts/constants.py</code> <pre><code>TASK_CONFIGS = {\n    ...\n    \"your_task\": {\n        'dataset_dir': [DATA_DIR + \"/sim-your_task/aloha-agilex-1-m1_b1_l1_h0.03_c0_D435-100\"],\n        'episode_len': 500,\n        'camera_names': ['cam_high', 'cam_left_wrist', 'cam_right_wrist'],\n        \"sample_weights\": [1, 1]\n    }\n}\n</code></pre> Then begin the training <pre><code>bash ./scripts/franks/train_robotwin_aloha.sh\n</code></pre> Configure the training by modifying the following items in the <code>train_robotwin_aloha.sh</code> file. <pre><code>TASK=your_task # Set the Task\nROOT=.../robotiwin/policy/TinyVLA # Set Root Path\nmnop=.../robotiwin/policy/TinyVLA/model_param/InternVL3-1B/ # Set The Path of base VLM\n</code></pre></p>"},{"location":"usage/TinyVLA.html#4-eval-policy","title":"4. Eval Policy","text":"<p>You need to modify the corresponding path in the <code>deploy_policy.yml</code> file: 1. model_path : Path to the trained model, in the OUTPUT path. 2. state_path : Path to <code>dataset_stats.pkl</code>, in the OUTPUT path. 3. model_base : Path to InternVL3-1B.</p> <p>Then execute:</p> <pre><code>bash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized 0 50 0 0\n</code></pre>"},{"location":"usage/TinyVLA.html#5-citation","title":"5. Citation","text":"<p>If you find Tiny-VLA useful for your research and applications, please cite using this BibTeX:</p> <pre><code>@inproceedings{wen2024tinyvla,\n    title={Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation},\n    author={Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and others},\n    booktitle={IEEE Robotics and Automation Letters (RA-L)},\n    year={2025}\n}\n</code></pre>"},{"location":"usage/collect-data.html","title":"Collect Data","text":"<p>We provide over 100,000 pre-collected trajectories as part of the open-source release RoboTwin Dataset. However, we strongly recommend users to perform data collection themselves due to the high configurability and diversity of task and embodiment setups.</p> <p>Running the following command will first search for a random seed for the target collection quantity, and then replay the seed to collect data.</p> <pre><code>bash collect_data.sh ${task_name} ${task_config} ${gpu_id}\n# Example: bash collect_data.sh beat_block_hammer demo_randomized 0\n</code></pre> <p>After data collection is completed, the collected data will be stored under <code>data/${task_name}/${task_config}</code>.</p> <p>An episode's data will be stored in one HDF5 file. Specifically, the images will be stored as bit streams. If you want to recover the image, you can use the following code:</p> <pre><code>image = cv2.imdecode(np.frombuffer(image_bit, np.uint8), cv2.IMREAD_COLOR)\n</code></pre> <ul> <li>Each trajectory's observation and action data are saved in HDF5 format in the <code>data</code> directory.</li> <li>The corresponding language instructions for each trajectory are stored in the <code>instructions</code> directory.</li> <li>Head camera videos of each trajectory can be found in the <code>video</code> directory.</li> <li>The <code>_traj_data</code>, <code>.cache</code>, <code>scene_info.json</code>, and <code>seed.txt</code> files are auxiliary outputs generated during the data collection process.</li> </ul> <p>All available <code>task_name</code> options can be found in the documentation. The <code>gpu_id</code> parameter specifies which GPU to use and should be set to an integer in the range <code>0</code> to <code>N-1</code>, where <code>N</code> is the number of GPUs available on your system.</p> <p>Our data synthesizer enables automated data collection by executing the task scripts in the <code>envs</code> directory, in combination with the <code>curobo</code> robot planner. Specifically, data collection is configured through a task-specific configuration file (see the tutorial in <code>./configurations.md</code>), which defines parameters such as the target embodiment, domain randomization settings, and the number of data samples to collect.</p> <p>The success rate of data generation for each embodiment across all tasks can be found at: https://robotwin-platform.github.io/doc/tasks/index.html. Due to the structural limitations of different robotic arms, not all embodiments are capable of completing every task.</p> <p>Our pipeline first explores a set of random seeds (<code>seed.txt</code>) to identify trajectories that can yield successful data collection. It then records fine-grained action trajectories (<code>_traj_data</code>) accordingly. Collected videos are available in the <code>videos</code> directory.</p> <p>The entire process is fully automated\u2014just run a single command to get started.</p> <p>\u26a0\ufe0f The <code>missing pytorch3d</code> warning can be ignored if 3D data is not required.</p>"},{"location":"usage/configurations.html","title":"Configuration Tutorial","text":"<p>All configuration files are stored in the <code>task_config</code> folder and follow the standard YAML format.</p> <p>You can run <code>bash task_config/create_task_config.sh ${task_config_name}</code> to create new task configuration.</p>"},{"location":"usage/configurations.html#1-minimal-example","title":"1. \u2705 Minimal Example","text":"<p>An episode's data will be stored in one HDF5 file. Specifically, the images will be stored as bit streams. If you want to recover the image, you can use the following code:</p> <pre><code>image = cv2.imdecode(np.frombuffer(image_bit, np.uint8), cv2.IMREAD_COLOR)\n</code></pre> <p>Below is a minimal configuration file to start a typical data collection session:</p> <pre><code>render_freq: 0\nepisode_num: 50\nuse_seed: false\nsave_freq: 15\nembodiment:\n- aloha-agilex\nlanguage_num: 100\ndomain_randomization:\n  random_background: true\n  cluttered_table: true\n  clean_background_rate: 0.02\n  random_head_camera_dis: 0\n  random_table_height: 0.03\n  random_light: true\n  crazy_random_light_rate: 0.02\ncamera:\n  head_camera_type: D435\n  wrist_camera_type: D435\n  collect_head_camera: true\n  collect_wrist_camera: true\ndata_type:\n  rgb: true\n  third_view: false\n  depth: false\n  pointcloud: false\n  observer: false\n  endpose: true\n  qpos: true\n  mesh_segmentation: false\n  actor_segmentation: false\npcd_down_sample_num: 1024\npcd_crop: true\nsave_path: ./data\nclear_cache_freq: 5\ncollect_data: true\neval_video_log: true\n</code></pre>"},{"location":"usage/configurations.html#2-configuration-breakdown","title":"2. \ud83d\udd27 Configuration Breakdown","text":""},{"location":"usage/configurations.html#21-task-embodiment-settings","title":"2.1 \ud83c\udfaf Task &amp; Embodiment Settings","text":"Field Type Required Description <code>embodiment</code> list \u2705 List of robot embodiment(s). For a dual-arm robot, use <code>[name]</code>, e.g., <code>[aloha-agilex]</code>; to combine two single-arm robots, use <code>[left, right, interval]</code>, e.g., <code>embodiment: [piper, franka-panda, 0.6]</code>, <code>embodiment: [franka-panda, franka-panda, 0.8]</code>. The <code>interval</code> specifies the distance between arms (typically 0.6\u20130.8 meters). Available Embodiment: <code>ur5-wsg</code>, <code>ARX-X5</code>, <code>franka-panda</code>, <code>piper</code>, <code>aloha-agilex</code>(dual-arm) <code>use_seed</code> bool \u2705 Whether to use a predefined seed list from <code>data/${task_name}/${task_config}/seed.txt</code>. If <code>false</code>, the system will automatically explore viable seeds. <code>episode_num</code> int \u2705 Number of successful episodes to collect. <code>language_num</code> int optional If using language-conditioned task planning, sets the number of language descriptions to sample for each task."},{"location":"usage/configurations.html#22-domain-randomization","title":"2.2 \ud83e\udde0 Domain Randomization","text":"<p>Configure task variation for better generalization.</p> <pre><code>domain_randomization:\n  random_background: true\n  cluttered_table: true\n  clean_background_rate: 0.02\n  random_head_camera_dis: 0\n  random_table_height: 0.03\n  random_light: true\n  crazy_random_light_rate: 0.02\n  random_embodiment: false\n</code></pre> Field Type Description <code>random_background</code> bool Enable random textures for the table and background. <code>cluttered_table</code> bool Add distractor objects to the table to simulate a cluttered environment. <code>clean_background_rate</code> float Ratio of clean backgrounds (e.g., <code>0.02</code> = 2%). Only effective if <code>random_background</code> is <code>true</code>. <code>random_head_camera_dis</code> float Random displacement applied to the head camera position (in meters). <code>random_table_height</code> float Random variation in the table height (in meters). <code>random_light</code> bool Enable randomized lighting during simulation. <code>crazy_random_light_rate</code> float Probability of applying extreme lighting. Only effective if <code>random_light</code> is <code>true</code>. <code>random_embodiment</code> bool Enable embodiment randomization (experimental, currently not fully supported)."},{"location":"usage/configurations.html#23-camera-configuration","title":"2.3 \ud83d\udcf7 Camera Configuration","text":"<pre><code>camera:\n  head_camera_type: D435\n  wrist_camera_type: D435\n  collect_head_camera: true\n  collect_wrist_camera: true\n</code></pre> Field Type Description <code>head_camera_type</code> str Camera used for global observation. Options: see <code>task_config/_camera_config.yml</code>. <code>wrist_camera_type</code> str Camera used for close-up view. <code>collect_head_camera</code> bool Whether to collect head-view data. <code>collect_wrist_camera</code> bool Whether to collect wrist-view data."},{"location":"usage/configurations.html#24-data-collection-settings","title":"2.4 \ud83d\udce6 Data Collection Settings","text":"Field Type Description <code>collect_data</code> bool Enable actual data saving. <code>save_freq</code> int Save every N steps. Per-step indicates 0.004s in the real world. <code>save_path</code> str Directory to save data. Default: <code>./data</code>. <code>clear_cache_freq</code> int Controls the frequency (in episodes) at which the Sapien scene cache is cleared. This helps manage GPU memory usage, especially when domain randomization is enabled and many diverse assets accumulate in memory. A smaller value (e.g., 1) increases clearing frequency but incurs additional time cost. <code>eval_video_log</code> bool Save evaluation videos for replay."},{"location":"usage/configurations.html#25-data-type","title":"2.5 \ud83d\udcbe Data Type","text":"<p>Specify which data to collect in each episode:</p> <pre><code>data_type:\n  rgb: true\n  third_view: false\n  depth: false\n  pointcloud: false\n  observer: false\n  endpose: false\n  qpos: true\n  mesh_segmentation: false\n  actor_segmentation: false\n</code></pre> Type Description <code>rgb</code> RGB image from multiple views. <code>third_view</code> Third-person video. <code>depth</code> Depth images from cameras (mm). <code>pointcloud</code> Merged point cloud of the scene. <code>observer</code> Observer-view RGB frame. <code>endpose</code> End-effector pose and gripper opening ratio. <code>qpos</code> Robot joint angles. <code>mesh_segmentation</code> Per-object segmentation from mesh. <code>actor_segmentation</code> Per-actor segmentation from RGB."},{"location":"usage/configurations.html#251-note","title":"2.5.1 Note","text":"<ul> <li><code>endpose</code> will get an dict containing <code>left_endpose</code>, <code>left_gripper</code>, <code>right_endpose</code> and <code>right_gripper</code>. The <code>left_endpose</code> and <code>right_endpose</code> are list of 7 elements represent the position in world and orientation of the  end-effectors, following the order <code>x, y, z, qw, qx, qy, qz</code>. And the <code>left_gripper</code> and <code>right_gripper</code> are float numbers, which repersent the opening ratio of the gripper, ranging from 0 to 1. The rotation of end-effector is as the image below: for all embodiments, the end-effector rotation is consistent, with the x-axis pointing across the gripper and the z-axis pointing across the camera.</li> </ul>"},{"location":"usage/configurations.html#_1","title":"Configurations","text":""},{"location":"usage/configurations.html#26-point-cloud-settings","title":"2.6 \ud83d\udd0d Point Cloud Settings","text":"Field Type Description <code>pcd_down_sample_num</code> int FPS (Farthest Point Sampling) number; set <code>0</code> to keep all points. <code>pcd_crop</code> bool Whether to crop out table/walls based on known transforms."},{"location":"usage/configurations.html#27-rendering","title":"2.7 \ud83c\udfa5 Rendering","text":"Field Type Description <code>render_freq</code> int Render visualization every N steps. Set to <code>0</code> to disable. For servers without display, recommend <code>0</code>. If you want to visualize the task, try to modify it to <code>20</code> (as example)"},{"location":"usage/configurations.html#3-notes","title":"3. \ud83d\udccc Notes","text":"<ul> <li>All task names must correspond to files in <code>env/&lt;task_name&gt;.py</code>.</li> <li> <p>For available embodiments and cameras, refer to:</p> </li> <li> <p><code>task_config/_embodiment_config.yml</code></p> </li> <li><code>task_config/_camera_config.yml</code></li> <li>The system supports both dual-arm and single-arm setups.</li> <li>Seeds, if used, are located in <code>task_config/seeds/</code>.</li> </ul>"},{"location":"usage/control-robot.html","title":"Control Robot","text":"<p>The <code>take_action</code> function in <code>_base_task</code> is used to control actions during task execution. It accepts two parameters: <code>action</code> and <code>action_type</code>.</p>"},{"location":"usage/control-robot.html#1-supported-action-types","title":"1. Supported Action Types","text":"<p>The parameter <code>action_type</code> supports two modes:</p> <ul> <li><code>qpos</code> (Joint Position Control) \u2014 default</li> <li><code>ee</code> (End-Effector Pose Control)</li> </ul> <p>Depending on the selected mode, the format and dimension of the input <code>action</code> will differ.</p>"},{"location":"usage/control-robot.html#11-qpos-mode-joint-position-control","title":"1.1 <code>qpos</code> Mode (Joint Position Control)","text":"<p>In <code>qpos</code> mode, the <code>action</code> is defined as:</p> <pre><code>[left_arm_joints + left_gripper + right_arm_joints + right_gripper]\n</code></pre> <ul> <li>The specific dimension of the <code>action</code> depends on the robotic arm configuration.</li> <li>The system will automatically adjust the input dimensions during deployment to match the specific robot configuration.</li> </ul>"},{"location":"usage/control-robot.html#12-ee-mode-end-effector-pose-control","title":"1.2 <code>ee</code> Mode (End-Effector Pose Control)","text":"<p>In <code>ee</code> mode, the <code>action</code> is defined as:</p> <pre><code>[left_end_effector_pose (xyz + quaternion) + left_gripper + right_end_effector_pose + right_gripper]\n</code></pre> <ul> <li>The dimension is fixed, regardless of the robot configuration.</li> </ul>"},{"location":"usage/control-robot.html#2-deployment-example","title":"2. Deployment Example","text":"<p>You can find a demonstration of usage in:</p> <pre><code>policy/Your_Policy/deploy_policy.py\n</code></pre> <p>This file provides a sample implementation to help you understand how to use the <code>take_action</code> function with different <code>action_type</code> settings during deployment.</p>"},{"location":"usage/deploy-your-policy.html","title":"\ud83d\ude80 Deploy Your Policy","text":"<p>To deploy and evaluate your policy, you need to modify the following three files:</p> <ul> <li><code>eval.sh</code>: eval.sh demo</li> <li><code>deploy_policy.yml</code>: deploy_policy.yml demo</li> <li><code>deploy_policy.py</code>: deploy_policy.py demo</li> </ul> <p>In <code>deploy_policy.py</code>, the following components are defined: <code>get_model</code> for loading the policy model, <code>encode_obs</code> for observation processing (modification may not be necessary), and <code>get_action</code> along with the control loop that handles observation acquisition and action execution.</p> <p>The <code>deploy_policy.yml</code> file specifies the input parameters, which are eventually passed into the <code>get_model</code> function as <code>usr_args</code> to assist in locating, defining, and loading your model.</p> <p>In <code>eval.sh</code>, the parameters specified after <code>overrides</code> can be used to overwrite those in <code>deploy_policy.yml</code>, allowing you to specify different settings without manually modifying the YAML file each time.</p>"},{"location":"usage/deploy-your-policy.html#1-deploy_policyyml","title":"1. \ud83d\udd27 <code>deploy_policy.yml</code>","text":"<p>You are free to add any parameters needed in <code>deploy_policy.yml</code> to specify your model setup (e.g., checkpoint path, model type, architecture details). The entire YAML content will be passed to <code>deploy_policy.py</code> as <code>usr_args</code>, which will be available in the <code>get_model()</code> function.</p>"},{"location":"usage/deploy-your-policy.html#2-evalsh","title":"2. \ud83d\udda5\ufe0f <code>eval.sh</code>","text":"<p>Update the script to pass additional arguments to override default values in <code>deploy_policy.yml</code>.</p> <pre><code>#!/bin/bash\n\npolicy_name=Your_Policy\ntask_name=${1}\ntask_config=${2}\nckpt_setting=${3}\nseed=${4}\ngpu_id=${5}\n# [TODO] Add your custom command-line arguments here\n\nexport CUDA_VISIBLE_DEVICES=${gpu_id}\necho -e \"\\033[33mgpu id (to use): ${gpu_id}\\033[0m\"\n\ncd ../.. # move to project root\n\npython script/eval_policy.py --config policy/$policy_name/deploy_policy.yml \\\n    --overrides \\\n    --task_name ${task_name} \\\n    --task_config ${task_config} \\\n    --ckpt_setting ${ckpt_setting} \\\n    --seed ${seed} \\\n    --policy_name ${policy_name} \n    # [TODO] Add your custom arguments here\n</code></pre>"},{"location":"usage/deploy-your-policy.html#3-deploy_policypy","title":"3. \ud83e\udde0  <code>deploy_policy.py</code>","text":"<p>You need to implement the following methods in <code>deploy_policy.py</code>:</p>"},{"location":"usage/deploy-your-policy.html#31-encode_obsobs-dict-dict","title":"3.1 <code>encode_obs(obs: dict) -&gt; dict</code>","text":"<p>Optional. This function is used to preprocess the raw environment observation (e.g., color channel normalization, reshaping, etc.). If not needed, it can be left unchanged.</p>"},{"location":"usage/deploy-your-policy.html#32-get_modelusr_args-dict-any","title":"3.2 <code>get_model(usr_args: dict) -&gt; Any</code>","text":"<p>Required. This function receives the full configuration from <code>deploy_policy.yml</code> via <code>usr_args</code> and must return the initialized model. You can define your own loading logic here, including parsing checkpoints and network parameters.</p>"},{"location":"usage/deploy-your-policy.html#33-evalenv-model-observation-instruction-any","title":"3.3 <code>eval(env, model, observation, instruction) -&gt; Any</code>","text":"<p>Required. The main evaluation loop. Given the current environment instance, model, and observation (as a dictionary), and a natural language <code>instruction</code> (string), this function must compute the next action and execute it in the environment.</p>"},{"location":"usage/deploy-your-policy.html#34-update_obsobs-dict-none","title":"3.4 <code>update_obs(obs: dict) -&gt; None</code>","text":"<p>Optional. Used to update any internal state of the model or observation buffer. Useful if your model requires a history of frames or a memory-based context.</p>"},{"location":"usage/deploy-your-policy.html#35-get_actionmodel-obs-dict-any","title":"3.5 <code>get_action(model, obs: dict) -&gt; Any</code>","text":"<p>Optional. Given a model and current observation, return the action to be executed. This is useful if action computation is separated from the evaluation loop.</p>"},{"location":"usage/deploy-your-policy.html#36-reset_model-none","title":"3.6 <code>reset_model() -&gt; None</code>","text":"<p>Optional but recommended. This function is called before the evaluation of each episode, allowing you to reset model states such as recurrent memory, history buffers, or context encodings.</p>"},{"location":"usage/deploy-your-policy.html#4-run-evalsh","title":"4. \u2714\ufe0f Run <code>eval.sh</code>","text":"<pre><code>bash eval.sh ...(input parameters you define)\n</code></pre>"},{"location":"usage/deploy-your-policy.html#5-notes","title":"5. \ud83d\udccc Notes","text":"<ul> <li>The variable <code>instruction</code> is a string containing the language command describing the task. You can choose how (or whether) to use it.</li> <li>Your policy should be compatible with the input/output format expected by the simulator.</li> </ul>"},{"location":"usage/description.html","title":"Description Gen (Object &amp; Task)","text":""},{"location":"usage/description.html#1-object-description","title":"1. Object Description","text":"<pre><code># Generate object description for all objects\npython3 utils/generate_object_description.py\n\n# Generate object description for a specific type of object with as many objects as this class contains\npython3 utils/generate_object_description.py 001_bottle\n\n# Generate object description for a specific object index of a specific type of object\npython3 utils/generate_object_description.py 001_bottle --index 0\n</code></pre>"},{"location":"usage/description.html#2-task-instruction","title":"2. Task Instruction","text":"<pre><code># Generate 60 task descriptions for a task\npython3 utils/generate_task_description.py place_shoe 60\n</code></pre> <p>It will call for <code>instruction_num % 12</code> times of API, each time returning 12 instructions shuffled into 10 seen and 2 unseen instructions.</p>"},{"location":"usage/description.html#3-episode-instruction","title":"3. Episode Instruction","text":"<pre><code># Generate 60 task descriptions for a task\npython3 utils/generate_episode_instructions.py place_shoe franka-panda-D435 1000\n</code></pre>"},{"location":"usage/description.html#31-parameters","title":"3.1 Parameters:","text":"<ul> <li><code>task_name</code>: Name of the task (JSON file name without extension)</li> <li><code>setting</code>: Setting name used to construct the data directory path</li> <li><code>max_num</code>: Maximum number of descriptions per episode</li> </ul>"},{"location":"usage/domain-randomization.html","title":"Domain Randomization","text":"<p>RoboTwin\u2019s domain randomization primarily focuses on scene clutter, random lighting, over 12,000 tabletop textures, randomized tabletop heights, and camera viewpoint perturbations. The corresponding configuration options can be found at: \ud83d\udc49 RoboTwin 2.0 Document (Usage: Configurations)</p> <p></p>"},{"location":"usage/dual-env-policy-deploy.html","title":"Dual Env Policy Deploy","text":""},{"location":"usage/expert-code-gen.html","title":"Expert Code Generation","text":""},{"location":"usage/expert-code-gen.html#1-code_gen-folder-structure","title":"1. Code_gen Folder Structure","text":"<p>This directory contains various modules for generating and testing robot task code:</p> <ul> <li>gpt_agent.py: API integration with LLM models</li> <li>observation_agent.py: Processes multi-modal observations for code correction</li> <li>prompt.py: Prompt templates for code generation</li> <li>run_code.py: Executes and tests generated code</li> <li>task_generation_simple.py: Basic single-pass code generation</li> <li>task_generation.py: Iterative code generation with error feedback</li> <li>task_generation_mm.py: Advanced code generation with multi-modal observation</li> <li>task_info.py: Task definitions and descriptions</li> <li>test_gen_code.py: Utility for testing generated code with detailed metrics</li> </ul> <p>The code generation system also interacts with these important directories: - ./envs/: Contains manually implemented task environments   - _base_task.py: Core environment with robot control functions and utilities     - Includes <code>save_camera_images(task_name, step_name, generate_num_id, save_dir)</code> for capturing visual observations during task execution - ./envs_gen/: Stores auto-generated task implementations - ./task_config/: Configuration files for tasks and embodiments - ./script/: Template scripts and utilities - ./assets/objects/: 3D models and metadata for simulation objects - ./camera_images/: Stores observation images captured during code generation for multi-modal feedback</p> <p>The entire pipeline enables automatic generation of robot control code from natural language task descriptions, with feedback-based refinement and multi-modal observation capabilities.</p>"},{"location":"usage/expert-code-gen.html#2-configure-llm-api-key","title":"2. Configure LLM API Key","text":"<p>Please configure the necessary API keys in the <code>code_gen/gpt_agent.py</code> file. Additionally, if the LLM you are utilizing does not support integration with the OpenAI API, you may need to make corresponding adjustments to the <code>generate()</code> function.</p>"},{"location":"usage/expert-code-gen.html#3-generate-your-task-code","title":"3. Generate Your Task Code","text":""},{"location":"usage/expert-code-gen.html#31-1-add-task-description","title":"3.1 1. Add Task Description","text":"<p>Add new task information, including the task name and natural language description, in <code>./code_gen/task_info.py</code>.</p>"},{"location":"usage/expert-code-gen.html#1-template-of-task-information","title":"1. Template of Task Information:","text":"<pre><code>TASK_NAME = {\n    \"task_name\": \"task_name\",                # Name of the task\n    \"task_description\": \"...\",               # Detailed description of the task\n    \"current_code\": '''\n                class gpt_{task_name}({task_name}):\n                    def play_once(self):\n                        pass\n                '''                          # Code template to be completed\n    \"actor_list\": {                          # List of involved objects; can be a dictionary or a simple list\n        \"self.object1\": {\n            \"name\": \"object1\",               # Object name\n            \"description\": \"...\",            # Description of the object\n            \"modelname\": \"model_name\"        # Name of the 3D model representing the object\n        },\n        \"self.object2\": {\n            \"name\": \"object2\",\n            \"description\": \"...\",\n            \"modelname\": \"model_name\"\n        },\n        # ... more objects\n    },\n    # Alternatively, the actor_list can be a simple list:\n    # \"actor_list\": [\"self.object1\", \"self.object2\", ...],\n    # To make code generation easier, the actor_list also includes some pose information\n    # like target poses or middle poses (optional and don't require modelname).\n}\n</code></pre>"},{"location":"usage/expert-code-gen.html#11-2-add-basic-task-code","title":"1.1 2. Add Basic Task Code","text":"<p>Add the basic code file <code>${task_name}.py</code> in the <code>./envs/</code> directory, following this structure:</p> <pre><code>from .base_task import Base_task\nfrom .utils import *\nimport sapien\n\nclass ${task_name}(Base_task):\n    def setup_demo(self, **kwargs):\n        # Initializes the simulation environment for the task\n        # Sets up the table, robot, planner, camera, and initial positions\n        # This function is called once at the beginning of each episode\n        pass\n\n    def load_actors(self):\n        # Loads all the necessary objects for the task into the environment\n        # Typically called from setup_demo to initialize scene objects\n        # Can also be used to set initial poses for objects\n        pass\n\n    def play_once(self):\n        # Contains the robot control code to complete the task\n        # This is the main function that will be generated by the LLM\n        # Implements the sequence of actions for the robot to achieve the task\n        pass\n\n    # Check success\n    def check_success(self):\n        # Defines criteria to determine if the task was completed successfully\n        # Returns a boolean indicating success or failure\n        # Used for evaluation and feedback during code generation\n        pass\n</code></pre> <p>In the code above, <code>{task_name}</code> should match the name of the basic code file, and the <code>check_success()</code> function is used to determine if the task is successful. No changes are needed for the rest of the code.</p> <p>Note: The <code>envs</code> folder contains manually written files with <code>setup_demo</code>, robot operation code in <code>play_once</code>, and <code>check_success</code> methods. Auto-generated code will be saved in the <code>envs_gen</code> folder.</p>"},{"location":"usage/expert-code-gen.html#12-3-generate-the-final-code","title":"1.2 3. Generate the Final Code","text":"<p>You can use three different code generation approaches depending on your needs:</p> <p>Note: The code generation process will only generate the <code>play_once()</code> method implementation, which contains the robot control logic to complete the task. Other methods like <code>setup_demo()</code>, <code>load_actors()</code>, and <code>check_success()</code> should be manually implemented.</p>"},{"location":"usage/expert-code-gen.html#121-basic-code-generation","title":"1.2.1 Basic Code Generation","text":"<p>For quick verification of new tasks or debugging existing ones without iterative correction:</p> <pre><code>python code_gen/task_generation_simple.py task_name\n</code></pre>"},{"location":"usage/expert-code-gen.html#122-code-generation-with-error-feedback","title":"1.2.2 Code Generation with Error Feedback","text":"<p>This script implements iterative code correction based on error feedback, consistent with RoboTwin 1.0:</p> <pre><code>python code_gen/task_generation.py task_name\n</code></pre>"},{"location":"usage/expert-code-gen.html#123-advanced-code-generation-with-multi-modal-observations","title":"1.2.3 Advanced Code Generation with Multi-Modal Observations","text":"<p>This script provides both error feedback iteration and multi-modal observation-based code correction, consistent with RoboTwin 2.0. It offers the best generation quality but runs slower:</p> <pre><code>python code_gen/task_generation_mm.py task_name\n</code></pre> <p>The multi-modal observation functionality is implemented in <code>code_gen/observation_agent.py</code>.</p> <p>The generated code file will be saved as <code>./envs_gen/gpt_${task_name}.py</code>. For example: <pre><code>python code_gen/task_generation_mm.py pick_dual_bottles_easy\n</code></pre> This will create <code>./envs_gen/gpt_pick_dual_bottles_easy.py</code>.</p>"},{"location":"usage/expert-code-gen.html#13-4-test-generated-code","title":"1.3 4. Test Generated Code","text":"<p>Run the following script to test the generated code:</p> <pre><code>python code_gen/run_code.py task_name\n</code></pre> <p>This will execute the task using the generated code and display the results, allowing you to validate the performance.</p>"},{"location":"usage/expert-code-gen.html#11-additional-resources","title":"1.1 Additional Resources","text":"<p>For more information on generating task descriptions and object descriptions, refer to the documentation in the description directory.</p> <p>For policy training and evaluation using the generated code, consult the policy/ACT documentation.</p>"},{"location":"usage/new-camera.html","title":"Configurating New Camera","text":"<p>Modify <code>task_config/_camera_config.yml</code> (Github file), adding new camera new and configurate <code>fov</code>, <code>h</code> and <code>w</code>, such as:</p> <pre><code>Demo_Camera:\n  fovy: 56\n  w: 224\n  h: 224\n</code></pre> <p>Finally, modify the camera type in the task config file.</p> <pre><code>camera:\n  head_camera_type: Demo_Camera\n  wrist_camera_type: D435\n</code></pre>"},{"location":"usage/new-embodiment.html","title":"Configure New Embodiment in RoboTwin","text":"<p>Embodiments are stored in the <code>assets/embodiments</code> directory. Each embodiment follows this file structure:</p> <pre><code># Using Franka as an example\n- embodiments\n  - franka-panda\n    - config.yml # RoboTwin config file\n    - curobo_tmp.yml # CuRobo config template\n    - collision_franka.yml # CuRobo collision annotations\n    - urdf_files/... # URDF files and corresponding GLB, STL files, etc.\n</code></pre> <p>This guide explains how to configure a new embodiment from scratch, using Franka as an example.</p>"},{"location":"usage/new-embodiment.html#1-step-1-configure-curobo-files","title":"1. Step 1: Configure CuRobo Files","text":"<p>For complete configuration instructions, refer to the official documentation: https://curobo.org/tutorials/1_robot_configuration.html. This section provides the minimal configuration steps.</p>"},{"location":"usage/new-embodiment.html#11-11-create-the-embodiment-directory-and-files","title":"1.1 1.1 Create the embodiment directory and files","text":"<pre><code>cd ${ROBOTWIN_ROOT_PATH}\nmkdir -p assets/embodiments/new_robot\ncd assets/embodiments/new_robot\ntouch curobo_tmp.yml\ntouch collision.yml\n</code></pre>"},{"location":"usage/new-embodiment.html#12-12-configure-curobo_tmpyml","title":"1.2 1.2 Configure curobo_tmp.yml","text":"<p>Here's a minimal Franka configuration example:</p> <pre><code>robot_cfg:\n  kinematics:\n    urdf_path: ${ASSETS_PATH}/assets/embodiments/franka-panda/panda.urdf\n    base_link: \"panda_link0\"\n    ee_link: \"panda_hand\"\n    collision_link_names:\n      [\n        \"panda_link0\",\n        \"panda_link1\",\n        \"panda_link2\",\n        \"panda_link3\",\n        \"panda_link4\",\n        \"panda_link5\",\n        \"panda_link6\",\n        \"panda_link7\",\n        \"panda_hand\",\n        \"panda_leftfinger\",\n        \"panda_rightfinger\",\n        \"attached_object\",\n      ]\n    collision_spheres: ${ASSETS_PATH}/assets/embodiments/franka-panda/collision_franka.yml\n    collision_sphere_buffer: 0.004\n    self_collision_ignore: {...}\n    self_collision_buffer: {...}\n    mesh_link_names: [...]\n    lock_joints: {\"panda_finger_joint1\": 0.04, \"panda_finger_joint2\": 0.04}\n    cspace:\n      joint_names: [\"panda_joint1\",\"panda_joint2\",\"panda_joint3\",\"panda_joint4\", \"panda_joint5\", \"panda_joint6\",\"panda_joint7\",\"panda_finger_joint1\", \"panda_finger_joint2\"]\n      retract_config: [0.2200, -1.4012, -0.0406, -1.4901,  0.3050,  0.4521,  0.2099, 0.04, 0.04]\n      null_space_weight: [1,1,1,1,1,1,1,1,1]\n      cspace_distance_weight: [1,1,1,1,1,1,1,1,1]\n      max_acceleration: 15.0\n      max_jerk: 500.0\nplanner:\n  frame_bias: [0., 0., 0.]\n</code></pre> <p>Key Parameter Explanations:</p> <ol> <li> <p>Path Requirements: Since this is a config template and CuRobo only supports absolute paths, both <code>urdf_path</code> and <code>collision_spheres</code> must keep the <code>${ASSETS_PATH}/assets/embodiments/</code> prefix unchanged. The <code>${ASSETS_PATH}</code> variable will be automatically replaced with the absolute path during subsequent operations.</p> </li> <li> <p>base_link and ee_link: These are the two most important links that directly determine your planning space. Replace these with your robot arm's actual link names.</p> </li> <li> <p>Collision Configuration: <code>collision_link_names</code> and <code>collision_spheres</code> determine self-collision and environment collision detection during planning. For detailed configuration, refer to the \"Robot Collision Representation\" section at https://curobo.org/tutorials/1_robot_configuration.html. All configurations in this repository are based on Isaac Sim 4.2.</p> </li> <li> <p>Joint Configuration: <code>cspace/joint_names</code> directly determines which joints need planning. This is defined by the URDF and must match the corresponding joint names. The lengths of <code>retract_config</code>, <code>null_space_weight</code>, and <code>cspace_distance_weight</code> must match the length of <code>joint_names</code>.</p> </li> <li> <p>Frame Bias: For single-arm URDFs, keep <code>planner/frame_bias</code> as <code>[0., 0., 0.]</code>. For dual-arm setups like ALOHA, slight adjustments are needed (detailed in the dual-arm configuration section).</p> </li> </ol>"},{"location":"usage/new-embodiment.html#13-13-configure-collisionyml","title":"1.3 1.3 Configure collision.yml","text":"<p>After annotating with Isaac Sim, you'll get collision spheres for different joints. Fill them into collision.yml in this format:</p> <pre><code>collision_spheres:\n    panda_link0:\n        - \"center\": [0.0, 0.0, 0.085]\n          \"radius\": 0.03\n        # ... more spheres\n    panda_link1:\n        - \"center\": [0.0, -0.08, 0.0]\n          \"radius\": 0.035\n        # ... more spheres\n</code></pre>"},{"location":"usage/new-embodiment.html#14-14-verify-curobo-configuration","title":"1.4 1.4 Verify CuRobo Configuration","text":"<p>After configuring CuRobo, verify the setup with a simple forward kinematics test. First, update the <code>${ASSETS_PATH}</code>:</p> <pre><code>cd ${ROBOTWIN_ROOT_PATH}\npython script/update_embodiment_config_path.py\n</code></pre> <p>This will generate <code>curobo.yml</code> from <code>curobo_tmp.yml</code>. Then run this verification code:</p> <pre><code>import torch\nfrom curobo.cuda_robot_model.cuda_robot_model import CudaRobotModel, CudaRobotModelConfig\nfrom curobo.types.base import TensorDeviceType\nfrom curobo.types.robot import RobotConfig\nfrom curobo.util_file import get_robot_path, join_path, load_yaml\n\ntensor_args = TensorDeviceType()\n\n# Modify to the absolute path of `curobo.yml`\nconfig_file = load_yaml(\"/abs_path/to/curobo.yml\")\n\nurdf_file = config_file[\"robot_cfg\"][\"kinematics\"][\"urdf_path\"]\nbase_link = config_file[\"robot_cfg\"][\"kinematics\"][\"base_link\"]\nee_link = config_file[\"robot_cfg\"][\"kinematics\"][\"ee_link\"]\nrobot_cfg = RobotConfig.from_basic(urdf_file, base_link, ee_link, tensor_args)\nkin_model = CudaRobotModel(robot_cfg.kinematics)\nq = torch.rand((10, kin_model.get_dof()), **(tensor_args.as_torch_dict()))\nout = kin_model.get_state(q)\n</code></pre> <p>If no errors occur, the configuration is successful.</p>"},{"location":"usage/new-embodiment.html#2-step-2-configure-robotwin-config-file","title":"2. Step 2: Configure RoboTwin Config File","text":""},{"location":"usage/new-embodiment.html#21-21-create-configyml","title":"2.1 2.1 Create config.yml","text":"<pre><code>cd assets/embodiments/new_robot\ntouch config.yml\n</code></pre>"},{"location":"usage/new-embodiment.html#22-22-parameter-configuration","title":"2.2 2.2 Parameter Configuration","text":"<p>Here's a Franka configuration example with detailed explanations:</p> <pre><code>urdf_path: \"./panda.urdf\"\nsrdf_path: \"./panda.srdf\"\njoint_stiffness: 1000\njoint_damping: 200\ngripper_stiffness: 1000\ngripper_damping: 200\nmove_group: [\"panda_hand\",\"panda_hand\"]\nee_joints: [\"panda_hand_joint\",\"panda_hand_joint\"]\narm_joints_name: [['panda_joint1', 'panda_joint2', 'panda_joint3', 'panda_joint4', 'panda_joint5', 'panda_joint6', 'panda_joint7'],\n                  ['panda_joint1', 'panda_joint2', 'panda_joint3', 'panda_joint4', 'panda_joint5', 'panda_joint6', 'panda_joint7']]\ngripper_name:\n  - base: \"panda_finger_joint1\"\n    mimic: [[\"panda_finger_joint2\", 1., 0.]]\n  - base: \"panda_finger_joint1\"\n    mimic: [[\"panda_finger_joint2\", 1., 0.]]\ngripper_bias: 0.08\ngripper_scale: [0.0, 0.04]\nhomestate: [[0, 0.19634954084936207, 0.0, -2.617993877991494, 0.0, 2.941592653589793, 0.7853981633974483],\n            [0, 0.19634954084936207, 0.0, -2.617993877991494, 0.0, 2.941592653589793, 0.7853981633974483]]\ndelta_matrix: [[0,0,1],[0,-1,0],[1,0,0]]\nglobal_trans_matrix: [[1,0,0],[0,-1,0],[0,0,-1]]\nrobot_pose: [[0, -0.65, 0.75, 0.707, 0, 0, 0.707],\n             [0, -0.65, 0.75, 0.707, 0, 0, 0.707]]\nplanner: \"curobo\"\ndual_arm: False\nrotate_lim: [0.1, 0.8]\ngrasp_perfect_direction: ['right', 'left']\nstatic_camera_list: \n- name: head_camera\n  position: [0.0, 0.8, 0.9]\n  forward: [0, -1, 0]\n  left: [1, 0, 0]\n</code></pre> <p>Parameter Explanations for New Embodiments:</p> <ol> <li> <p>urdf_path and srdf_path: Relative paths to URDF and SRDF files within <code>assets/embodiments/new_robot</code>. These are loaded by Sapien into the simulator and directly determine the physical collision properties.</p> </li> <li> <p>move_group: Used by MPLib, equivalent to CuRobo's <code>ee_link</code>. This is a list containing the ee_links for left and right arms.</p> </li> <li> <p>ee_joints: Since Sapien only supports global pose reading for joints, use the parent joint of the link specified in <code>move_group</code>.</p> </li> <li> <p>arm_joints_name: Joint names, same as CuRobo's <code>joint_names</code> parameter, but organized as a 2D list containing joint names for both left and right arms.</p> </li> <li> <p>gripper_name: Controls gripper movement with structure: <code>list[dict{\"base\":str, \"mimic\":[[str, float, float], ...]}, dict{\"base\":str, \"mimic\":[[str, float, float], ...]}]</code></p> </li> <li>First level list represents left and right grippers</li> <li>Second level dict distinguishes \"base\" (actively controlled joint) and \"mimic\" (passive joints)</li> <li>\"base\": String representing any gripper finger, controlled by <code>gripper_scale</code> where <code>gripper_scale[0]</code> is closed state and <code>gripper_scale[1]</code> is open state</li> <li> <p>\"mimic\": 2D array where each element contains [str, float1, float2] - joint name, scale, and bias. Joint angle = float1 * base_joint + bias</p> </li> <li> <p>gripper_bias: Adjusts distance from <code>ee_joint</code> to gripper center. For example, in vertical downward grasping, larger values move the gripper down, smaller values move it up.</p> </li> <li> <p>homestate: Initial robot arm state. Set carefully to avoid self-collision that could cause planning failures.</p> </li> <li> <p>delta_matrix: Rotation matrix to unify different ee_joint coordinate systems. To avoid errors, initially use an identity matrix as placeholder: <code>[[1,0,0],[0,1,0],[0,0,1]]</code>.</p> </li> <li> <p>global_trans_matrix: Rotation matrix to unify ee_joint pose reading in Sapien. To avoid errors, initially use an identity matrix as placeholder: <code>[[1,0,0],[0,1,0],[0,0,1]]</code>.</p> </li> <li> <p>robot_pose: Base_link placement positions in format <code>[[x,y,z,qw,qx,qy,qz],[x,y,z,qw,qx,qy,qz]]</code>. The x-coordinate represents the center position between two arms, recommended as 0. Actual spacing is adjusted in task configs like <code>demo_randomized.yml</code>.</p> </li> <li> <p>dual_arm: Boolean indicating whether the URDF is dual-arm (true for ALOHA) or single-arm (false for Franka).</p> </li> <li> <p>static_camera_list: Adjusts head_camera position, where <code>forward</code> and <code>left</code> represent the z-axis and x-axis directions of the camera coordinate system.</p> </li> </ol>"},{"location":"usage/new-embodiment.html#3-step-3-add-embodiment-path","title":"3. Step 3: Add Embodiment Path","text":"<p>Edit <code>task_config/_embodiment_config.yml</code> and add your new robot path:</p> <pre><code>new_robot:\n  file_path: \"./assets/embodiments/new_robot\" \n</code></pre> <p>Note: Your <code>config.yml</code> and <code>curobo_tmp.yml</code> must be directly located under <code>file_path</code>.</p>"},{"location":"usage/new-embodiment.html#4-step-4-modify-task-config","title":"4. Step 4: Modify Task Config","text":"<p>In your task config (e.g., <code>task_config/demo_randomized.yml</code>), change the <code>embodiment</code> section to:</p> <pre><code>embodiment:\n- new_robot\n- new_robot\n- 0.8  # Distance between the two robot arms\n</code></pre>"},{"location":"usage/new-embodiment.html#5-step-5-calibrate-delta_matrix","title":"5. Step 5: Calibrate delta_matrix","text":"<p>This calibration requires the desktop environment and is extremely important.</p>"},{"location":"usage/new-embodiment.html#51-51-create-temporary-urdf","title":"5.1 5.1 Create Temporary URDF","text":"<p>Before calibrating <code>delta_matrix</code> and <code>global_trans_matrix</code>, you must create a temporary URDF. Using Franka as an example:</p> <pre><code>cd assets/embodiments/franka-panda\ncp panda.urdf panda.urdf.save\n</code></pre> <p>Modify <code>panda.urdf</code> by: 1. Remove or comment out all collision tags for every link 2. Remove all joint limits and change all <code>revolute</code> joints to <code>continuous</code></p> <p>Example modifications:</p> <pre><code>&lt;!-- Comment out collision --&gt;\n&lt;link name=\"panda_link1\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;mesh filename=\"franka_description/meshes/visual/link1.glb\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/visual&gt; \n    &lt;!-- &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;mesh filename=\"franka_description/meshes/collision/link1.stl\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt; --&gt;\n&lt;/link&gt;\n\n&lt;!-- Remove joint limits, change revolute to continuous --&gt;\n&lt;!-- &lt;joint name=\"panda_joint3\" type=\"revolute\"&gt; --&gt;\n&lt;joint name=\"panda_joint3\" type=\"continuous\"&gt;\n    &lt;origin rpy=\"1.57079632679 0 0\" xyz=\"0 -0.316 0\"/&gt;\n    &lt;parent link=\"panda_link2\"/&gt;\n    &lt;child link=\"panda_link3\"/&gt;\n    &lt;axis xyz=\"0 0 1\"/&gt;\n    &lt;!-- Remove this line: &lt;limit effort=\"87\" lower=\"-2.8973\" upper=\"2.8973\" velocity=\"2.1750\"/&gt; --&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"usage/new-embodiment.html#52-52-find-valid-pose","title":"5.2 5.2 Find Valid Pose","text":"<p>The <code>delta_matrix</code> unifies coordinate systems across different robot arms. First, run this script to find a valid pose:</p> <pre><code>import torch\nfrom curobo.types.base import TensorDeviceType\nfrom curobo.types.math import Pose\nfrom curobo.types.robot import RobotConfig\nfrom curobo.util_file import get_robot_configs_path, join_path, load_yaml\nfrom curobo.wrap.reacher.ik_solver import IKSolver, IKSolverConfig\n\ntensor_args = TensorDeviceType()\nconfig_file = load_yaml(join_path(get_robot_configs_path(), \"franka.yml\"))\nurdf_file = config_file[\"robot_cfg\"][\"kinematics\"][\"urdf_path\"]\nbase_link = config_file[\"robot_cfg\"][\"kinematics\"][\"base_link\"]\nee_link = config_file[\"robot_cfg\"][\"kinematics\"][\"ee_link\"]\nrobot_cfg = RobotConfig.from_basic(urdf_file, base_link, ee_link, tensor_args)\n\nik_config = IKSolverConfig.load_from_robot_config(\n    robot_cfg,\n    None,\n    num_seeds=20,\n    self_collision_check=False,\n    self_collision_opt=False,\n    tensor_args=tensor_args,\n    use_cuda_graph=True,\n)\nik_solver = IKSolver(ik_config)\nx_values = torch.linspace(0.35, 0.0, 25).tolist() + torch.linspace(0.35, 0.7, 25).tolist()\ny_values = torch.linspace(0.25, 0.0, 25).tolist() + torch.linspace(0.25, 0.5, 25).tolist()\nz_values = torch.linspace(0.25, 0.0, 25).tolist() + torch.linspace(0.25, 0.5, 25).tolist()\nquaternion = torch.tensor([[1.0, 0.0, 0.0, 0.0]], device='cuda:0')\n\nprint(\"Testing IK solutions for different positions:\")\nprint(\"x, y, z, success\")\nfor x in x_values:\n    for y in y_values:\n        for z in z_values:\n            goal = Pose(\n                position=torch.tensor([[float(x), float(y), float(z)]], device='cuda:0'),\n                quaternion=quaternion\n            )\n            result = ik_solver.solve_single(goal)\n            if result.success.item() == True:\n                print(f\"{x:.2f}, {y:.2f}, {z:.2f}, {result.success}\")\n</code></pre> <p>Expected output: <pre><code>x, y, z, success\n0.35, 0.23, 0.09, tensor([[True]], device='cuda:0')\n0.35, 0.23, 0.08, tensor([[True]], device='cuda:0')\n0.35, 0.23, 0.07, tensor([[True]], device='cuda:0')\n...\n</code></pre></p>"},{"location":"usage/new-embodiment.html#53-53-test-in-simulation","title":"5.3 5.3 Test in Simulation","text":"<p>Choose any successful xyz coordinates and modify <code>envs/robot/planner.py</code> around line 266:</p> <pre><code>target_pose_p[0] += self.frame_bias[0]\ntarget_pose_p[1] += self.frame_bias[1]\ntarget_pose_p[2] += self.frame_bias[2]\n## Temporarily add the successful xyz coordinates ##\ntarget_pose_p = [0.35, 0.23, 0.09]  # Example: using 0.35, 0.23, 0.09\ntarget_pose_q = [1., 0., 0., 0.]\n## End temporary addition ## \ngoal_pose_of_gripper = CuroboPose.from_list(list(target_pose_p) + list(target_pose_q))\n</code></pre> <p>Modify <code>envs/beat_block_hammer.py</code> to add a sleep:</p> <pre><code>self.move(self.grasp_actor(self.hammer, arm_tag=arm_tag, pre_grasp_dis=0.12, grasp_dis=0.01, gripper_pos=0.35))\n# Add sleep for observation\ntime.sleep(100)\n</code></pre> <p>Set <code>render_freq</code> to a positive number in your task config (e.g., <code>demo_randomized.yml</code>), then run:</p> <pre><code>bash collect_data.sh beat_block_hammer demo_randomized 0\n</code></pre>"},{"location":"usage/new-embodiment.html#54-54-analyze-coordinate-systems","title":"5.4 5.4 Analyze Coordinate Systems","text":"<p>You should see a visualization similar to this:</p> <p></p> <p>Coordinate System Analysis: - ee_joint_frame:    - X-axis (red): Should point from the link toward the gripper direction   - Y-axis (green): Should be parallel to gripper movement direction (positive or negative)   - Z-axis (blue): Determined by right-hand rule</p> <ul> <li>reference_frame: </li> <li>X-axis: Robot's forward direction</li> <li>Z-axis: Opposite to gravity direction (upward)</li> <li>Y-axis: Determined by right-hand rule</li> <li>This frame is fixed and consistent across all robots</li> </ul>"},{"location":"usage/new-embodiment.html#55-55-calculate-delta_matrix","title":"5.5 5.5 Calculate delta_matrix","text":"<p>The <code>delta_matrix</code> represents the rotation from ee_joint frame to reference frame: <code>{ee_joint}_Rotation_{reference}</code>.</p> <p>From the example image above, the delta_matrix would be: <pre><code>delta_matrix = [[0, 0, 1],\n                [0, -1, 0],\n                [1, 0, 0]]\n</code></pre></p> <p>Update this matrix in your <code>config.yml</code>.</p>"},{"location":"usage/new-embodiment.html#6-step-6-calibrate-global_trans_matrix","title":"6. Step 6: Calibrate global_trans_matrix","text":""},{"location":"usage/new-embodiment.html#61-61-get-actual-planned-pose","title":"6.1 6.1 Get Actual Planned Pose","text":"<p>Keep the <code>time.sleep</code> in <code>beat_block_hammer.py</code> and modify <code>envs/robot/planner.py</code> to output the target quaternion:</p> <pre><code>target_pose_p[0] += self.frame_bias[0]\ntarget_pose_p[1] += self.frame_bias[1]\ntarget_pose_p[2] += self.frame_bias[2]\n# Remove the hardcoded position and quaternion\n# target_pose_p = np.array([0.35, 0.23, 0.09])\n# target_pose_q = np.array([1.0, 0.0, 0.0, 0.0])\nprint('[debug]: target_pose_q: ', target_pose_q)\ngoal_pose_of_gripper = CuroboPose.from_list(list(target_pose_p) + list(target_pose_q))\n</code></pre> <p>Expected output: <pre><code>[debug]: target_pose_q:  [ 1.68244557e-03 -9.98540531e-01 -3.19133105e-04 -5.39803316e-02]\n</code></pre></p> <p>Important: Use your actual output quaternion values, not the example above. Each robot arm will produce different quaternion values based on its specific configuration.</p>"},{"location":"usage/new-embodiment.html#62-62-test-with-new-quaternion","title":"6.2 6.2 Test with New Quaternion","text":"<p>Use the output quaternion to test valid positions by modifying the test script, and REMEMBER TO UPDATE THE QUATERNION: </p> <pre><code>import torch\nfrom curobo.types.base import TensorDeviceType\nfrom curobo.types.math import Pose\nfrom curobo.types.robot import RobotConfig\nfrom curobo.util_file import get_robot_configs_path, join_path, load_yaml\nfrom curobo.wrap.reacher.ik_solver import IKSolver, IKSolverConfig\n\ntensor_args = TensorDeviceType()\nconfig_file = load_yaml(join_path(get_robot_configs_path(), \"franka.yml\"))\nurdf_file = config_file[\"robot_cfg\"][\"kinematics\"][\"urdf_path\"]\nbase_link = config_file[\"robot_cfg\"][\"kinematics\"][\"base_link\"]\nee_link = config_file[\"robot_cfg\"][\"kinematics\"][\"ee_link\"]\nrobot_cfg = RobotConfig.from_basic(urdf_file, base_link, ee_link, tensor_args)\n\nik_config = IKSolverConfig.load_from_robot_config(\n    robot_cfg,\n    None,\n    num_seeds=20,\n    self_collision_check=False,\n    self_collision_opt=False,\n    tensor_args=tensor_args,\n    use_cuda_graph=True,\n)\nik_solver = IKSolver(ik_config)\nx_values = torch.linspace(0.35, 0.0, 25).tolist() + torch.linspace(0.35, 0.7, 25).tolist()\ny_values = torch.linspace(0.25, 0.0, 25).tolist() + torch.linspace(0.25, 0.5, 25).tolist()\nz_values = torch.linspace(0.25, 0.0, 25).tolist() + torch.linspace(0.25, 0.5, 25).tolist()\n\n###### REMEMBER TO UPDATE THE QUATERNION ####\n#############################################\n# Update the quaternion from the debug output\nquaternion = torch.tensor([[1.68244557e-03, -9.98540531e-01, -3.19133105e-04, -5.39803316e-02]], device='cuda:0')\n#############################################\n\nprint(\"Testing IK solutions for different positions:\")\nprint(\"x, y, z, success\")\nfor x in x_values:\n    for y in y_values:\n        for z in z_values:\n            goal = Pose(\n                position=torch.tensor([[float(x), float(y), float(z)]], device='cuda:0'),\n                quaternion=quaternion\n            )\n            result = ik_solver.solve_single(goal)\n            if result.success.item() == True:\n                print(f\"{x:.2f}, {y:.2f}, {z:.2f}, {result.success}\")\n</code></pre> <p>Expected output: <pre><code>x, y, z, success\n0.35, 0.24, 0.27, tensor([[True]], device='cuda:0')\n0.35, 0.24, 0.28, tensor([[True]], device='cuda:0')\n...\n</code></pre></p>"},{"location":"usage/new-embodiment.html#63-63-calculate-global_trans_matrix","title":"6.3 6.3 Calculate global_trans_matrix","text":"<p>Update <code>envs/robot/planner.py</code> with a successful position:</p> <pre><code>target_pose_p[0] += self.frame_bias[0]\ntarget_pose_p[1] += self.frame_bias[1]\ntarget_pose_p[2] += self.frame_bias[2]\n\n# Update with successful position, remove debug print and target_pose_q\ntarget_pose_p = np.array([0.35, 0.24, 0.27])\n# target_pose_q = np.array([1.0, 0.0, 0.0, 0.0])\n# print('[debug]: target_pose_q: ', target_pose_q)\ngoal_pose_of_gripper = CuroboPose.from_list(list(target_pose_p) + list(target_pose_q))\n</code></pre> <p>Replace the entire <code>play_once(self)</code> function in <code>envs/beat_block_hammer.py</code> and UPDATE THE DELTA_MATRIX BELOW:</p> <pre><code>def play_once(self):\n    # Get the position of the block's functional point\n    block_pose = self.block.get_functional_point(0, \"pose\").p\n    # Use left arm for testing\n    arm_tag = \"left\"\n\n    # Grasp the hammer with the selected arm\n    action = self.grasp_actor(self.hammer, arm_tag=arm_tag, pre_grasp_dis=0.12, grasp_dis=0.01, gripper_pos=0.35)\n    self.move(action)\n\n    import transforms3d as t3d\n    while True:\n        left_ee_global_pose_q = list(self.robot.left_ee.global_pose.q)\n        w_R_joint = t3d.quaternions.quat2mat(left_ee_global_pose_q)\n        w_R_aloha = t3d.quaternions.quat2mat(action[1][0].target_pose[3:])\n        ######## REMEMBER TO UPDATE THE DELTA_MATRIX!!!! ####\n        # Update this delta_matrix with your calculated value\n        delta_matrix = np.matrix([[0,0,1],[0,-1,0],[1,0,0]])\n        #####################################################\n        global_trans_matrix = w_R_joint.T @ w_R_aloha @ delta_matrix.T\n        print(np.round(global_trans_matrix))\n</code></pre> <p>Run the simulation again:</p> <pre><code>bash collect_data.sh beat_block_hammer demo_randomized 0\n</code></pre> <p>Expected output: <pre><code>[[ 1.  0.  0.]\n [ 0. -1.  0.]\n [ 0. -0. -1.]]\n</code></pre></p> <p>This is your <code>global_trans_matrix</code>. Add it to your <code>config.yml</code>.</p>"},{"location":"usage/new-embodiment.html#64-64-clean-up","title":"6.4 6.4 Clean Up","text":"<p>Restore the modified files:</p> <pre><code>git checkout -- envs/robot/planner.py\ngit checkout -- envs/beat_block_hammer.py\n</code></pre> <p>Congratulations! Your new embodiment configuration is now complete.</p>"},{"location":"usage/new-embodiment.html#7-dual-arm-urdf-configuration","title":"7. Dual-Arm URDF Configuration","text":"<p>Dual-arm URDFs have a slightly different structure:</p> <pre><code># Using ALOHA as an example\n- embodiments\n  - aloha\n    - config.yml # RoboTwin config file\n    - curobo_left_tmp.yml # Left arm CuRobo config template\n    - curobo_right_tmp.yml # Right arm CuRobo config template\n    - collision_aloha_left.yml # Left arm collision annotations\n    - collision_aloha_right.yml # Right arm collision annotations\n    - urdf_files/... # URDF files and corresponding GLB, STL files, etc.\n</code></pre>"},{"location":"usage/new-embodiment.html#71-key-considerations-for-dual-arm-setup","title":"7.1 Key Considerations for Dual-Arm Setup:","text":"<ol> <li> <p>Frame Bias Configuration: In <code>curobo_left_tmp.yml</code> and <code>curobo_right_tmp.yml</code>, if your CuRobo config's <code>robot_cfg/kinematics/base_link</code> doesn't match the URDF's <code>base_link</code> (e.g., using <code>fl_base_link</code> in ALOHA), you need <code>planner/frame_bias</code>. This represents the translation vector from the URDF's <code>base_link</code> to the CuRobo's <code>base_link</code> (e.g., <code>fl_base_link</code>). The same applies to the right arm.</p> </li> <li> <p>Config.yml Settings: Set <code>dual_arm: True</code> in config.yml for dual-arm configurations.</p> </li> </ol> <p>This completes the embodiment configuration process. The setup allows RoboTwin to properly load and control your new robot embodiment in both simulation and planning contexts.</p>"},{"location":"usage/object-annotation.html","title":"Calibration Tool Instructions","text":""},{"location":"usage/object-annotation.html#1-rigid-body-object-annotation","title":"1. Rigid Body Object Annotation","text":""},{"location":"usage/object-annotation.html#11-create-calibration-window","title":"1.1 Create Calibration Window:","text":"<p><pre><code>python script/create_object_data.py [-s START] model_name\n\npositional arguments:\n    model_name            Model name\n\noptions:\n    -s START, --start START Start id\n</code></pre> Here, <code>model_name</code> is the name of a subdirectory under the <code>assets/objects/</code> directory. For example, to calibrate the hammer model located at <code>assets/objects/020_hammer</code>, run the command: <code>python script/create_object_data.py 020_hammer</code>. A window will then appear as shown below: </p>"},{"location":"usage/object-annotation.html#12-calibration-commands","title":"1.2 Calibration Commands:","text":"<p><pre><code>resize:\n    Usage:\n        resize &lt;x_size&gt; &lt;y_size&gt; &lt;z_size&gt;: Set scaling along x, y, z axes\n        resize &lt;size&gt;: Uniformly scale all three axes\n    Example:\n        resize 0.1\ncreate:\n    Usage:\n        create &lt;type&gt;: Create (t)arget, (c)ontact, (f)unctional, or (o)rientation point\n        create: Waits for input of point name\n    Examples:\n        create t\n        create f\nclone:\n    Usage:\n        clone &lt;type&gt; &lt;id&gt;: Clone a specified type and ID point in place\n        clone: Waits for input of point type and ID\n    Examples:\n        clone t 1: Clones contact point target_1 to create a new target point (e.g., target_2)\nrotate:\n    Usage:\n        rotate &lt;id&gt; &lt;axis&gt; &lt;interval&gt;: Rotate a specified contact point around its own axis by a given interval, generating points belonging to the same group\n    Example:\n        rotate 1 x 90: Rotates contact_1 around its x-axis every 90 degrees, creating three additional contact points, and writes the group into concat_points_group\nalign:\n    Usage:\n        align: Aligns all group points' positions to the first point in the group\nremove:\n    Usage:\n        remove &lt;type&gt; &lt;id&gt;: Removes a point with the specified name\n        remove: Waits for input of point name\n    Examples:\n        remove t 0\nsave:\n    Saves current calibration data \u2014 always remember to save!\nexit:\n    Exits the calibration window\n</code></pre> As an example using <code>020_hammer</code>, entering <code>create c</code> creates a cube centered on the object. Use your mouse to select this cube and check \"Enable\" under the Transform section in the UI window. Then choose \"Local\" to display the cube's center position and coordinate system, which represents the contact point's location and orientation: </p> <p>You can move the calibration point's position with the mouse. Click on \"Rotate\" in the Transform options to adjust the rotation along the x, y, and z axes, changing the point's coordinate system orientation: </p> <p>Next, add a functional point to the head of the hammer, adjust its orientation, and use the command <code>create f</code> to move it to the center position of the hammer head. The adjusted point is shown in the following image: </p> <p>Finally, enter <code>save</code> to save the point information, and then enter <code>exit</code> to end the calibration.</p> <p>Notes :</p> <ol> <li>After adjusting the position, you must click \"Teleport\" under the Transform menu to apply the movement.</li> <li>Always remember to save your changes before exiting the calibration window!</li> </ol>"},{"location":"usage/object-annotation.html#13-view-calibration-files","title":"1.3 View Calibration Files","text":"<p>Navigate to the asset folder you just calibrated, and you will find a newly generated <code>model_data{id}.json</code> file. You can modify the <code>\"scale\"</code> field within this file to adjust the asset's scaling in the simulation environment. </p> <p>The meanings of each field in the asset can be found in the model_data_info file.</p>"},{"location":"usage/object-annotation.html#2-urdf-articulation-objects-annotation","title":"2. URDF Articulation Objects Annotation","text":""},{"location":"usage/object-annotation.html#21-create-calibration-window","title":"2.1 Create Calibration Window:","text":"<p>Similar to rigid body object annotation, use the same command to create the articulation calibration window. The calibration program will automatically recognize the asset type.</p>"},{"location":"usage/object-annotation.html#22-calibration-commands","title":"2.2 Calibration Commands:","text":"<p><pre><code>run:\n    Usage:\n        run\n        Press &lt;Ctrl + C&gt; to stop and save information\n    Used to obtain stable points through steps, generally selected at the beginning of calibration to determine if running is necessary.\n    Since this command does not limit the step upper limit, you need to manually stop running (press Ctrl+C) based on whether the asset in the UI interface is stable.\nqpos:\n    Usage:\n        qpos\n    Get the current joint state as the initial pose when loading the asset into the task.\nmass:\n    Usage:\n        mass &lt;m1&gt; &lt;m2&gt; ...: Set the mass of the articulation joint, ensuring that the input matches the displayed link count (excluding base) in order.\n    Example:\n        mass 0.5 0.05\nresize:\n    Usage:\n        resize &lt;size&gt;: Synchronize the scaling of all three axes of the object\n    Example:\n        resize 0.1\ncreate:\n    Usage:\n        create &lt;type&gt; &lt;base_link&gt;: Create (t)arget, (c)ontact, (f)unctional, (o)rientation points\n    Example:\n        create c link1\nrebase:\n    Usage:\n        rebase &lt;type&gt; &lt;id&gt; &lt;base_link&gt;: Modify the base link of the specified point\n    Example:\n        rebase c 0 link1\nclone:\n    Usage:\n        clone &lt;type&gt; &lt;id&gt;: Create an in-place copy of the specified point (without base)\n    Example:\n        clone t 1: Create a new target point (e.g., target_2&lt;link1&gt;) by copying target_1&lt;link1&gt;\nrotate:\n    Usage:\n        rotate &lt;id&gt; &lt;axis&gt; &lt;interval&gt;: Rotate the specified contact point around its own specified axis by the specified interval, generating points belonging to the same group\n    Example:\n        rotate 1 x 90: Rotate contact_1 around its own x-axis by 90 degrees, generating three contact points, and write the grouping of the four points into concat_points_group\nalign:\n    Usage:\n        align: Align the positions of all group points to the first point in the group\nremove:\n    Usage:\n        remove &lt;type&gt; &lt;id&gt;: Remove the specified point (without base)\n    Example:\n        remove t 0\nsave:\n    Usage:\n        save: Save the current calibration data, and make sure to save!\nexit:\n    Usage:\n        exit: Exit the calibration window\n</code></pre> The calibration process is similar to rigid body object annotation, and you also need to save the data and exit after completion.</p>"},{"location":"usage/robotwin-install.html","title":"Install &amp; Download","text":""},{"location":"usage/robotwin-install.html#1-dependencies","title":"1. Dependencies","text":"<p>System Support: </p> <p>We currently best support Linux based systems. There is limited support for windows and no support for MacOS at the moment. We are working on trying to support more features on other systems but this may take some time. Most constraints stem from what the SAPIEN package is capable of supporting.</p> System / GPU CPU Sim GPU Sim Rendering Linux / NVIDIA GPU \u2705 \u2705 \u2705 Windows / NVIDIA GPU \u2705 \u274c \u2705 Windows / AMD GPU \u2705 \u274c \u2705 WSL / Anything \u2705 \u274c \u274c MacOS / Anything \u2705 \u274c \u2705 <p>Occasionally, data collection may get stuck when using A/H series GPUs. This issue may be related to RoboTwin issue #83 and SAPIEN issue #219.</p> <p>Python versions:</p> <ul> <li>Python 3.10</li> </ul> <p>CUDA version:</p> <ul> <li>12.1 (Recommended)</li> </ul> <p>Hardware:</p> <ul> <li> <p>Rendering: NVIDIA or AMD GPU</p> </li> <li> <p>Ray tracing: NVIDIA RTX GPU or AMD equivalent</p> </li> <li> <p>Ray-tracing Denoising: NVIDIA GPU</p> </li> <li> <p>GPU Simulation: NVIDIA GPU</p> </li> </ul> <p>Software:</p> <ul> <li>Ray tracing: NVIDIA Driver &gt;= 470</li> <li>Denoising (OIDN): NVIDIA Driver &gt;= 520</li> </ul>"},{"location":"usage/robotwin-install.html#11-additional-requirements-for-docker","title":"1.1 Additional Requirements for Docker","text":"<p>When running in a Docker container, ensure that the following environment variable is set when starting the container:</p> <pre><code>-e NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics\n</code></pre> <p>Important : The graphics capability is essential. Omitting it may result in segmentation faults due to missing Vulkan support.</p> <p>For more information, see HERE.</p>"},{"location":"usage/robotwin-install.html#2-install-vulkan-if-not-installed","title":"2. Install Vulkan (if not installed)","text":"<p><pre><code>sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools\n</code></pre> Check by running <code>vulkaninfo</code></p>"},{"location":"usage/robotwin-install.html#3-basic-env","title":"3. Basic Env","text":"<p>First, prepare a conda environment. <pre><code>conda create -n RoboTwin python=3.10 -y\nconda activate RoboTwin\n</code></pre></p> <p>RoboTwin 2.0 Code Repo: https://github.com/RoboTwin-Platform/RoboTwin</p> <pre><code>git clone https://github.com/RoboTwin-Platform/RoboTwin.git\n</code></pre> <p>Then, run <code>script/_install.sh</code> to install basic envs and CuRobo: <pre><code>bash script/_install.sh\n</code></pre></p> <p>If you meet curobo config path issue, try to run <code>python script/update_embodiment_config_path.py</code></p> <p>If you encounter any problems, please refer to the manual installation section. If you are not using 3D data, a failed installation of pytorch3d will not affect the functionality of the project.</p> <p>If you haven't installed ffmpeg, please turn to https://ffmpeg.org/. Check it by running <code>ffmpeg -version</code>.</p>"},{"location":"usage/robotwin-install.html#4-download-assets-robotwin-od-texture-library-and-embodiments","title":"4. Download Assets (RoboTwin-OD, Texture Library and Embodiments)","text":"<p>To download the assets, run the following command. If you encounter any rate-limit issues, please log in to your Hugging Face account by running <code>huggingface-cli login</code>: <pre><code>bash script/_download_assets.sh\n</code></pre></p> <p>The structure of the <code>assets</code> folder should be like this:</p> <pre><code>assets\n\u251c\u2500\u2500 background_texture\n\u251c\u2500\u2500 embodiments\n\u2502   \u251c\u2500\u2500 embodiment_1\n\u2502   \u2502   \u251c\u2500\u2500 config.yml\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 objects\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"usage/robotwin-install.html#5-manual-installation-only-when-step-3-failed","title":"5. Manual Installation (Only when step 3 failed)","text":"<ol> <li> <p>Install requirements <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install pytorch3d <pre><code>pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\n</code></pre></p> </li> <li> <p>Install CuRobo <pre><code>cd envs\ngit clone https://github.com/NVlabs/curobo.git\ncd curobo\npip install -e . --no-build-isolation\ncd ../..\n</code></pre></p> </li> <li> <p>Adjust code in <code>mplib</code> (Important)</p> </li> <li> <p>You can use <code>pip show mplib</code> to find where the <code>mplib</code> installed.</p> </li> <li> <p>Remove <code>or collide</code></p> </li> </ol> <pre><code># mplib.planner (mplib/planner.py) line 807\n# remove `or collide`\n\nif np.linalg.norm(delta_twist) &lt; 1e-4 or collide or not within_joint_limit:\n                return {\"status\": \"screw plan failed\"}\n=&gt;\nif np.linalg.norm(delta_twist) &lt; 1e-4 or not within_joint_limit:\n                return {\"status\": \"screw plan failed\"}\n</code></pre>"},{"location":"usage/object_marking/model_data_info.html","title":"Model data info","text":"<p>center: \u200b\u7269\u4f53\u200b\u4e2d\u5fc3\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u7269\u4f53\u200b\u8d44\u4ea7\u200b\u5750\u6807\u200b\u7684\u200b\u504f\u79fb\u200b\u5411\u91cf\u200b\uff08x y z\uff09: 1*3 matrix extents: \u200b\u8d44\u4ea7\u200b\u957f\u5bbd\u200b\u9ad8\u200b\uff08x y z \u200b\u5750\u6807\u8f74\u200b\u4e0a\u200b\u7684\u200b\u957f\u5ea6\u200b\uff09: 1*3 matrix scale: \u200b\u573a\u666f\u200b\u4e2d\u200b\u5bf9\u200b\u5b9e\u9645\u200b\u7269\u4f53\u200b\u8d44\u4ea7\u200b\u7684\u200b\u7f29\u653e\u200b\uff08x y z\u200b\u65b9\u5411\u200b\u4e0a\u200b\uff09: 1*3 matrix target_pose: \u200b\u5df2\u200b\u88ab\u200b\u5e9f\u9664\u200b\uff0c\u200b\u65e0\u200b\u542b\u4e49\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u200b\u6807\u5b9a\u200b contact_points_pose: list\u200b\u5217\u8868\u200b\uff0c\u200b\u5217\u8868\u200b\u5143\u7d20\u200b\u4e3a\u200b4*4\u200b\u65cb\u8f6c\u200b+\u200b\u5e73\u79fb\u200b\u77e9\u9635\u200b\uff0c\u200b\u8868\u793a\u200b\u6293\u53d6\u200b\u70b9\u200b\u4e0e\u200b\u7269\u4f53\u200b\u4e2d\u5fc3\u200b\u5750\u6807\u200b\u7684\u200b\u504f\u79fb\u200b: n*4*4 matrix transform_matrix: \u200b\u4e0d\u200b\u91cd\u8981\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u200b\u6807\u5b9a\u200b functional_matrix: list\u200b\u5217\u8868\u200b\uff0c\u200b\u5217\u8868\u200b\u5143\u7d20\u200b\u4e3a\u200b4*4\u200b\u65cb\u8f6c\u200b+\u200b\u5e73\u79fb\u200b\u77e9\u9635\u200b\uff0c\u200b\u8868\u793a\u200b\u529f\u80fd\u200b\u70b9\u200b\u4e0e\u200b\u7269\u4f53\u200b\u4e2d\u5fc3\u200b\u5750\u6807\u200b\u7684\u200b\u504f\u79fb\u200b: n*4*4 matrix orientation_point: \u200b\u65b9\u5411\u200b\u70b9\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u200b\u6807\u5b9a\u200b contact_points_group: \u200b\u6293\u53d6\u200b\u70b9\u200b\u5206\u7ec4\u200b\uff0c\u200b\u539f\u56e0\u200b\u4e3a\u200b\u6709\u4e9b\u200b\u6293\u53d6\u200b\u70b9\u200bxyz\u200b\u5750\u6807\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u662f\u200b\u8f74\u200b\u7684\u200b\u65b9\u5411\u4e0d\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u628a\u200b\u8fd9\u90e8\u5206\u200b\u6293\u53d6\u200b\u70b9\u5206\u200b\u5230\u200b\u540c\u4e2a\u200bgroup\u200b\u5185\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u4e00\u4e9b\u200b\u64cd\u4f5c\u200b contact_points_mask: \u200b\u81ea\u52a8\u200b\u751f\u6210\u200b\uff0c\u200b\u65e0\u9700\u200b\u6807\u5b9a\u200b\uff08\u200b\u4f3c\u4e4e\u200b\u88ab\u200b\u5e9f\u9664\u200b\u4e86\u200b\uff09 contact_points_discription: \u200b\u6293\u53d6\u200b\u70b9\u200b\u63cf\u8ff0\u200b: 1*n str functional_point_discription: \u200b\u529f\u80fd\u200b\u70b9\u200b\u63cf\u8ff0\u200b: 1*n str orientation_point_discription\uff1a \u200b\u65b9\u5411\u200b\u70b9\u200b\u63cf\u8ff0\u200b: 1*n str</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u63cf\u8ff0\u200b\u6807\u5b9a\u200b\u4e00\u822c\u200b\u901a\u8fc7\u200b\u76f4\u63a5\u200b\u66f4\u6539\u200bjson\u200b\u6587\u4ef6\u200b\u6765\u200b\u6807\u5b9a\u200b\uff0c\u200b\u4e0d\u662f\u200b\u5f3a\u200b\u683c\u5f0f\u200b\u6807\u5b9a\u200b\u6570\u636e\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u200b\u901a\u8fc7\u200b\u5176\u4ed6\u200b\u8868\u793a\u200b\u65b9\u6cd5\u200b\u5916\u90e8\u200b\u6807\u5b9a\u200b\u3002</p>"}]}